{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisandoryan/Camstroke-Inference/blob/main/Camstroke_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBjlSxp4LrGr"
      },
      "source": [
        "# Import and Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6pPCxpiFLS2G"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import shlex\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVRaCR2_LvE8"
      },
      "source": [
        "# Model Builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MZTpw5pRLdHE"
      },
      "outputs": [],
      "source": [
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "\n",
        "def build_models(num_encoder_tokens, num_decoder_tokens):\n",
        "    \"\"\"\n",
        "    ## Build the model\n",
        "    \"\"\"\n",
        "\n",
        "    # Define an input sequence and process it.\n",
        "    encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "    # We discard `encoder_outputs` and only keep the states.\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "    # We set up our decoder to return full output sequences,\n",
        "    # and to return internal states as well. We don't use the\n",
        "    # return states in the training model, but we will use them in inference.\n",
        "    decoder_lstm = keras.layers.LSTM(latent_dim, dropout=0.2, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "    # dropout = keras.layers.Dropout(rate=0.2)\n",
        "    # decoder_outputs = dropout(decoder_outputs)\n",
        "    \n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model that will turn\n",
        "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_models(path):\n",
        "    \"\"\"\n",
        "    ## Run inference (sampling)\n",
        "\n",
        "    1. encode input and retrieve initial decoder state\n",
        "    2. run one step of decoder with this initial state\n",
        "    and a \"start of sequence\" token as target.\n",
        "    Output will be the next target token.\n",
        "    3. Repeat with the current target token and current states\n",
        "    \"\"\"\n",
        "\n",
        "    # Define sampling models\n",
        "    # Restore the model and construct the encoder and decoder.\n",
        "    model = keras.models.load_model(path)\n",
        "    model.summary()\n",
        "\n",
        "    encoder_inputs = model.input[0]  # input_1\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1]  # input_2\n",
        "    decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_7\")\n",
        "    decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_8\")\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_lstm = model.layers[3]\n",
        "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs\n",
        "    )\n",
        "    decoder_states = [state_h_dec, state_c_dec]\n",
        "    decoder_dense = model.layers[4]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "    )\n",
        "\n",
        "    return model, encoder_model, decoder_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32aJNHtmL3Q5"
      },
      "source": [
        "# Training and Evaluation Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Z4Wt8-Mi05"
      },
      "source": [
        "## Parameter Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "94Ib_9KNMYxP"
      },
      "outputs": [],
      "source": [
        "batch_size = 32  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "\n",
        "START_TOKEN = \"\\t\"\n",
        "END_TOKEN = \"\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z79A-zZNMp16"
      },
      "source": [
        "## Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gyIdXN8HLx7P"
      },
      "outputs": [],
      "source": [
        "def prepare(datasets, keydelay_tokens, keytext_tokens):\n",
        "    # Vectorize the data.\n",
        "    keydelay_inputs = []\n",
        "    keytext_targets = []\n",
        "    keydelay_tokens = sorted(list(keydelay_tokens))\n",
        "    keytext_tokens = [START_TOKEN, END_TOKEN] + list(keytext_tokens)\n",
        "    keytext_tokens = sorted(list(keytext_tokens))\n",
        "    num_encoder_tokens = max(keydelay_tokens) + 1\n",
        "    num_decoder_tokens = len(keytext_tokens)\n",
        "\n",
        "    for d in datasets:\n",
        "        # train_df, test_df = np.split(d, [int(.9*len(d))])\n",
        "        keydelays = d['keydelay'].tolist()\n",
        "        keytexts = d['key2'].tolist()\n",
        "        keytexts = [START_TOKEN] + keytexts + [END_TOKEN]\n",
        "\n",
        "        keydelays = to_categorical(keydelays, num_classes=num_encoder_tokens)\n",
        "        keydelay_inputs.append(keydelays)\n",
        "        keytext_targets.append(keytexts)\n",
        "\n",
        "    max_encoder_seq_length = max([len(i) for i in keydelay_inputs])\n",
        "    max_decoder_seq_length = max([len(t) for t in keytext_targets])\n",
        "\n",
        "    print(\"Number of samples:\", len(keydelay_inputs))\n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "    input_token_index = dict([(char, i)\n",
        "                              for i, char in enumerate(keydelay_tokens)])\n",
        "    target_token_index = dict([(char, i)\n",
        "                               for i, char in enumerate(keytext_tokens)])\n",
        "\n",
        "    # one-hot encoding\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(keydelay_inputs), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(keydelay_inputs), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(keydelay_inputs), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    for i, (keydelay, keytext) in enumerate(zip(keydelay_inputs, keytext_targets)):\n",
        "        encoder_input_data[i, 0:len(keydelay)] = keydelay\n",
        "\n",
        "        for t, text in enumerate(keytext):\n",
        "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "            decoder_input_data[i, t, target_token_index[text]] = 1.0\n",
        "            if t > 0:\n",
        "                # decoder_target_data will be ahead by one timestep\n",
        "                # and will not include the start character.\n",
        "                decoder_target_data[i, t - 1, target_token_index[text]] = 1.0\n",
        "\n",
        "        # what's this for?\n",
        "        # i thinks this is to disregard the ' ' (on the tutorial they used target_token_index[' ']) so that the model won't predict it.\n",
        "        # decoder_input_data[i, t + 1:, target_token_index['a']] = 1.0\n",
        "        # decoder_target_data[i, t:, target_token_index['a']] = 1.0\n",
        "\n",
        "    # print(encoder_input_data)\n",
        "    print(encoder_input_data.shape)\n",
        "    # print(decoder_input_data)\n",
        "    print(decoder_input_data.shape)\n",
        "    # print(decoder_target_data)\n",
        "    print(decoder_target_data.shape)\n",
        "    # input()\n",
        "\n",
        "    # variables to be passed back for futher processing\n",
        "    data = (encoder_input_data, decoder_input_data, decoder_target_data)\n",
        "    tokens = (num_encoder_tokens, num_decoder_tokens)\n",
        "    sequences = (max_encoder_seq_length, max_decoder_seq_length)\n",
        "    indices = (input_token_index, target_token_index)\n",
        "\n",
        "    return data, tokens, sequences, indices\n",
        "\n",
        "def plot_training_performance(H):\n",
        "    # construct a plot that plots and saves the training history\n",
        "    N = np.arange(0, min(epochs, len(H.history[\"loss\"])))\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.savefig(f\"{model_path}/loss_plot.png\")\n",
        "\n",
        "    # construct a plot that plots and saves the training history\n",
        "    N = np.arange(0, min(epochs, len(H.history[\"accuracy\"])))\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, H.history[\"accuracy\"], label=\"train_accuracy\")\n",
        "    plt.plot(N, H.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.savefig(f\"{model_path}/acc_plot.png\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YbAPPotgMdCL"
      },
      "outputs": [],
      "source": [
        "def generator(X_data, y_data, batch_size):\n",
        "  samples_per_epoch = len(X_data)\n",
        "  number_of_batches = samples_per_epoch / batch_size\n",
        "  counter = 0\n",
        "\n",
        "  while True:\n",
        "    X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "    y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "    counter += 1\n",
        "    yield X_batch,y_batch\n",
        "\n",
        "    # restart counter to yield data in the next epoch as well\n",
        "    if counter >= number_of_batches:\n",
        "        counter = 0\n",
        "\n",
        "def train(train_datasets, test_datasets, keydelay_tokens, keytext_tokens):\n",
        "    data, tokens, sequences, indices = prepare(\n",
        "        train_datasets, keydelay_tokens, keytext_tokens)\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data = data\n",
        "    num_encoder_tokens, num_decoder_tokens = tokens\n",
        "    max_encoder_seq_length, max_decoder_seq_length = sequences\n",
        "    input_token_index, target_token_index = indices\n",
        "\n",
        "    model = build_models(num_encoder_tokens, num_decoder_tokens)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    overfitCallback = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
        "\n",
        "    history = model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[overfitCallback]\n",
        "    )\n",
        "\n",
        "    # history = model.fit_generator(\n",
        "    #     generator([encoder_input_data, decoder_input_data], decoder_target_data, batch_size),\n",
        "    #     epochs=epochs,\n",
        "    #     steps_per_epoch = len(train_datasets) / batch_size,\n",
        "    # )\n",
        "\n",
        "    # Save model\n",
        "    model.save(model_path)\n",
        "\n",
        "    # Plot training loss and accuracy\n",
        "    plot_training_performance(history)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA59tq1aM2nd"
      },
      "source": [
        "## Inference Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VskNqs_aMxEr"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/greatrxt/smartreply-word-level-seq2seq-with-beam-search\n",
        "def generate_beam_predictions(input_seq, n_next_sequences, beam_search_n, break_at_eos, models, tokens, indices):\n",
        "    model, encoder_model, decoder_model = models\n",
        "    num_encoder_tokens, num_decoder_tokens = tokens\n",
        "    input_token_index, target_token_index = indices\n",
        "\n",
        "    # Reverse-lookup token index to decode sequences back to\n",
        "    # something readable.\n",
        "    reverse_input_char_index = dict((i, char)\n",
        "                                    for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char)\n",
        "                                     for char, i in target_token_index.items())\n",
        "\n",
        "    distributions_scores_states = [[list(), 0.0, [None, None]]]\n",
        "    decoder_states_value = None\n",
        "\n",
        "    for _ in range(n_next_sequences):\n",
        "        sequence_temp_candidates = list()\n",
        "        for i in range(len(distributions_scores_states)):\n",
        "            # Generate empty target sequence of length 1.\n",
        "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "\n",
        "            seq, score, states_values = distributions_scores_states[i]\n",
        "\n",
        "            if len(distributions_scores_states) == 1:\n",
        "                # Encode the input as state vectors.\n",
        "                decoder_states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "                # Populate the first character of target sequence with the start character.\n",
        "                target_seq[0, 0, target_token_index[START_TOKEN]] = 1.0\n",
        "            else:\n",
        "                target_seq[0, 0, seq[-1]] = 1.0\n",
        "                decoder_states_value = states_values\n",
        "\n",
        "                candidate_sentence = \"\"\n",
        "                for token_index in seq:\n",
        "                    word = reverse_target_char_index[token_index]\n",
        "                    # print(\"Token Index\", token_index)\n",
        "                    # print(\"Char\", word)\n",
        "                    if word == END_TOKEN:\n",
        "                        break\n",
        "\n",
        "                    candidate_sentence += word\n",
        "\n",
        "                print(\"score :\", score, \" | \", candidate_sentence)\n",
        "\n",
        "            output_tokens_distribution, h, c = decoder_model.predict([target_seq] + decoder_states_value)\n",
        "            # print(\"OutputTokens\", output_tokens_distribution)\n",
        "\n",
        "            # Update states\n",
        "            decoder_states_value = [h, c]\n",
        "\n",
        "            predicted_distribution = output_tokens_distribution[0][-1]\n",
        "\n",
        "            for j in range(len(predicted_distribution)):\n",
        "                # print(\"J\", j)\n",
        "                # print(\"PREDIST\", predicted_distribution[j])\n",
        "                if predicted_distribution[j] > 0:\n",
        "                    candidate = [seq + [j], score - log(predicted_distribution[j]), decoder_states_value]\n",
        "                    # print(candidate)\n",
        "                    # input()\n",
        "                    if break_at_eos and j == END_TOKEN:\n",
        "                        continue\n",
        "                    else:\n",
        "                        sequence_temp_candidates.append(candidate)\n",
        "\n",
        "        # 2. score and sort all candidates\n",
        "        # print(\"SEQTEMP\", sequence_temp_candidates[:beam_search_n])\n",
        "        ordered = sorted(sequence_temp_candidates, key=lambda tup: tup[1], reverse=False) \n",
        "        # print(\"ORDERED\", ordered[:beam_search_n])\n",
        "        distributions_scores_states = ordered[:beam_search_n]\n",
        "        # print(\"DIST\", distributions_scores_states)\n",
        "        # input()\n",
        "        print(\"-----\")\n",
        "\n",
        "def decode_sequence(models, input_seq, num_decoder_tokens, max_decoder_seq_length, input_token_index, target_token_index):\n",
        "    model, encoder_model, decoder_model = models\n",
        "\n",
        "    # Reverse-lookup token index to decode sequences back to\n",
        "    # something readable.\n",
        "    reverse_input_char_index = dict((i, char)\n",
        "                                    for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char)\n",
        "                                     for char, i in target_token_index.items())\n",
        "\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[START_TOKEN]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        # print(output_tokens)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == END_TOKEN or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpVaJjspNHLL"
      },
      "source": [
        "## Inference Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XmScw3RrM5Hw"
      },
      "outputs": [],
      "source": [
        "def inference(test_datasets, keydelay_tokens, keytext_tokens):\n",
        "    models = load_models(model_path)\n",
        "    data, tokens, sequences, indices = prepare(\n",
        "        test_datasets, keydelay_tokens, keytext_tokens)\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data = data\n",
        "    num_encoder_tokens, num_decoder_tokens = tokens\n",
        "    max_encoder_seq_length, max_decoder_seq_length = sequences\n",
        "    input_token_index, target_token_index = indices\n",
        "\n",
        "    total, correct = len(test_datasets), 0\n",
        "    for i, d in enumerate(test_datasets):\n",
        "        # Take one sequence (part of the training set)\n",
        "        # for trying out decoding.\n",
        "        input_seq = encoder_input_data[i: i + 1]\n",
        "        target_text = d['key2'].tolist()\n",
        "        decoded_sentence = decode_sequence(\n",
        "            models, input_seq, num_decoder_tokens, max_decoder_seq_length, input_token_index, target_token_index)\n",
        "        print(\"-\")\n",
        "        print(\"Input sentence:\", target_text)\n",
        "        print(\"Decoded sentence:\", decoded_sentence)\n",
        "        if ''.join(target_text) == decoded_sentence.strip():\n",
        "            correct += 1\n",
        "            print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
        "\n",
        "    return\n",
        "\n",
        "def beam_search_inference(test_datasets, keydelay_tokens, keytext_tokens):\n",
        "    models = load_models(model_path)\n",
        "    data, tokens, sequences, indices = prepare(\n",
        "        test_datasets, keydelay_tokens, keytext_tokens)\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data = data\n",
        "\n",
        "    for i, d in enumerate(test_datasets):\n",
        "        # Take one sequence (part of the training set)\n",
        "        # for trying out decoding.\n",
        "        input_seq = encoder_input_data[i: i + 1]\n",
        "        target_text = d['key2'].tolist()\n",
        "        print(\"Input sentence:\", target_text)\n",
        "        generate_beam_predictions(input_seq, n_next_sequences=len(target_text) + 1, beam_search_n=10, break_at_eos=True, models=models, tokens=tokens, indices=indices)\n",
        "        input()\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12H_GmDXMlJR"
      },
      "source": [
        "## Dataset Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7WS6z4F-MZYZ"
      },
      "outputs": [],
      "source": [
        "def read_cmu_sessionmap():\n",
        "    data_path = \"./drive/MyDrive/S2/Thesis S2/CMU-SessionMap.txt\"\n",
        "    cols = [\"user\", \"session_index\", \"key\", \"value\"]\n",
        "    \n",
        "    with open(data_path, \"r\") as f:\n",
        "        raw_data = f.read().splitlines()\n",
        "        data = [shlex.split(r) for r in raw_data]\n",
        "        df = pd.DataFrame(data[1:], columns=cols)\n",
        "        return df\n",
        "\n",
        "def split_dataset(df, datasets):\n",
        "    # shuffle the datasets\n",
        "    random.shuffle(datasets)\n",
        "\n",
        "    # split to train/test dataset with 80:20 ratio.\n",
        "    train_datasets = datasets[:int(.80*len(datasets))]\n",
        "    test_datasets = datasets[:int(.20*len(datasets))]\n",
        "\n",
        "    keytext_tokens = df['key2'].unique()\n",
        "    keydelay_tokens = df['keydelay'].unique()\n",
        "\n",
        "    df = None\n",
        "    return train_datasets, test_datasets, keytext_tokens, keydelay_tokens\n",
        "\n",
        "def read_villani_dataset():\n",
        "    data_path = \"./drive/MyDrive/S2/Thesis S2/villani_keystrokes.csv\"\n",
        "    df = pd.read_csv(data_path, header=0)\n",
        "    df['keydelay'] = df[\"timerelease\"] - df[\"timepress\"]\n",
        "    df['key2'] = df[\"keyname\"]\n",
        "\n",
        "    return df\n",
        "  \n",
        "def read_cmu_dataset():\n",
        "    data_path = \"./drive/MyDrive/S2/Thesis S2/CMU-TimingFeatures-DD.txt\"\n",
        "    cols = [\"subject_id\", \"session_index\", \"exercise_index\", \"digraph_index\", \"key1\", \"key2\", \"keydelay\"]\n",
        "\n",
        "    with open(data_path, \"r\") as f:\n",
        "        raw_data = f.read().splitlines()\n",
        "        data = [r.split() for r in raw_data]\n",
        "        df = pd.DataFrame(data[1:], columns=cols)\n",
        "        return df\n",
        "\n",
        "def split_words_villani(df):\n",
        "    chunk = []\n",
        "    for i, d in df.iterrows():\n",
        "        if len(d['keyname']) > 1:\n",
        "            if len(chunk) > 1:\n",
        "                chunk_df = pd.DataFrame(chunk)\n",
        "                yield chunk_df\n",
        "                chunk = []\n",
        "        else:\n",
        "            chunk.append(d.to_dict())\n",
        "\n",
        "def split_words_cmu(df):\n",
        "    chunk = []\n",
        "    for i, d in df.iterrows():\n",
        "        if len(d['key2']) > 1:\n",
        "            if len(chunk) > 1:\n",
        "                chunk_df = pd.DataFrame(chunk)\n",
        "                yield chunk_df\n",
        "                chunk = []\n",
        "        else:\n",
        "            chunk.append(d.to_dict())\n",
        "\n",
        "def clean_dataset_villani(df):\n",
        "    prune = [\"inputtype\",\"repetition\",\"platform\",\"gender\",\"agegroup\",\"handedness\",\"awareness\",\"location\"]\n",
        "    for p in prune:\n",
        "        if p in df.columns:\n",
        "            df.drop(p, axis=1, inplace=True)\n",
        "\n",
        "def create_directory(directory):\n",
        "  if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "def save_processed_dataset(data):\n",
        "  path = f\"{model_path}/camstroke-inference-dataset.dat\"\n",
        "  create_directory(model_path)\n",
        "  with open(path, \"wb\") as f:\n",
        "        return pickle.dump(data, f)\n",
        "\n",
        "def load_processed_dataset(data):\n",
        "    path = f\"{model_path}/camstroke-inference-dataset.dat\"\n",
        "    create_directory(model_path)\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def read_rockyou_wordlist(n_data=50000):\n",
        "    data_path = \"./drive/MyDrive/S2/Thesis S2/rockyou.txt\"\n",
        "    with open(data_path, 'r', encoding='latin-1') as f:\n",
        "        data = [r for r in f.read().splitlines()]\n",
        "        if n_data > 0:\n",
        "            data = data[:n_data] # Top n only\n",
        "        df = pd.Series(data)\n",
        "        df.columns = ['password']\n",
        "        print(df.head())\n",
        "\n",
        "    return df\n",
        "\n",
        "def dfword_to_gram(df, n):\n",
        "    for i in range(0, df.shape[0], n):\n",
        "        yield df[max(0, i-1):i+n]\n",
        "    # return np.array_split(df, n)\n",
        "  \n",
        "def make_balanced_dataset(datasets, word_dictionary, min_sample=1000):\n",
        "    balanced_dataset = []\n",
        "    for d in datasets:\n",
        "        for key, value in d.items():\n",
        "            if len(value) >= min_sample:\n",
        "                balanced_dataset += value[:min_sample]\n",
        "    \n",
        "    return balanced_dataset\n",
        "\n",
        "def _make_balanced_dataset_old(datasets, word_dictionary, min_sample=1000):\n",
        "    vc = pd.value_counts(np.array(word_dictionary))\n",
        "    print(vc)\n",
        "    for col_name, data in vc[(vc < min_sample)].items():\n",
        "      # print(col_name, data, 'hehe')\n",
        "      indices = [i for i, x in enumerate(word_dictionary) if x == col_name]\n",
        "      for i, x in enumerate(indices):\n",
        "        x = x - i\n",
        "        word_dictionary.pop(x)\n",
        "        datasets.pop(x)\n",
        "    vc = pd.value_counts(np.array(word_dictionary))\n",
        "    print(vc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Vc3dULOGQT"
      },
      "source": [
        "# Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b5pdnRD1RYjN"
      },
      "outputs": [],
      "source": [
        "model_path = \"./drive/MyDrive/S2/camstroke-inference-models/s2s_model_beamsearch_CMUandVillani_v1\"  # Path to store/load the model\n",
        "MIN_CHUNK = 6 # minimum word length to be inserted into the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q2ju8IEzOyaO"
      },
      "outputs": [],
      "source": [
        "def prepare_villani_dataset(mode=\"Free\"):\n",
        "    print(\"Reading and Preparing Villani Dataset...\")\n",
        "    df = read_villani_dataset()\n",
        "    df.loc[df['keydelay'] >= 1000, 'keydelay'] = 1000\n",
        "\n",
        "    # wl = read_rockyou_wordlist(n_data=100000)\n",
        "\n",
        "    if mode == \"Free\":\n",
        "      # freetext-mode only\n",
        "      df = df[df[\"inputtype\"] == \"free\"]\n",
        "    elif mode == \"Fixed\":\n",
        "      # fixed-text only\n",
        "      df = df[df[\"inputtype\"] == \"fixed\"]\n",
        "    else:\n",
        "      # all-text mode\n",
        "      df = df[(df[\"inputtype\"] == \"fixed\") & (df[\"inputtype\"] == \"free\")]\n",
        "\n",
        "    clean_dataset_villani(df)\n",
        "    subjects = df.groupby('user')\n",
        "    datasets = defaultdict(list)\n",
        "    word_dictionary = []\n",
        "\n",
        "    for i, (subject_id, subject) in enumerate(subjects):\n",
        "        sessions = subject.groupby(\"session\")\n",
        "        for sess_id, session in sessions:\n",
        "            exercises = session.groupby(\"task\")\n",
        "            for exercise_id, exercise in exercises:\n",
        "                print(f\"Subject {i + 1}/{len(subjects)} | Session ID: {sess_id} | Exercise ID: {exercise_id}\")\n",
        "                print(\"Current Dataset Length: \", len(datasets))\n",
        "                for exerchunk in split_words_villani(exercise):\n",
        "                    word_chunk = \"\".join(exerchunk['key2'].tolist())\n",
        "                    \n",
        "                    # Mode 1: Get all word combinations\n",
        "                    # print(word_chunk)\n",
        "                    if len(word_chunk) < MIN_CHUNK:\n",
        "                        continue\n",
        "\n",
        "                    datasets[word_chunk].append(exerchunk)\n",
        "                    word_dictionary.append(word_chunk)\n",
        "\n",
        "                    # Mode 2: Isolate to RockYou wordlist only\n",
        "                    # if word_chunk in wl.values:\n",
        "                    #     word_dictionary.append(word_chunk)\n",
        "                    #     datasets.append(exerchunk)\n",
        "                        # for gram in dfword_to_gram(exerchunk, 3):\n",
        "                        #     # print(gram.head())\n",
        "                        #     datasets.append(gram)\n",
        "\n",
        "    return df, datasets, word_dictionary\n",
        "\n",
        "def prepare_cmu_dataset(mode=\"Free\"):\n",
        "    print(\"Reading and Preparing CMU Dataset...\")\n",
        "    df = read_cmu_dataset()\n",
        "    df['keydelay'] = df['keydelay'].apply(lambda x: int(float(x) * 1000)) # convert to miliseconds\n",
        "    df.loc[df['keydelay'] >= 1000, 'keydelay'] = 1000\n",
        "\n",
        "    sessionmap = read_cmu_sessionmap()\n",
        "\n",
        "    gss = GroupShuffleSplit(n_splits=2, train_size=.8, random_state=42)\n",
        "    split = gss.split(df, groups=df[\"session_index\"])\n",
        "    train_indices, test_indices = next(split)\n",
        "    train_df = df.iloc[train_indices]\n",
        "    test_df = df.iloc[test_indices]\n",
        "\n",
        "    subjects = df.groupby('subject_id')\n",
        "    datasets = defaultdict(list)\n",
        "    word_dictionary = []\n",
        "\n",
        "    for subject_id, subject in subjects:\n",
        "        sessions = subject.groupby(\"session_index\")\n",
        "        for sess_id, session in sessions:\n",
        "            smap = sessionmap[(sessionmap[\"user\"] == subject_id) & (sessionmap[\"session_index\"] == sess_id)]\n",
        "            smap_value = smap[\"value\"].values[0]\n",
        "            \n",
        "            exercises = session.groupby(\"exercise_index\")\n",
        "            for exercise_id, exercise in exercises:\n",
        "                text_mode = smap_value.split(\" - \")[2]\n",
        "\n",
        "                if mode == \"Free\":\n",
        "                    if text_mode != \"Free\":\n",
        "                        continue\n",
        "                elif mode == \"Fixed\":\n",
        "                    if text_mode != \"Trans\":\n",
        "                        continue\n",
        "                \n",
        "                print(f\"Subject ID: {subject_id} | Session ID: {sess_id} | Session Name: {smap_value} | Exercise ID: {exercise_id} | Mode: {text_mode}\")\n",
        "                print(\"Current Dataset Length: \", len(datasets))\n",
        "                for exerchunk in split_words_cmu(exercise):\n",
        "                    word_chunk = \"\".join(exerchunk['key2'].tolist())\n",
        "                    \n",
        "                    # Mode 1: Get all word combinations\n",
        "                    # print(word_chunk)\n",
        "                    if len(word_chunk) < MIN_CHUNK:\n",
        "                        continue\n",
        "\n",
        "                    datasets[word_chunk].append(exerchunk)\n",
        "                    word_dictionary.append(word_chunk)\n",
        "\n",
        "                    # Mode 2: Isolate to RockYou wordlist only\n",
        "                    # if word_chunk in wl.values:\n",
        "                    #     word_dictionary.append(word_chunk)\n",
        "                    #     datasets.append(exerchunk)\n",
        "                        # for gram in dfword_to_gram(exerchunk, 3):\n",
        "                        #     # print(gram.head())\n",
        "                        #     datasets.append(gram)\n",
        "\n",
        "    return df, datasets, word_dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the datasets (from Villani and CMU, combined)."
      ],
      "metadata": {
        "id": "rCvZMHDcfDXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1, dataset1, word_dictionary1 = prepare_villani_dataset(mode=\"Free\")\n",
        "df2, dataset2, word_dictionary2 = prepare_cmu_dataset(mode=\"Free\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zavIdxj7fDDm",
        "outputId": "7442c67a-0c98-4633-b905-c3c5f88bc1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading and Preparing Villani Dataset...\n",
            "Subject 1/139 | Session ID: 447 | Exercise ID: freetext1\n",
            "Current Dataset Length:  0\n",
            "Subject 1/139 | Session ID: 558 | Exercise ID: freetext4\n",
            "Current Dataset Length:  31\n",
            "Subject 1/139 | Session ID: 1134 | Exercise ID: freetext4\n",
            "Current Dataset Length:  53\n",
            "Subject 1/139 | Session ID: 1288 | Exercise ID: freetext4\n",
            "Current Dataset Length:  77\n",
            "Subject 1/139 | Session ID: 1473 | Exercise ID: freetext4\n",
            "Current Dataset Length:  101\n",
            "Subject 2/139 | Session ID: 237 | Exercise ID: freetext1\n",
            "Current Dataset Length:  119\n",
            "Subject 2/139 | Session ID: 832 | Exercise ID: freetext1\n",
            "Current Dataset Length:  137\n",
            "Subject 2/139 | Session ID: 900 | Exercise ID: freetext1\n",
            "Current Dataset Length:  158\n",
            "Subject 2/139 | Session ID: 969 | Exercise ID: freetext1\n",
            "Current Dataset Length:  173\n",
            "Subject 3/139 | Session ID: 292 | Exercise ID: freetext7\n",
            "Current Dataset Length:  194\n",
            "Subject 3/139 | Session ID: 428 | Exercise ID: freetext7\n",
            "Current Dataset Length:  216\n",
            "Subject 3/139 | Session ID: 1108 | Exercise ID: freetext7\n",
            "Current Dataset Length:  239\n",
            "Subject 3/139 | Session ID: 1412 | Exercise ID: freetext7\n",
            "Current Dataset Length:  258\n",
            "Subject 3/139 | Session ID: 1724 | Exercise ID: freetext7\n",
            "Current Dataset Length:  272\n",
            "Subject 4/139 | Session ID: 8 | Exercise ID: freetext5\n",
            "Current Dataset Length:  290\n",
            "Subject 4/139 | Session ID: 408 | Exercise ID: freetext1\n",
            "Current Dataset Length:  311\n",
            "Subject 4/139 | Session ID: 526 | Exercise ID: freetext5\n",
            "Current Dataset Length:  332\n",
            "Subject 4/139 | Session ID: 645 | Exercise ID: freetext5\n",
            "Current Dataset Length:  349\n",
            "Subject 4/139 | Session ID: 855 | Exercise ID: freetext1\n",
            "Current Dataset Length:  370\n",
            "Subject 4/139 | Session ID: 879 | Exercise ID: freetext1\n",
            "Current Dataset Length:  385\n",
            "Subject 4/139 | Session ID: 1094 | Exercise ID: freetext1\n",
            "Current Dataset Length:  397\n",
            "Subject 4/139 | Session ID: 1668 | Exercise ID: freetext1\n",
            "Current Dataset Length:  420\n",
            "Subject 4/139 | Session ID: 1690 | Exercise ID: freetext5\n",
            "Current Dataset Length:  431\n",
            "Subject 5/139 | Session ID: 40 | Exercise ID: freetext5\n",
            "Current Dataset Length:  450\n",
            "Subject 5/139 | Session ID: 300 | Exercise ID: freetext5\n",
            "Current Dataset Length:  464\n",
            "Subject 5/139 | Session ID: 480 | Exercise ID: freetext5\n",
            "Current Dataset Length:  482\n",
            "Subject 5/139 | Session ID: 761 | Exercise ID: freetext5\n",
            "Current Dataset Length:  502\n",
            "Subject 5/139 | Session ID: 1628 | Exercise ID: freetext5\n",
            "Current Dataset Length:  526\n",
            "Subject 6/139 | Session ID: 740 | Exercise ID: freetext1\n",
            "Current Dataset Length:  540\n",
            "Subject 6/139 | Session ID: 1295 | Exercise ID: freetext1\n",
            "Current Dataset Length:  560\n",
            "Subject 6/139 | Session ID: 1335 | Exercise ID: freetext1\n",
            "Current Dataset Length:  577\n",
            "Subject 6/139 | Session ID: 1435 | Exercise ID: freetext1\n",
            "Current Dataset Length:  597\n",
            "Subject 6/139 | Session ID: 1562 | Exercise ID: freetext1\n",
            "Current Dataset Length:  609\n",
            "Subject 7/139 | Session ID: 33 | Exercise ID: freetext10\n",
            "Current Dataset Length:  623\n",
            "Subject 7/139 | Session ID: 42 | Exercise ID: freetext10\n",
            "Current Dataset Length:  653\n",
            "Subject 7/139 | Session ID: 306 | Exercise ID: freetext10\n",
            "Current Dataset Length:  663\n",
            "Subject 7/139 | Session ID: 376 | Exercise ID: freetext10\n",
            "Current Dataset Length:  675\n",
            "Subject 7/139 | Session ID: 816 | Exercise ID: freetext10\n",
            "Current Dataset Length:  689\n",
            "Subject 7/139 | Session ID: 1171 | Exercise ID: freetext10\n",
            "Current Dataset Length:  732\n",
            "Subject 7/139 | Session ID: 1224 | Exercise ID: freetext10\n",
            "Current Dataset Length:  743\n",
            "Subject 8/139 | Session ID: 412 | Exercise ID: freetext8\n",
            "Current Dataset Length:  769\n",
            "Subject 8/139 | Session ID: 504 | Exercise ID: freetext6\n",
            "Current Dataset Length:  781\n",
            "Subject 8/139 | Session ID: 538 | Exercise ID: freetext1\n",
            "Current Dataset Length:  790\n",
            "Subject 8/139 | Session ID: 629 | Exercise ID: freetext2\n",
            "Current Dataset Length:  798\n",
            "Subject 8/139 | Session ID: 911 | Exercise ID: freetext4\n",
            "Current Dataset Length:  814\n",
            "Subject 9/139 | Session ID: 324 | Exercise ID: freetext2\n",
            "Current Dataset Length:  821\n",
            "Subject 9/139 | Session ID: 327 | Exercise ID: freetext10\n",
            "Current Dataset Length:  854\n",
            "Subject 9/139 | Session ID: 897 | Exercise ID: freetext6\n",
            "Current Dataset Length:  883\n",
            "Subject 9/139 | Session ID: 940 | Exercise ID: freetext2\n",
            "Current Dataset Length:  904\n",
            "Subject 9/139 | Session ID: 1165 | Exercise ID: freetext2\n",
            "Current Dataset Length:  929\n",
            "Subject 9/139 | Session ID: 1328 | Exercise ID: freetext6\n",
            "Current Dataset Length:  949\n",
            "Subject 9/139 | Session ID: 1349 | Exercise ID: freetext10\n",
            "Current Dataset Length:  967\n",
            "Subject 9/139 | Session ID: 1376 | Exercise ID: freetext10\n",
            "Current Dataset Length:  984\n",
            "Subject 10/139 | Session ID: 114 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1003\n",
            "Subject 10/139 | Session ID: 737 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1023\n",
            "Subject 10/139 | Session ID: 818 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1028\n",
            "Subject 10/139 | Session ID: 859 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1034\n",
            "Subject 10/139 | Session ID: 1715 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1047\n",
            "Subject 11/139 | Session ID: 145 | Exercise ID: freetext4\n",
            "Current Dataset Length:  1057\n",
            "Subject 11/139 | Session ID: 202 | Exercise ID: freetext4\n",
            "Current Dataset Length:  1079\n",
            "Subject 11/139 | Session ID: 213 | Exercise ID: freetext5\n",
            "Current Dataset Length:  1100\n",
            "Subject 11/139 | Session ID: 269 | Exercise ID: freetext4\n",
            "Current Dataset Length:  1118\n",
            "Subject 11/139 | Session ID: 319 | Exercise ID: freetext3\n",
            "Current Dataset Length:  1137\n",
            "Subject 11/139 | Session ID: 339 | Exercise ID: freetext4\n",
            "Current Dataset Length:  1158\n",
            "Subject 11/139 | Session ID: 574 | Exercise ID: freetext4\n",
            "Current Dataset Length:  1179\n",
            "Subject 11/139 | Session ID: 762 | Exercise ID: freetext4\n",
            "Current Dataset Length:  1190\n",
            "Subject 11/139 | Session ID: 815 | Exercise ID: freetext7\n",
            "Current Dataset Length:  1207\n",
            "Subject 11/139 | Session ID: 877 | Exercise ID: freetext2\n",
            "Current Dataset Length:  1224\n",
            "Subject 11/139 | Session ID: 883 | Exercise ID: freetext8\n",
            "Current Dataset Length:  1241\n",
            "Subject 11/139 | Session ID: 1060 | Exercise ID: freetext4\n",
            "Current Dataset Length:  1260\n",
            "Subject 11/139 | Session ID: 1085 | Exercise ID: freetext4\n",
            "Current Dataset Length:  1273\n",
            "Subject 11/139 | Session ID: 1294 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1288\n",
            "Subject 11/139 | Session ID: 1352 | Exercise ID: freetext10\n",
            "Current Dataset Length:  1299\n",
            "Subject 11/139 | Session ID: 1518 | Exercise ID: freetext9\n",
            "Current Dataset Length:  1324\n",
            "Subject 11/139 | Session ID: 1586 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1348\n",
            "Subject 11/139 | Session ID: 1632 | Exercise ID: freetext4\n",
            "Current Dataset Length:  1367\n",
            "Subject 11/139 | Session ID: 1677 | Exercise ID: freetext4\n",
            "Current Dataset Length:  1379\n",
            "Subject 11/139 | Session ID: 1729 | Exercise ID: freetext4\n",
            "Current Dataset Length:  1391\n",
            "Subject 12/139 | Session ID: 945 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1402\n",
            "Subject 13/139 | Session ID: 252 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1421\n",
            "Subject 13/139 | Session ID: 431 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1441\n",
            "Subject 13/139 | Session ID: 527 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1451\n",
            "Subject 13/139 | Session ID: 679 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1465\n",
            "Subject 13/139 | Session ID: 688 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1477\n",
            "Subject 13/139 | Session ID: 736 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1488\n",
            "Subject 13/139 | Session ID: 961 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1500\n",
            "Subject 13/139 | Session ID: 1135 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1528\n",
            "Subject 13/139 | Session ID: 1172 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1540\n",
            "Subject 13/139 | Session ID: 1268 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1566\n",
            "Subject 13/139 | Session ID: 1463 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1572\n",
            "Subject 13/139 | Session ID: 1537 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1601\n",
            "Subject 13/139 | Session ID: 1645 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1614\n",
            "Subject 13/139 | Session ID: 1661 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1634\n",
            "Subject 13/139 | Session ID: 1703 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1658\n",
            "Subject 14/139 | Session ID: 208 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1687\n",
            "Subject 14/139 | Session ID: 366 | Exercise ID: freetext2\n",
            "Current Dataset Length:  1698\n",
            "Subject 14/139 | Session ID: 1081 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1706\n",
            "Subject 14/139 | Session ID: 1095 | Exercise ID: freetext2\n",
            "Current Dataset Length:  1718\n",
            "Subject 14/139 | Session ID: 1184 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1729\n",
            "Subject 14/139 | Session ID: 1311 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1739\n",
            "Subject 14/139 | Session ID: 1563 | Exercise ID: freetext2\n",
            "Current Dataset Length:  1747\n",
            "Subject 14/139 | Session ID: 1693 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1752\n",
            "Subject 15/139 | Session ID: 65 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1762\n",
            "Subject 15/139 | Session ID: 82 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1771\n",
            "Subject 15/139 | Session ID: 417 | Exercise ID: freetext7\n",
            "Current Dataset Length:  1779\n",
            "Subject 15/139 | Session ID: 586 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1788\n",
            "Subject 15/139 | Session ID: 731 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1800\n",
            "Subject 15/139 | Session ID: 1063 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1810\n",
            "Subject 15/139 | Session ID: 1076 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1819\n",
            "Subject 15/139 | Session ID: 1131 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1833\n",
            "Subject 15/139 | Session ID: 1652 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1837\n",
            "Subject 16/139 | Session ID: 9 | Exercise ID: freetext9\n",
            "Current Dataset Length:  1849\n",
            "Subject 16/139 | Session ID: 25 | Exercise ID: freetext9\n",
            "Current Dataset Length:  1865\n",
            "Subject 16/139 | Session ID: 473 | Exercise ID: freetext9\n",
            "Current Dataset Length:  1881\n",
            "Subject 16/139 | Session ID: 521 | Exercise ID: freetext9\n",
            "Current Dataset Length:  1897\n",
            "Subject 16/139 | Session ID: 646 | Exercise ID: freetext9\n",
            "Current Dataset Length:  1912\n",
            "Subject 17/139 | Session ID: 285 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1932\n",
            "Subject 17/139 | Session ID: 696 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1940\n",
            "Subject 17/139 | Session ID: 948 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1962\n",
            "Subject 17/139 | Session ID: 1002 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1977\n",
            "Subject 17/139 | Session ID: 1124 | Exercise ID: freetext1\n",
            "Current Dataset Length:  1983\n",
            "Subject 18/139 | Session ID: 185 | Exercise ID: freetext6\n",
            "Current Dataset Length:  1993\n",
            "Subject 18/139 | Session ID: 354 | Exercise ID: freetext9\n",
            "Current Dataset Length:  2008\n",
            "Subject 18/139 | Session ID: 690 | Exercise ID: freetext5\n",
            "Current Dataset Length:  2023\n",
            "Subject 18/139 | Session ID: 1122 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2051\n",
            "Subject 18/139 | Session ID: 1127 | Exercise ID: freetext4\n",
            "Current Dataset Length:  2062\n",
            "Subject 19/139 | Session ID: 210 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2078\n",
            "Subject 19/139 | Session ID: 1191 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2093\n",
            "Subject 19/139 | Session ID: 1247 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2102\n",
            "Subject 19/139 | Session ID: 1418 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2110\n",
            "Subject 19/139 | Session ID: 1461 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2120\n",
            "Subject 20/139 | Session ID: 359 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2126\n",
            "Subject 20/139 | Session ID: 552 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2139\n",
            "Subject 20/139 | Session ID: 728 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2154\n",
            "Subject 20/139 | Session ID: 975 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2167\n",
            "Subject 21/139 | Session ID: 16 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2189\n",
            "Subject 21/139 | Session ID: 231 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2199\n",
            "Subject 21/139 | Session ID: 234 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2209\n",
            "Subject 21/139 | Session ID: 241 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2222\n",
            "Subject 21/139 | Session ID: 335 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2241\n",
            "Subject 21/139 | Session ID: 381 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2253\n",
            "Subject 21/139 | Session ID: 491 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2266\n",
            "Subject 21/139 | Session ID: 509 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2279\n",
            "Subject 21/139 | Session ID: 626 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2292\n",
            "Subject 21/139 | Session ID: 641 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2306\n",
            "Subject 21/139 | Session ID: 786 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2321\n",
            "Subject 21/139 | Session ID: 860 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2331\n",
            "Subject 21/139 | Session ID: 933 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2342\n",
            "Subject 21/139 | Session ID: 994 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2359\n",
            "Subject 21/139 | Session ID: 1019 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2370\n",
            "Subject 21/139 | Session ID: 1057 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2379\n",
            "Subject 21/139 | Session ID: 1084 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2395\n",
            "Subject 21/139 | Session ID: 1093 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2407\n",
            "Subject 21/139 | Session ID: 1103 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2418\n",
            "Subject 21/139 | Session ID: 1106 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2433\n",
            "Subject 21/139 | Session ID: 1214 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2445\n",
            "Subject 21/139 | Session ID: 1260 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2458\n",
            "Subject 21/139 | Session ID: 1308 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2464\n",
            "Subject 21/139 | Session ID: 1423 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2473\n",
            "Subject 21/139 | Session ID: 1474 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2485\n",
            "Subject 21/139 | Session ID: 1605 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2503\n",
            "Subject 21/139 | Session ID: 1662 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2515\n",
            "Subject 21/139 | Session ID: 1676 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2531\n",
            "Subject 21/139 | Session ID: 1713 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2539\n",
            "Subject 22/139 | Session ID: 26 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2549\n",
            "Subject 22/139 | Session ID: 54 | Exercise ID: freetext9\n",
            "Current Dataset Length:  2571\n",
            "Subject 22/139 | Session ID: 224 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2595\n",
            "Subject 22/139 | Session ID: 229 | Exercise ID: freetext10\n",
            "Current Dataset Length:  2611\n",
            "Subject 22/139 | Session ID: 235 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2628\n",
            "Subject 22/139 | Session ID: 278 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2650\n",
            "Subject 22/139 | Session ID: 387 | Exercise ID: freetext9\n",
            "Current Dataset Length:  2666\n",
            "Subject 22/139 | Session ID: 432 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2688\n",
            "Subject 22/139 | Session ID: 456 | Exercise ID: freetext7\n",
            "Current Dataset Length:  2699\n",
            "Subject 22/139 | Session ID: 478 | Exercise ID: freetext3\n",
            "Current Dataset Length:  2710\n",
            "Subject 22/139 | Session ID: 518 | Exercise ID: freetext9\n",
            "Current Dataset Length:  2732\n",
            "Subject 22/139 | Session ID: 653 | Exercise ID: freetext8\n",
            "Current Dataset Length:  2759\n",
            "Subject 22/139 | Session ID: 655 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2779\n",
            "Subject 22/139 | Session ID: 704 | Exercise ID: freetext5\n",
            "Current Dataset Length:  2805\n",
            "Subject 22/139 | Session ID: 743 | Exercise ID: freetext9\n",
            "Current Dataset Length:  2814\n",
            "Subject 22/139 | Session ID: 769 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2836\n",
            "Subject 22/139 | Session ID: 851 | Exercise ID: freetext7\n",
            "Current Dataset Length:  2850\n",
            "Subject 22/139 | Session ID: 1006 | Exercise ID: freetext4\n",
            "Current Dataset Length:  2868\n",
            "Subject 22/139 | Session ID: 1023 | Exercise ID: freetext4\n",
            "Current Dataset Length:  2887\n",
            "Subject 22/139 | Session ID: 1039 | Exercise ID: freetext9\n",
            "Current Dataset Length:  2899\n",
            "Subject 22/139 | Session ID: 1116 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2913\n",
            "Subject 22/139 | Session ID: 1130 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2925\n",
            "Subject 22/139 | Session ID: 1147 | Exercise ID: freetext6\n",
            "Current Dataset Length:  2943\n",
            "Subject 22/139 | Session ID: 1272 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2959\n",
            "Subject 22/139 | Session ID: 1277 | Exercise ID: freetext5\n",
            "Current Dataset Length:  2969\n",
            "Subject 22/139 | Session ID: 1310 | Exercise ID: freetext1\n",
            "Current Dataset Length:  2994\n",
            "Subject 22/139 | Session ID: 1324 | Exercise ID: freetext6\n",
            "Current Dataset Length:  3015\n",
            "Subject 22/139 | Session ID: 1357 | Exercise ID: freetext10\n",
            "Current Dataset Length:  3040\n",
            "Subject 22/139 | Session ID: 1409 | Exercise ID: freetext2\n",
            "Current Dataset Length:  3060\n",
            "Subject 22/139 | Session ID: 1454 | Exercise ID: freetext2\n",
            "Current Dataset Length:  3077\n",
            "Subject 22/139 | Session ID: 1515 | Exercise ID: freetext3\n",
            "Current Dataset Length:  3087\n",
            "Subject 22/139 | Session ID: 1566 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3106\n",
            "Subject 22/139 | Session ID: 1567 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3126\n",
            "Subject 22/139 | Session ID: 1597 | Exercise ID: freetext6\n",
            "Current Dataset Length:  3138\n",
            "Subject 22/139 | Session ID: 1647 | Exercise ID: freetext3\n",
            "Current Dataset Length:  3151\n",
            "Subject 22/139 | Session ID: 1698 | Exercise ID: freetext9\n",
            "Current Dataset Length:  3165\n",
            "Subject 23/139 | Session ID: 747 | Exercise ID: freetext4\n",
            "Current Dataset Length:  3181\n",
            "Subject 23/139 | Session ID: 858 | Exercise ID: freetext4\n",
            "Current Dataset Length:  3199\n",
            "Subject 23/139 | Session ID: 977 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3209\n",
            "Subject 23/139 | Session ID: 1182 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3216\n",
            "Subject 23/139 | Session ID: 1217 | Exercise ID: freetext4\n",
            "Current Dataset Length:  3229\n",
            "Subject 23/139 | Session ID: 1257 | Exercise ID: freetext4\n",
            "Current Dataset Length:  3239\n",
            "Subject 23/139 | Session ID: 1367 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3263\n",
            "Subject 23/139 | Session ID: 1731 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3274\n",
            "Subject 24/139 | Session ID: 597 | Exercise ID: freetext6\n",
            "Current Dataset Length:  3287\n",
            "Subject 25/139 | Session ID: 59 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3301\n",
            "Subject 25/139 | Session ID: 472 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3309\n",
            "Subject 25/139 | Session ID: 929 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3318\n",
            "Subject 25/139 | Session ID: 1099 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3325\n",
            "Subject 25/139 | Session ID: 1102 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3331\n",
            "Subject 26/139 | Session ID: 592 | Exercise ID: freetext8\n",
            "Current Dataset Length:  3350\n",
            "Subject 26/139 | Session ID: 1702 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3368\n",
            "Subject 27/139 | Session ID: 146 | Exercise ID: freetext4\n",
            "Current Dataset Length:  3386\n",
            "Subject 27/139 | Session ID: 225 | Exercise ID: freetext10\n",
            "Current Dataset Length:  3398\n",
            "Subject 27/139 | Session ID: 618 | Exercise ID: freetext6\n",
            "Current Dataset Length:  3412\n",
            "Subject 27/139 | Session ID: 1194 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3431\n",
            "Subject 27/139 | Session ID: 1326 | Exercise ID: freetext9\n",
            "Current Dataset Length:  3441\n",
            "Subject 27/139 | Session ID: 1455 | Exercise ID: freetext4\n",
            "Current Dataset Length:  3463\n",
            "Subject 27/139 | Session ID: 1532 | Exercise ID: freetext6\n",
            "Current Dataset Length:  3483\n",
            "Subject 27/139 | Session ID: 1659 | Exercise ID: freetext8\n",
            "Current Dataset Length:  3508\n",
            "Subject 28/139 | Session ID: 196 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3528\n",
            "Subject 28/139 | Session ID: 340 | Exercise ID: freetext6\n",
            "Current Dataset Length:  3543\n",
            "Subject 28/139 | Session ID: 755 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3554\n",
            "Subject 28/139 | Session ID: 920 | Exercise ID: freetext9\n",
            "Current Dataset Length:  3565\n",
            "Subject 29/139 | Session ID: 128 | Exercise ID: freetext6\n",
            "Current Dataset Length:  3577\n",
            "Subject 29/139 | Session ID: 683 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3584\n",
            "Subject 29/139 | Session ID: 819 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3590\n",
            "Subject 29/139 | Session ID: 905 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3595\n",
            "Subject 29/139 | Session ID: 1009 | Exercise ID: freetext10\n",
            "Current Dataset Length:  3606\n",
            "Subject 29/139 | Session ID: 1048 | Exercise ID: freetext4\n",
            "Current Dataset Length:  3615\n",
            "Subject 29/139 | Session ID: 1118 | Exercise ID: freetext3\n",
            "Current Dataset Length:  3628\n",
            "Subject 29/139 | Session ID: 1396 | Exercise ID: freetext2\n",
            "Current Dataset Length:  3637\n",
            "Subject 29/139 | Session ID: 1578 | Exercise ID: freetext7\n",
            "Current Dataset Length:  3644\n",
            "Subject 29/139 | Session ID: 1669 | Exercise ID: freetext8\n",
            "Current Dataset Length:  3650\n",
            "Subject 30/139 | Session ID: 261 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3661\n",
            "Subject 30/139 | Session ID: 348 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3670\n",
            "Subject 30/139 | Session ID: 393 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3677\n",
            "Subject 30/139 | Session ID: 460 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3678\n",
            "Subject 30/139 | Session ID: 1283 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3680\n",
            "Subject 31/139 | Session ID: 2 | Exercise ID: freetext3\n",
            "Current Dataset Length:  3686\n",
            "Subject 31/139 | Session ID: 4 | Exercise ID: freetext4\n",
            "Current Dataset Length:  3699\n",
            "Subject 31/139 | Session ID: 94 | Exercise ID: freetext7\n",
            "Current Dataset Length:  3715\n",
            "Subject 31/139 | Session ID: 357 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3729\n",
            "Subject 31/139 | Session ID: 596 | Exercise ID: freetext6\n",
            "Current Dataset Length:  3742\n",
            "Subject 31/139 | Session ID: 729 | Exercise ID: freetext9\n",
            "Current Dataset Length:  3758\n",
            "Subject 31/139 | Session ID: 895 | Exercise ID: freetext5\n",
            "Current Dataset Length:  3770\n",
            "Subject 31/139 | Session ID: 923 | Exercise ID: freetext10\n",
            "Current Dataset Length:  3788\n",
            "Subject 31/139 | Session ID: 1293 | Exercise ID: freetext2\n",
            "Current Dataset Length:  3798\n",
            "Subject 31/139 | Session ID: 1466 | Exercise ID: freetext7\n",
            "Current Dataset Length:  3812\n",
            "Subject 32/139 | Session ID: 27 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3824\n",
            "Subject 32/139 | Session ID: 71 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3835\n",
            "Subject 32/139 | Session ID: 81 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3845\n",
            "Subject 32/139 | Session ID: 124 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3850\n",
            "Subject 32/139 | Session ID: 169 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3856\n",
            "Subject 32/139 | Session ID: 171 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3865\n",
            "Subject 32/139 | Session ID: 181 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3874\n",
            "Subject 32/139 | Session ID: 205 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3879\n",
            "Subject 32/139 | Session ID: 217 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3894\n",
            "Subject 32/139 | Session ID: 326 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3902\n",
            "Subject 32/139 | Session ID: 369 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3911\n",
            "Subject 32/139 | Session ID: 384 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3919\n",
            "Subject 32/139 | Session ID: 465 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3924\n",
            "Subject 32/139 | Session ID: 616 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3932\n",
            "Subject 32/139 | Session ID: 639 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3939\n",
            "Subject 32/139 | Session ID: 701 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3945\n",
            "Subject 32/139 | Session ID: 734 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3950\n",
            "Subject 32/139 | Session ID: 982 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3954\n",
            "Subject 32/139 | Session ID: 985 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3958\n",
            "Subject 32/139 | Session ID: 1035 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3962\n",
            "Subject 32/139 | Session ID: 1173 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3979\n",
            "Subject 32/139 | Session ID: 1200 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3988\n",
            "Subject 32/139 | Session ID: 1286 | Exercise ID: freetext1\n",
            "Current Dataset Length:  3993\n",
            "Subject 32/139 | Session ID: 1315 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4001\n",
            "Subject 32/139 | Session ID: 1536 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4004\n",
            "Subject 32/139 | Session ID: 1694 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4011\n",
            "Subject 32/139 | Session ID: 1732 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4020\n",
            "Subject 32/139 | Session ID: 1738 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4023\n",
            "Subject 33/139 | Session ID: 80 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4031\n",
            "Subject 33/139 | Session ID: 240 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4043\n",
            "Subject 33/139 | Session ID: 565 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4060\n",
            "Subject 33/139 | Session ID: 836 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4073\n",
            "Subject 33/139 | Session ID: 947 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4083\n",
            "Subject 33/139 | Session ID: 1030 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4089\n",
            "Subject 33/139 | Session ID: 1275 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4097\n",
            "Subject 33/139 | Session ID: 1462 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4113\n",
            "Subject 34/139 | Session ID: 37 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4121\n",
            "Subject 34/139 | Session ID: 62 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4137\n",
            "Subject 34/139 | Session ID: 184 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4147\n",
            "Subject 34/139 | Session ID: 262 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4154\n",
            "Subject 34/139 | Session ID: 264 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4160\n",
            "Subject 34/139 | Session ID: 267 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4168\n",
            "Subject 34/139 | Session ID: 321 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4174\n",
            "Subject 34/139 | Session ID: 330 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4186\n",
            "Subject 34/139 | Session ID: 571 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4195\n",
            "Subject 34/139 | Session ID: 650 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4208\n",
            "Subject 34/139 | Session ID: 676 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4216\n",
            "Subject 34/139 | Session ID: 681 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4229\n",
            "Subject 34/139 | Session ID: 746 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4239\n",
            "Subject 34/139 | Session ID: 763 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4248\n",
            "Subject 34/139 | Session ID: 806 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4259\n",
            "Subject 34/139 | Session ID: 912 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4269\n",
            "Subject 34/139 | Session ID: 953 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4284\n",
            "Subject 34/139 | Session ID: 1072 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4296\n",
            "Subject 34/139 | Session ID: 1170 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4309\n",
            "Subject 34/139 | Session ID: 1181 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4318\n",
            "Subject 34/139 | Session ID: 1192 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4335\n",
            "Subject 34/139 | Session ID: 1196 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4346\n",
            "Subject 34/139 | Session ID: 1484 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4355\n",
            "Subject 34/139 | Session ID: 1486 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4363\n",
            "Subject 34/139 | Session ID: 1512 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4370\n",
            "Subject 34/139 | Session ID: 1627 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4381\n",
            "Subject 34/139 | Session ID: 1655 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4397\n",
            "Subject 34/139 | Session ID: 1678 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4404\n",
            "Subject 34/139 | Session ID: 1682 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4408\n",
            "Subject 34/139 | Session ID: 1722 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4411\n",
            "Subject 35/139 | Session ID: 7 | Exercise ID: freetext4\n",
            "Current Dataset Length:  4418\n",
            "Subject 35/139 | Session ID: 86 | Exercise ID: freetext4\n",
            "Current Dataset Length:  4429\n",
            "Subject 35/139 | Session ID: 98 | Exercise ID: freetext4\n",
            "Current Dataset Length:  4441\n",
            "Subject 35/139 | Session ID: 138 | Exercise ID: freetext3\n",
            "Current Dataset Length:  4453\n",
            "Subject 35/139 | Session ID: 355 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4459\n",
            "Subject 35/139 | Session ID: 452 | Exercise ID: freetext5\n",
            "Current Dataset Length:  4471\n",
            "Subject 35/139 | Session ID: 630 | Exercise ID: freetext7\n",
            "Current Dataset Length:  4485\n",
            "Subject 35/139 | Session ID: 938 | Exercise ID: freetext4\n",
            "Current Dataset Length:  4491\n",
            "Subject 35/139 | Session ID: 988 | Exercise ID: freetext4\n",
            "Current Dataset Length:  4499\n",
            "Subject 35/139 | Session ID: 1040 | Exercise ID: freetext4\n",
            "Current Dataset Length:  4513\n",
            "Subject 35/139 | Session ID: 1319 | Exercise ID: freetext4\n",
            "Current Dataset Length:  4523\n",
            "Subject 35/139 | Session ID: 1508 | Exercise ID: freetext4\n",
            "Current Dataset Length:  4529\n",
            "Subject 35/139 | Session ID: 1569 | Exercise ID: freetext4\n",
            "Current Dataset Length:  4551\n",
            "Subject 35/139 | Session ID: 1606 | Exercise ID: freetext4\n",
            "Current Dataset Length:  4558\n",
            "Subject 35/139 | Session ID: 1686 | Exercise ID: freetext4\n",
            "Current Dataset Length:  4562\n",
            "Subject 36/139 | Session ID: 274 | Exercise ID: freetext2\n",
            "Current Dataset Length:  4573\n",
            "Subject 36/139 | Session ID: 320 | Exercise ID: freetext2\n",
            "Current Dataset Length:  4586\n",
            "Subject 36/139 | Session ID: 512 | Exercise ID: freetext2\n",
            "Current Dataset Length:  4594\n",
            "Subject 36/139 | Session ID: 698 | Exercise ID: freetext2\n",
            "Current Dataset Length:  4605\n",
            "Subject 36/139 | Session ID: 789 | Exercise ID: freetext2\n",
            "Current Dataset Length:  4622\n",
            "Subject 36/139 | Session ID: 866 | Exercise ID: freetext2\n",
            "Current Dataset Length:  4633\n",
            "Subject 36/139 | Session ID: 910 | Exercise ID: freetext2\n",
            "Current Dataset Length:  4639\n",
            "Subject 36/139 | Session ID: 917 | Exercise ID: freetext2\n",
            "Current Dataset Length:  4652\n",
            "Subject 36/139 | Session ID: 1101 | Exercise ID: freetext2\n",
            "Current Dataset Length:  4665\n",
            "Subject 36/139 | Session ID: 1643 | Exercise ID: freetext2\n",
            "Current Dataset Length:  4676\n",
            "Subject 36/139 | Session ID: 1721 | Exercise ID: freetext2\n",
            "Current Dataset Length:  4683\n",
            "Subject 37/139 | Session ID: 57 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4699\n",
            "Subject 37/139 | Session ID: 253 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4707\n",
            "Subject 37/139 | Session ID: 411 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4714\n",
            "Subject 37/139 | Session ID: 750 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4735\n",
            "Subject 37/139 | Session ID: 834 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4748\n",
            "Subject 37/139 | Session ID: 857 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4753\n",
            "Subject 37/139 | Session ID: 926 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4764\n",
            "Subject 37/139 | Session ID: 1074 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4771\n",
            "Subject 37/139 | Session ID: 1555 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4784\n",
            "Subject 37/139 | Session ID: 1615 | Exercise ID: freetext1\n",
            "Current Dataset Length:  4797\n",
            "Subject 38/139 | Session ID: 542 | Exercise ID: freetext5\n",
            "Current Dataset Length:  4803\n",
            "Subject 38/139 | Session ID: 937 | Exercise ID: freetext5\n",
            "Current Dataset Length:  4808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2Q3Y_oNFbOHq"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df1, df2], join=\"inner\", keys=[\"villani\", \"cmu\"])\n",
        "datasets = [dataset1, dataset2]\n",
        "word_dictionary = word_dictionary1 + word_dictionary2\n",
        "\n",
        "# !-- Whole-dataset Inference Attack\n",
        "balanced_dataset = make_balanced_dataset(datasets, word_dictionary, min_sample=50)\n",
        "save_processed_dataset(balanced_dataset)\n",
        "train_datasets, test_datasets, keytext_tokens, keydelay_tokens = split_dataset(df, balanced_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7hjHXdm7NyoU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9f6788a-51e6-40fd-af49-228c0a639d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 400\n",
            "Number of unique input tokens: 1001\n",
            "Number of unique output tokens: 168\n",
            "Max sequence length for inputs: 11\n",
            "Max sequence length for outputs: 13\n",
            "(400, 11, 1001)\n",
            "(400, 13, 168)\n",
            "(400, 13, 168)\n",
            "Epoch 1/100\n",
            "10/10 [==============================] - 5s 94ms/step - loss: 2.7223 - accuracy: 0.1149 - val_loss: 1.9377 - val_accuracy: 0.1663\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 1.9611 - accuracy: 0.1599 - val_loss: 1.8431 - val_accuracy: 0.1731\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 1.8551 - accuracy: 0.1719 - val_loss: 1.7581 - val_accuracy: 0.1923\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.7988 - accuracy: 0.1950 - val_loss: 1.7168 - val_accuracy: 0.1808\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.7492 - accuracy: 0.2065 - val_loss: 1.7181 - val_accuracy: 0.1865\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.7084 - accuracy: 0.2094 - val_loss: 1.6977 - val_accuracy: 0.2048\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.6770 - accuracy: 0.2286 - val_loss: 1.6536 - val_accuracy: 0.2058\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.6358 - accuracy: 0.2240 - val_loss: 1.6975 - val_accuracy: 0.2096\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.6089 - accuracy: 0.2332 - val_loss: 1.6132 - val_accuracy: 0.2173\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.5786 - accuracy: 0.2380 - val_loss: 1.5802 - val_accuracy: 0.2221\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 1.5411 - accuracy: 0.2373 - val_loss: 1.5739 - val_accuracy: 0.2365\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.5044 - accuracy: 0.2599 - val_loss: 1.5054 - val_accuracy: 0.2625\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.4812 - accuracy: 0.2584 - val_loss: 1.4620 - val_accuracy: 0.2510\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.4272 - accuracy: 0.2873 - val_loss: 1.4052 - val_accuracy: 0.2587\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.4274 - accuracy: 0.2952 - val_loss: 1.4321 - val_accuracy: 0.2510\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.3972 - accuracy: 0.3005 - val_loss: 1.4183 - val_accuracy: 0.3173\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.3735 - accuracy: 0.3002 - val_loss: 1.3516 - val_accuracy: 0.3327\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.3139 - accuracy: 0.3430 - val_loss: 1.4049 - val_accuracy: 0.3058\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.2998 - accuracy: 0.3394 - val_loss: 1.2973 - val_accuracy: 0.3269\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.2617 - accuracy: 0.3702 - val_loss: 1.2677 - val_accuracy: 0.3798\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.2396 - accuracy: 0.3784 - val_loss: 1.2226 - val_accuracy: 0.4163\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.1839 - accuracy: 0.4180 - val_loss: 1.2578 - val_accuracy: 0.3269\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.1631 - accuracy: 0.4130 - val_loss: 1.2951 - val_accuracy: 0.3106\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.1563 - accuracy: 0.4214 - val_loss: 1.1961 - val_accuracy: 0.4058\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 1.1296 - accuracy: 0.4272 - val_loss: 1.1371 - val_accuracy: 0.3971\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.0752 - accuracy: 0.4683 - val_loss: 1.0422 - val_accuracy: 0.4644\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.0605 - accuracy: 0.4620 - val_loss: 1.1135 - val_accuracy: 0.4337\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.0271 - accuracy: 0.4976 - val_loss: 1.1382 - val_accuracy: 0.4163\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.0157 - accuracy: 0.4913 - val_loss: 0.9516 - val_accuracy: 0.5173\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.9791 - accuracy: 0.5096 - val_loss: 0.9476 - val_accuracy: 0.4971\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.9591 - accuracy: 0.5159 - val_loss: 0.9388 - val_accuracy: 0.5163\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.9392 - accuracy: 0.5224 - val_loss: 0.9170 - val_accuracy: 0.5394\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.9365 - accuracy: 0.5325 - val_loss: 0.8575 - val_accuracy: 0.5740\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.8659 - accuracy: 0.5589 - val_loss: 0.8626 - val_accuracy: 0.5673\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.8533 - accuracy: 0.5615 - val_loss: 0.9848 - val_accuracy: 0.4856\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.8508 - accuracy: 0.5587 - val_loss: 0.8489 - val_accuracy: 0.5635\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.8380 - accuracy: 0.5630 - val_loss: 0.8139 - val_accuracy: 0.5875\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.8124 - accuracy: 0.5673 - val_loss: 0.7866 - val_accuracy: 0.5356\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.8158 - accuracy: 0.5680 - val_loss: 0.8146 - val_accuracy: 0.5971\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.7830 - accuracy: 0.5748 - val_loss: 0.7855 - val_accuracy: 0.5798\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.7677 - accuracy: 0.5837 - val_loss: 0.6900 - val_accuracy: 0.6115\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.7165 - accuracy: 0.5998 - val_loss: 0.7457 - val_accuracy: 0.5981\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.7417 - accuracy: 0.5834 - val_loss: 0.6610 - val_accuracy: 0.6269\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.7006 - accuracy: 0.6002 - val_loss: 0.6421 - val_accuracy: 0.6327\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6909 - accuracy: 0.6000 - val_loss: 0.6950 - val_accuracy: 0.6096\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6784 - accuracy: 0.6031 - val_loss: 0.6110 - val_accuracy: 0.6394\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6581 - accuracy: 0.6058 - val_loss: 0.6333 - val_accuracy: 0.6375\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6670 - accuracy: 0.6017 - val_loss: 0.5735 - val_accuracy: 0.6317\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6474 - accuracy: 0.6050 - val_loss: 0.5565 - val_accuracy: 0.6404\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6392 - accuracy: 0.6077 - val_loss: 0.5781 - val_accuracy: 0.6346\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6202 - accuracy: 0.6127 - val_loss: 0.5393 - val_accuracy: 0.6394\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.6154 - accuracy: 0.6173 - val_loss: 0.5483 - val_accuracy: 0.6346\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5833 - accuracy: 0.6187 - val_loss: 0.5358 - val_accuracy: 0.6375\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5765 - accuracy: 0.6214 - val_loss: 0.5516 - val_accuracy: 0.6346\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5885 - accuracy: 0.6125 - val_loss: 0.5085 - val_accuracy: 0.6471\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5739 - accuracy: 0.6187 - val_loss: 0.5210 - val_accuracy: 0.6375\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5834 - accuracy: 0.6144 - val_loss: 0.5069 - val_accuracy: 0.6394\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5325 - accuracy: 0.6274 - val_loss: 0.5109 - val_accuracy: 0.6404\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5395 - accuracy: 0.6274 - val_loss: 0.4638 - val_accuracy: 0.6394\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5516 - accuracy: 0.6197 - val_loss: 0.4843 - val_accuracy: 0.6452\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5284 - accuracy: 0.6303 - val_loss: 0.4703 - val_accuracy: 0.6452\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5508 - accuracy: 0.6139 - val_loss: 0.4787 - val_accuracy: 0.6375\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5122 - accuracy: 0.6262 - val_loss: 0.4514 - val_accuracy: 0.6471\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5269 - accuracy: 0.6214 - val_loss: 0.4385 - val_accuracy: 0.6385\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5016 - accuracy: 0.6252 - val_loss: 0.4565 - val_accuracy: 0.6452\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4888 - accuracy: 0.6334 - val_loss: 0.4583 - val_accuracy: 0.6442\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.4927 - accuracy: 0.6344 - val_loss: 0.4435 - val_accuracy: 0.6471\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.5109 - accuracy: 0.6240 - val_loss: 0.4350 - val_accuracy: 0.6471\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.4872 - accuracy: 0.6272 - val_loss: 0.4186 - val_accuracy: 0.6394\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4774 - accuracy: 0.6267 - val_loss: 0.4174 - val_accuracy: 0.6471\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4770 - accuracy: 0.6284 - val_loss: 0.4143 - val_accuracy: 0.6452\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.4517 - accuracy: 0.6349 - val_loss: 0.4110 - val_accuracy: 0.6385\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4683 - accuracy: 0.6303 - val_loss: 0.4068 - val_accuracy: 0.6462\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4636 - accuracy: 0.6274 - val_loss: 0.3976 - val_accuracy: 0.6452\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4574 - accuracy: 0.6293 - val_loss: 0.3947 - val_accuracy: 0.6471\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4543 - accuracy: 0.6337 - val_loss: 0.3842 - val_accuracy: 0.6462\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.4618 - accuracy: 0.6255 - val_loss: 0.3799 - val_accuracy: 0.6471\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.4258 - accuracy: 0.6370 - val_loss: 0.4041 - val_accuracy: 0.6490\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4437 - accuracy: 0.6349 - val_loss: 0.3768 - val_accuracy: 0.6481\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.4291 - accuracy: 0.6353 - val_loss: 0.3757 - val_accuracy: 0.6519\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4056 - accuracy: 0.6421 - val_loss: 0.3605 - val_accuracy: 0.6462\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.4180 - accuracy: 0.6404 - val_loss: 0.3648 - val_accuracy: 0.6519\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.4101 - accuracy: 0.6411 - val_loss: 0.3636 - val_accuracy: 0.6490\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.4130 - accuracy: 0.6382 - val_loss: 0.3596 - val_accuracy: 0.6490\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.3976 - accuracy: 0.6430 - val_loss: 0.3843 - val_accuracy: 0.6471\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4153 - accuracy: 0.6365 - val_loss: 0.3569 - val_accuracy: 0.6538\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3906 - accuracy: 0.6454 - val_loss: 0.3597 - val_accuracy: 0.6452\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3828 - accuracy: 0.6486 - val_loss: 0.3548 - val_accuracy: 0.6519\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.4026 - accuracy: 0.6394 - val_loss: 0.3496 - val_accuracy: 0.6510\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3829 - accuracy: 0.6466 - val_loss: 0.3401 - val_accuracy: 0.6519\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3750 - accuracy: 0.6488 - val_loss: 0.3319 - val_accuracy: 0.6548\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3779 - accuracy: 0.6452 - val_loss: 0.3418 - val_accuracy: 0.6548\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3838 - accuracy: 0.6442 - val_loss: 0.3296 - val_accuracy: 0.6548\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3777 - accuracy: 0.6462 - val_loss: 0.3478 - val_accuracy: 0.6481\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3616 - accuracy: 0.6510 - val_loss: 0.3341 - val_accuracy: 0.6529\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.3571 - accuracy: 0.6529 - val_loss: 0.3266 - val_accuracy: 0.6558\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.3603 - accuracy: 0.6505 - val_loss: 0.3365 - val_accuracy: 0.6558\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.3610 - accuracy: 0.6526 - val_loss: 0.3262 - val_accuracy: 0.6567\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.3596 - accuracy: 0.6505 - val_loss: 0.3296 - val_accuracy: 0.6558\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3472 - accuracy: 0.6562 - val_loss: 0.3286 - val_accuracy: 0.6548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/S2/camstroke-inference-models/s2s_model_beamsearch_CMUandVillani_v1/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/S2/camstroke-inference-models/s2s_model_beamsearch_CMUandVillani_v1/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7efdb958edd0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7efdb961fe10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = train(train_datasets, test_datasets, keydelay_tokens, keytext_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwTB-PrQQ8mk"
      },
      "source": [
        "# Testing Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xF9jPAa2Q_my",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9145517e-44fd-4dc5-d86e-5a302cafaf81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 1001)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None, 168)]  0           []                               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        1288192     ['input_1[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  435200      ['input_2[0][0]',                \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 168)    43176       ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,766,568\n",
            "Trainable params: 1,766,568\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Number of samples: 100\n",
            "Number of unique input tokens: 1001\n",
            "Number of unique output tokens: 168\n",
            "Max sequence length for inputs: 11\n",
            "Max sequence length for outputs: 13\n",
            "(100, 11, 1001)\n",
            "(100, 13, 168)\n",
            "(100, 13, 168)\n",
            "Input sentence: ['s', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g']\n",
            "-----\n",
            "score : 0.6953021822597317  |  s\n",
            "score : 2.324862607268669  |  d\n",
            "score : 2.6326481062856733  |  t\n",
            "score : 2.67449496021727  |  i\n",
            "score : 2.833563489505778  |  a\n",
            "score : 2.901080337494968  |  e\n",
            "score : 3.0278340782825  |  f\n",
            "score : 3.158747436650694  |  b\n",
            "score : 3.2809164177965564  |  v\n",
            "score : 6.024580175422032  |  n\n",
            "-----\n",
            "score : 1.1251758899085347  |  so\n",
            "score : 2.0297011372831872  |  sa\n",
            "score : 2.4859777182389875  |  di\n",
            "score : 2.7604898820857873  |  to\n",
            "score : 2.9618672159872235  |  in\n",
            "score : 3.187085537869755  |  fa\n",
            "score : 3.2439040833068735  |  ac\n",
            "score : 3.335796577558707  |  ev\n",
            "score : 3.4750461237796038  |  br\n",
            "score : 3.555501435936789  |  va\n",
            "-----\n",
            "score : 1.2130709113348457  |  som\n",
            "score : 2.3145471136729796  |  sat\n",
            "score : 2.516474011561073  |  dif\n",
            "score : 2.950834181551202  |  tog\n",
            "score : 2.9915574825296867  |  int\n",
            "score : 3.278264704324864  |  act\n",
            "score : 3.357773963344078  |  fav\n",
            "score : 3.3597629522799926  |  eve\n",
            "score : 3.490599344719967  |  bre\n",
            "score : 3.6659140927193503  |  vac\n",
            "-----\n",
            "score : 1.2299680912736677  |  some\n",
            "score : 2.3590905334227545  |  satu\n",
            "score : 2.5466560970783774  |  diff\n",
            "score : 2.961383565296785  |  toge\n",
            "score : 3.009604988821869  |  inte\n",
            "score : 3.297210888856781  |  actu\n",
            "score : 3.3885539779243943  |  ever\n",
            "score : 3.4062809054910557  |  favo\n",
            "score : 3.5169190491469307  |  brea\n",
            "score : 3.698159563377916  |  vaca\n",
            "-----\n",
            "score : 1.242228517657709  |  somet\n",
            "score : 2.4033615287360677  |  satur\n",
            "score : 2.5612727955839283  |  diffe\n",
            "score : 2.9766010936504412  |  toget\n",
            "score : 3.0223548313725024  |  inter\n",
            "score : 3.325639917907554  |  actua\n",
            "score : 3.422520241784764  |  every\n",
            "score : 3.4287192576223045  |  favor\n",
            "score : 3.5690354898396355  |  break\n",
            "score : 3.7171367281821444  |  vacat\n",
            "-----\n",
            "score : 1.2649310931652045  |  someth\n",
            "score : 2.4358842332599804  |  saturd\n",
            "score : 2.571879164933253  |  differ\n",
            "score : 2.995329046321772  |  togeth\n",
            "score : 3.052893526346268  |  intere\n",
            "score : 3.354991984332577  |  actual\n",
            "score : 3.4476973682503056  |  everyo\n",
            "score : 3.4686149755710174  |  favori\n",
            "score : 3.60290482264085  |  breakf\n",
            "score : 3.767246046922696  |  vacati\n",
            "-----\n",
            "score : 1.2973321328584584  |  somethi\n",
            "score : 2.4814028701595103  |  saturda\n",
            "score : 2.5967522438885244  |  differe\n",
            "score : 3.058481948994379  |  togethe\n",
            "score : 3.1150467668839696  |  interes\n",
            "score : 3.4511556972570676  |  actuall\n",
            "score : 3.514563527062678  |  everyon\n",
            "score : 3.569416698980675  |  favorit\n",
            "score : 3.6389533017638445  |  breakfa\n",
            "score : 3.8546502197951416  |  vacatio\n",
            "-----\n",
            "score : 1.3717486112170114  |  somethin\n",
            "score : 2.628712959178824  |  saturday\n",
            "score : 2.6747722833897827  |  differen\n",
            "score : 3.2210872980508674  |  interest\n",
            "score : 3.3244536256423203  |  together\n",
            "score : 3.6221274510669503  |  actually\n",
            "score : 3.758786565054499  |  breakfas\n",
            "score : 3.8390030074423027  |  everyone\n",
            "score : 3.980734553394761  |  favorite\n",
            "score : 4.109131427991603  |  vacation\n",
            "-----\n",
            "score : 1.7914934069555348  |  something\n",
            "score : 3.1061011942493724  |  saturday\n",
            "score : 3.1444791732197483  |  different\n",
            "score : 3.801014937090231  |  somethinn\n",
            "score : 3.852976817459971  |  interesti\n",
            "score : 3.9222718565295955  |  together\n",
            "score : 4.105104157819374  |  actually\n",
            "score : 4.143650923582149  |  somethin\n",
            "score : 4.170349423642256  |  breakfast\n",
            "score : 4.389455151533511  |  everyone\n",
            "-----\n",
            "\n",
            "Input sentence: ['a', 'c', 't', 'u', 'a', 'l', 'l', 'y']\n",
            "-----\n",
            "score : 1.163127349692688  |  s\n",
            "score : 2.220111400645119  |  i\n",
            "score : 2.2208685663012515  |  a\n",
            "score : 2.285004054598932  |  d\n",
            "score : 2.5973619411323967  |  f\n",
            "score : 2.631472049885038  |  e\n",
            "score : 2.6637395475821344  |  b\n",
            "score : 2.7004564401691873  |  t\n",
            "score : 2.7560957856458472  |  v\n",
            "score : 5.715434915120039  |  n\n",
            "-----\n",
            "score : 1.7060402116718107  |  sa\n",
            "score : 2.378256555429732  |  in\n",
            "score : 2.387114537313158  |  so\n",
            "score : 2.4535529646109024  |  di\n",
            "score : 2.4659926336609406  |  ac\n",
            "score : 2.6796454659524187  |  fa\n",
            "score : 2.8473281478103183  |  br\n",
            "score : 2.885099113732565  |  va\n",
            "score : 2.9684234438138404  |  ev\n",
            "score : 3.049061564782108  |  to\n",
            "-----\n",
            "score : 1.8188484025767173  |  sat\n",
            "score : 2.40112621173905  |  int\n",
            "score : 2.4743352018363463  |  dif\n",
            "score : 2.487994566581367  |  act\n",
            "score : 2.60894882109895  |  som\n",
            "score : 2.858572203635492  |  fav\n",
            "score : 2.860444560530562  |  bre\n",
            "score : 2.9667204192203203  |  vac\n",
            "score : 2.9831994048079893  |  eve\n",
            "score : 3.2160949289179803  |  tog\n",
            "-----\n",
            "score : 1.8448409548765248  |  satu\n",
            "score : 2.4220132490456545  |  inte\n",
            "score : 2.5065328729618934  |  actu\n",
            "score : 2.506575809428849  |  diff\n",
            "score : 2.6397500776351377  |  some\n",
            "score : 2.8813122436852168  |  brea\n",
            "score : 2.911758965249555  |  favo\n",
            "score : 2.9881265066075504  |  vaca\n",
            "score : 3.0045034994424618  |  ever\n",
            "score : 3.231048812416005  |  toge\n",
            "-----\n",
            "score : 1.893303475330743  |  satur\n",
            "score : 2.4331011739878923  |  inter\n",
            "score : 2.521514204364995  |  diffe\n",
            "score : 2.5296220329382586  |  actua\n",
            "score : 2.655888699781424  |  somet\n",
            "score : 2.921110206048742  |  break\n",
            "score : 2.930331093197272  |  favor\n",
            "score : 3.0120628449793565  |  vacat\n",
            "score : 3.0391002294180276  |  every\n",
            "score : 3.246709798468486  |  toget\n",
            "-----\n",
            "score : 1.9250705089712394  |  saturd\n",
            "score : 2.459746423607232  |  intere\n",
            "score : 2.5313388410944944  |  differ\n",
            "score : 2.5538038166966763  |  actual\n",
            "score : 2.6821769508592705  |  someth\n",
            "score : 2.952643212648819  |  breakf\n",
            "score : 2.968955865984965  |  favori\n",
            "score : 3.05816291203495  |  vacati\n",
            "score : 3.062844649490769  |  everyo\n",
            "score : 3.2696456240527993  |  togeth\n",
            "-----\n",
            "score : 1.9704970141705287  |  saturda\n",
            "score : 2.5215025008734275  |  interes\n",
            "score : 2.555735958147667  |  differe\n",
            "score : 2.6439399946472144  |  actuall\n",
            "score : 2.7193051568811653  |  somethi\n",
            "score : 2.9913778941467206  |  breakfa\n",
            "score : 3.075721017329799  |  favorit\n",
            "score : 3.1284660499230905  |  everyon\n",
            "score : 3.140699640067889  |  vacatio\n",
            "score : 3.332252849924599  |  togethe\n",
            "-----\n",
            "score : 2.1238770049747107  |  saturday\n",
            "score : 2.6264456347712297  |  differen\n",
            "score : 2.6284590050919494  |  interest\n",
            "score : 2.7916566106943583  |  somethin\n",
            "score : 2.821194402644372  |  actually\n",
            "score : 3.10276990316604  |  breakfas\n",
            "score : 3.3918279233393203  |  vacation\n",
            "score : 3.4767891008330123  |  everyone\n",
            "score : 3.512314943030513  |  favorite\n",
            "score : 3.613787320063062  |  together\n",
            "-----\n",
            "\n",
            "Input sentence: ['s', 'a', 't', 'u', 'r', 'd', 'a', 'y']\n",
            "-----\n",
            "score : 0.7066341039458509  |  s\n",
            "score : 2.067361414049175  |  a\n",
            "score : 2.69440329502579  |  t\n",
            "score : 2.74964393017328  |  d\n",
            "score : 3.1050306740413194  |  b\n",
            "score : 3.1089816639855066  |  e\n",
            "score : 3.151970483694658  |  f\n",
            "score : 3.430281184012688  |  v\n",
            "score : 3.4633995096135877  |  i\n",
            "score : 4.740785222980183  |  n\n",
            "-----\n",
            "score : 0.8376951959373893  |  sa\n",
            "score : 3.018740074129627  |  aa\n",
            "score : 3.041776602193551  |  ac\n",
            "score : 3.1782650376017254  |  fa\n",
            "score : 3.398833631568918  |  ta\n",
            "score : 3.475070734381903  |  va\n",
            "score : 3.4767942503261424  |  da\n",
            "score : 3.6582240270490747  |  ea\n",
            "score : 3.711747622092037  |  di\n",
            "score : 3.861652417919749  |  so\n",
            "-----\n",
            "score : 0.8964427676060843  |  sat\n",
            "score : 3.0645836914002413  |  act\n",
            "score : 3.069812303640913  |  aat\n",
            "score : 3.546175007381944  |  tat\n",
            "score : 3.917827914079799  |  fat\n",
            "score : 3.923375567288805  |  dif\n",
            "score : 4.153028108872931  |  vac\n",
            "score : 4.209602882719821  |  eat\n",
            "score : 4.273999731295749  |  dat\n",
            "score : 4.391919409834283  |  fav\n",
            "-----\n",
            "score : 0.9308413893327676  |  satu\n",
            "score : 3.0869250653407487  |  actu\n",
            "score : 3.0913661924187865  |  aatu\n",
            "score : 3.593153913756874  |  tatu\n",
            "score : 4.011910862642716  |  diff\n",
            "score : 4.063385158988901  |  fatu\n",
            "score : 4.295062268363965  |  vaca\n",
            "score : 4.377750318500628  |  eatu\n",
            "score : 4.47170488121187  |  datu\n",
            "score : 4.902455754499767  |  favo\n",
            "-----\n",
            "score : 0.9757041860085544  |  satur\n",
            "score : 3.1524129429193986  |  actua\n",
            "score : 3.454250332972144  |  aatur\n",
            "score : 3.767886248198467  |  tatur\n",
            "score : 4.043952474158379  |  diffe\n",
            "score : 4.084535766526841  |  fatur\n",
            "score : 4.3504646172484085  |  vacat\n",
            "score : 4.434034931458578  |  eatur\n",
            "score : 4.436099925015342  |  aatua\n",
            "score : 4.559333497611688  |  datur\n",
            "-----\n",
            "score : 1.0125216594201611  |  saturd\n",
            "score : 3.212223912459847  |  actual\n",
            "score : 3.610712358542364  |  aaturd\n",
            "score : 3.870753526071316  |  taturd\n",
            "score : 4.05882970371988  |  differ\n",
            "score : 4.142500584614633  |  faturd\n",
            "score : 4.45693979120885  |  vacati\n",
            "score : 4.520558601309413  |  eaturd\n",
            "score : 4.678551598080423  |  daturd\n",
            "score : 5.05398575922608  |  aatuad\n",
            "-----\n",
            "score : 1.0743817207667414  |  saturda\n",
            "score : 3.410737737514999  |  actuall\n",
            "score : 3.760711225144639  |  aaturda\n",
            "score : 3.9889340695936846  |  taturda\n",
            "score : 4.106951431536425  |  differe\n",
            "score : 4.224127387382997  |  faturda\n",
            "score : 4.639624346578998  |  eaturda\n",
            "score : 4.653404534102605  |  vacatio\n",
            "score : 4.843428150870345  |  daturda\n",
            "score : 5.293879348049071  |  saturdy\n",
            "-----\n",
            "score : 1.248706827326631  |  saturday\n",
            "score : 3.6329416757487962  |  actually\n",
            "score : 3.956226305618503  |  aaturday\n",
            "score : 4.196851799293114  |  taturday\n",
            "score : 4.2034336543544955  |  differen\n",
            "score : 4.4356260067178885  |  faturday\n",
            "score : 4.831562016243926  |  saturdan\n",
            "score : 4.848853281949632  |  eaturday\n",
            "score : 4.9817550911611646  |  vacation\n",
            "score : 5.020816769580266  |  saturdat\n",
            "-----\n",
            "\n",
            "Input sentence: ['e', 'v', 'e', 'r', 'y', 'o', 'n', 'e']\n",
            "-----\n",
            "score : 1.1330907412619768  |  s\n",
            "score : 2.1701604947538105  |  d\n",
            "score : 2.2239349303480793  |  i\n",
            "score : 2.3554217035591742  |  f\n",
            "score : 2.529006233959372  |  a\n",
            "score : 2.572418907073795  |  v\n",
            "score : 2.712404009324411  |  b\n",
            "score : 2.7977497731254024  |  e\n",
            "score : 2.8474295222704513  |  t\n",
            "score : 5.888474282613793  |  n\n",
            "-----\n",
            "score : 1.7072692184977245  |  sa\n",
            "score : 2.2080539207668233  |  so\n",
            "score : 2.2894458161563915  |  di\n",
            "score : 2.4326928942958124  |  fa\n",
            "score : 2.4394035724908916  |  in\n",
            "score : 2.684212897215011  |  va\n",
            "score : 2.898073688472673  |  ac\n",
            "score : 2.9056476800922115  |  br\n",
            "score : 3.0886488011834765  |  to\n",
            "score : 3.1611607264416635  |  ev\n",
            "-----\n",
            "score : 1.8808053229284234  |  sat\n",
            "score : 2.306251342322851  |  dif\n",
            "score : 2.361103147336921  |  som\n",
            "score : 2.469180970643312  |  int\n",
            "score : 2.51671881150584  |  fav\n",
            "score : 2.776609386516673  |  vac\n",
            "score : 2.9190914688313776  |  bre\n",
            "score : 2.9234035152708944  |  act\n",
            "score : 3.17894851272633  |  eve\n",
            "score : 3.226611435764422  |  tog\n",
            "-----\n",
            "score : 1.9162495921896787  |  satu\n",
            "score : 2.3366823774852934  |  diff\n",
            "score : 2.3821748591327427  |  some\n",
            "score : 2.487462950377535  |  inte\n",
            "score : 2.5508376436600884  |  favo\n",
            "score : 2.8029978144462175  |  vaca\n",
            "score : 2.940625430537516  |  actu\n",
            "score : 2.944026388755651  |  brea\n",
            "score : 3.1982391182178165  |  ever\n",
            "score : 3.238117837140283  |  toge\n",
            "-----\n",
            "score : 1.9503304959367824  |  satur\n",
            "score : 2.349825604432588  |  diffe\n",
            "score : 2.398262842374747  |  somet\n",
            "score : 2.4971264029096223  |  inter\n",
            "score : 2.570416319661559  |  favor\n",
            "score : 2.827211107497956  |  vacat\n",
            "score : 2.9685509980651785  |  actua\n",
            "score : 2.984256942375736  |  break\n",
            "score : 3.2325723500101096  |  every\n",
            "score : 3.2558257791479424  |  toget\n",
            "-----\n",
            "score : 1.9799608984369148  |  saturd\n",
            "score : 2.35984980128168  |  differ\n",
            "score : 2.424811867943289  |  someth\n",
            "score : 2.521714402899573  |  intere\n",
            "score : 2.6106913031147205  |  favori\n",
            "score : 2.8719176310949783  |  vacati\n",
            "score : 2.997355949272783  |  actual\n",
            "score : 3.016664939223538  |  breakf\n",
            "score : 3.2578249067921945  |  everyo\n",
            "score : 3.278921404814155  |  togeth\n",
            "-----\n",
            "score : 2.026041866151279  |  saturda\n",
            "score : 2.3845009686411727  |  differe\n",
            "score : 2.4614129246507375  |  somethi\n",
            "score : 2.5906176755896206  |  interes\n",
            "score : 2.711954484200717  |  favorit\n",
            "score : 2.957624625812667  |  vacatio\n",
            "score : 3.056223329164703  |  breakfa\n",
            "score : 3.105007800431506  |  actuall\n",
            "score : 3.3265509936769857  |  everyon\n",
            "score : 3.3318944002299866  |  togethe\n",
            "-----\n",
            "score : 2.18214583894496  |  saturday\n",
            "score : 2.4627124168933725  |  differen\n",
            "score : 2.536331646803586  |  somethin\n",
            "score : 2.6950836872987605  |  interest\n",
            "score : 3.13011884653174  |  favorite\n",
            "score : 3.17682940713725  |  breakfas\n",
            "score : 3.2256092798692775  |  vacation\n",
            "score : 3.2860477491409292  |  actually\n",
            "score : 3.61375932252488  |  together\n",
            "score : 3.6749086661170107  |  everyone\n",
            "-----\n",
            "\n",
            "Input sentence: ['f', 'a', 'v', 'o', 'r', 'i', 't', 'e']\n",
            "-----\n",
            "score : 1.948475826370045  |  e\n",
            "score : 1.9590514887164838  |  t\n",
            "score : 2.0531672208465888  |  i\n",
            "score : 2.2113689150784093  |  f\n",
            "score : 2.2468447158116334  |  s\n",
            "score : 2.288228908722792  |  d\n",
            "score : 2.3750380819146595  |  a\n",
            "score : 2.499990925026259  |  b\n",
            "score : 2.6312626510220345  |  v\n",
            "score : 5.698685365683111  |  c\n",
            "-----\n",
            "score : 2.170872201732304  |  to\n",
            "score : 2.1969511301959983  |  ev\n",
            "score : 2.287966307882009  |  in\n",
            "score : 2.3475538818853074  |  fa\n",
            "score : 2.4042941842004186  |  di\n",
            "score : 2.6764159752635965  |  br\n",
            "score : 2.698880731365458  |  ac\n",
            "score : 2.8346854073844865  |  va\n",
            "score : 2.960231989880862  |  so\n",
            "score : 3.1931702630787826  |  sa\n",
            "-----\n",
            "score : 2.218627198461172  |  eve\n",
            "score : 2.2477635460099004  |  tog\n",
            "score : 2.3591116637428784  |  int\n",
            "score : 2.411307963969832  |  fav\n",
            "score : 2.4189342291332627  |  dif\n",
            "score : 2.692500384804203  |  bre\n",
            "score : 2.758535731745977  |  act\n",
            "score : 2.938164941415221  |  vac\n",
            "score : 3.1348218679665076  |  som\n",
            "score : 3.603821225987075  |  sat\n",
            "-----\n",
            "score : 2.2378471360378405  |  ever\n",
            "score : 2.2604400423566693  |  toge\n",
            "score : 2.372470396893817  |  inte\n",
            "score : 2.444597941174661  |  favo\n",
            "score : 2.452877245948049  |  diff\n",
            "score : 2.7251284497311445  |  brea\n",
            "score : 2.7817927082109226  |  actu\n",
            "score : 2.963340050778609  |  vaca\n",
            "score : 3.14900693722097  |  some\n",
            "score : 3.6605254994726892  |  satu\n",
            "-----\n",
            "score : 2.2714400942614024  |  every\n",
            "score : 2.281108058785522  |  toget\n",
            "score : 2.383770913950435  |  inter\n",
            "score : 2.465926574010685  |  favor\n",
            "score : 2.4660438453418934  |  diffe\n",
            "score : 2.7653302739374737  |  break\n",
            "score : 2.805822394209082  |  actua\n",
            "score : 2.992054397280099  |  vacat\n",
            "score : 3.164020478124563  |  somet\n",
            "score : 3.6966891664477934  |  satur\n",
            "-----\n",
            "score : 2.2923183065151544  |  everyo\n",
            "score : 2.30803499448617  |  togeth\n",
            "score : 2.4087626062734135  |  intere\n",
            "score : 2.4764098851820426  |  differ\n",
            "score : 2.5059930770248853  |  favori\n",
            "score : 2.794356035649365  |  breakf\n",
            "score : 2.8283324345682583  |  actual\n",
            "score : 3.0395088257431797  |  vacati\n",
            "score : 3.188855672209263  |  someth\n",
            "score : 3.7302890901232377  |  saturd\n",
            "-----\n",
            "score : 2.352960382044219  |  togethe\n",
            "score : 2.3618658423252197  |  everyon\n",
            "score : 2.4686614101285325  |  interes\n",
            "score : 2.498170622965083  |  differe\n",
            "score : 2.6087514898762487  |  favorit\n",
            "score : 2.8335452875818574  |  breakfa\n",
            "score : 2.9187181359710714  |  actuall\n",
            "score : 3.1170117292821753  |  vacatio\n",
            "score : 3.232097645302631  |  somethi\n",
            "score : 3.7785776967393128  |  saturda\n",
            "-----\n",
            "score : 2.5723198332133226  |  differen\n",
            "score : 2.5747833271561795  |  interest\n",
            "score : 2.616558424747774  |  together\n",
            "score : 2.674588315375766  |  everyone\n",
            "score : 2.9457247502279973  |  breakfas\n",
            "score : 3.006533644590137  |  favorite\n",
            "score : 3.101112317701347  |  actually\n",
            "score : 3.3197759713726107  |  somethin\n",
            "score : 3.380565628974002  |  vacation\n",
            "score : 3.933702731387082  |  saturday\n",
            "-----\n",
            "\n",
            "Input sentence: ['s', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g']\n",
            "-----\n",
            "score : 1.6039493921786503  |  s\n",
            "score : 1.792022509315137  |  a\n",
            "score : 2.1123874792659474  |  b\n",
            "score : 2.319870697491928  |  e\n",
            "score : 2.3989579622203854  |  i\n",
            "score : 2.4222733646372547  |  v\n",
            "score : 2.5611980019239415  |  d\n",
            "score : 2.7071321374274118  |  f\n",
            "score : 2.7969091298870175  |  t\n",
            "score : 5.359110658354629  |  n\n",
            "-----\n",
            "score : 1.937653597015503  |  sa\n",
            "score : 2.0343182060585363  |  ac\n",
            "score : 2.283527344752452  |  br\n",
            "score : 2.5532145293772674  |  va\n",
            "score : 2.5951008775777735  |  in\n",
            "score : 2.6280002317183917  |  ev\n",
            "score : 2.7896175335543525  |  fa\n",
            "score : 2.8025144683657666  |  di\n",
            "score : 3.5385110103849073  |  to\n",
            "score : 3.6085601164998025  |  so\n",
            "-----\n",
            "score : 2.0420030933673603  |  sat\n",
            "score : 2.0595428272869025  |  act\n",
            "score : 2.2951382418534743  |  bre\n",
            "score : 2.6223726056011136  |  int\n",
            "score : 2.638603441031301  |  vac\n",
            "score : 2.6395911091736286  |  eve\n",
            "score : 2.8303520235971473  |  dif\n",
            "score : 2.9649233766894003  |  fav\n",
            "score : 3.704412545077731  |  tog\n",
            "score : 3.969415618072225  |  som\n",
            "-----\n",
            "score : 2.0665037408036318  |  satu\n",
            "score : 2.0788159931985577  |  actu\n",
            "score : 2.3177316201412017  |  brea\n",
            "score : 2.6461991846606963  |  inte\n",
            "score : 2.659165402626541  |  vaca\n",
            "score : 2.6634433871922494  |  ever\n",
            "score : 2.866368989042183  |  diff\n",
            "score : 3.028817104021855  |  favo\n",
            "score : 3.724744911850853  |  toge\n",
            "score : 4.015132207773604  |  some\n",
            "-----\n",
            "score : 2.1014040064360753  |  actua\n",
            "score : 2.117876921154326  |  satur\n",
            "score : 2.3560032055735176  |  break\n",
            "score : 2.6577694398878355  |  inter\n",
            "score : 2.684561500555511  |  vacat\n",
            "score : 2.702304100646668  |  every\n",
            "score : 2.8827484233645877  |  diffe\n",
            "score : 3.0479777403062602  |  favor\n",
            "score : 3.7412139006874767  |  toget\n",
            "score : 4.0341218864501585  |  somet\n",
            "-----\n",
            "score : 2.1261422951088744  |  actual\n",
            "score : 2.1502992018693625  |  saturd\n",
            "score : 2.387226844080976  |  breakf\n",
            "score : 2.6857222216989283  |  intere\n",
            "score : 2.7252037002975147  |  everyo\n",
            "score : 2.729268398131547  |  vacati\n",
            "score : 2.8926811727902404  |  differ\n",
            "score : 3.086193094858531  |  favori\n",
            "score : 3.7670373205393837  |  togeth\n",
            "score : 4.062661426488026  |  someth\n",
            "-----\n",
            "score : 2.194490889011547  |  saturda\n",
            "score : 2.216412196840292  |  actuall\n",
            "score : 2.425529333072359  |  breakfa\n",
            "score : 2.745884258293467  |  interes\n",
            "score : 2.7935247083332064  |  everyon\n",
            "score : 2.8107543507012362  |  vacatio\n",
            "score : 2.9167185516563845  |  differe\n",
            "score : 3.1978913065977035  |  favorit\n",
            "score : 3.8315027928924454  |  togethe\n",
            "score : 4.101488988404499  |  somethi\n",
            "-----\n",
            "score : 2.349485071924307  |  saturday\n",
            "score : 2.3944893150446163  |  actually\n",
            "score : 2.5343838372261795  |  breakfas\n",
            "score : 2.8524720844144245  |  interest\n",
            "score : 2.986884487444815  |  differen\n",
            "score : 3.0564288371684905  |  vacation\n",
            "score : 3.1588388519888584  |  everyone\n",
            "score : 3.645034497098097  |  favorite\n",
            "score : 4.1170726427526105  |  together\n",
            "score : 4.176474973858731  |  somethin\n",
            "-----\n",
            "score : 2.8297070904849013  |  saturday\n",
            "score : 2.8789250679543326  |  actually\n",
            "score : 2.918687106265235  |  breakfast\n",
            "score : 3.440352592424634  |  different\n",
            "score : 3.510853398371072  |  interesti\n",
            "score : 3.5736197498527287  |  vacation\n",
            "score : 3.7180193078637886  |  everyone\n",
            "score : 4.194058032926132  |  favorite\n",
            "score : 4.282276765258317  |  interestt\n",
            "score : 4.616465084943718  |  something\n",
            "-----\n",
            "\n",
            "Input sentence: ['t', 'o', 'g', 'e', 't', 'h', 'e', 'r']\n",
            "-----\n",
            "score : 1.9708995801985223  |  t\n",
            "score : 2.018129725149123  |  e\n",
            "score : 2.1206044807291233  |  i\n",
            "score : 2.1255431067384043  |  s\n",
            "score : 2.130264709865963  |  f\n",
            "score : 2.3249138805741785  |  d\n",
            "score : 2.3995857101512597  |  a\n",
            "score : 2.5133375798679447  |  v\n",
            "score : 2.586834333327471  |  b\n",
            "score : 5.684983426716254  |  c\n",
            "-----\n",
            "score : 2.1519554588089522  |  to\n",
            "score : 2.2586741148174547  |  fa\n",
            "score : 2.27906367302808  |  ev\n",
            "score : 2.3767152068280946  |  in\n",
            "score : 2.444279188015847  |  di\n",
            "score : 2.711485098482735  |  va\n",
            "score : 2.744824154152309  |  ac\n",
            "score : 2.7682865824899907  |  so\n",
            "score : 2.7821991833734483  |  br\n",
            "score : 3.121460769884723  |  sa\n",
            "-----\n",
            "score : 2.231502438336106  |  tog\n",
            "score : 2.3029215670591734  |  eve\n",
            "score : 2.321342133018931  |  fav\n",
            "score : 2.4513861795125185  |  int\n",
            "score : 2.4590420828450537  |  dif\n",
            "score : 2.7992175507394443  |  bre\n",
            "score : 2.8100486023175417  |  act\n",
            "score : 2.8158552371110868  |  vac\n",
            "score : 2.9204785962419177  |  som\n",
            "score : 3.5929259234851973  |  sat\n",
            "-----\n",
            "score : 2.243944323326193  |  toge\n",
            "score : 2.3226426072696227  |  ever\n",
            "score : 2.3542682968531086  |  favo\n",
            "score : 2.4645472830106345  |  inte\n",
            "score : 2.4934411980969333  |  diff\n",
            "score : 2.8327209486697766  |  brea\n",
            "score : 2.832950641383978  |  actu\n",
            "score : 2.8416920519438214  |  vaca\n",
            "score : 2.9340186853377412  |  some\n",
            "score : 3.653266673772741  |  satu\n",
            "-----\n",
            "score : 2.2638751171662705  |  toget\n",
            "score : 2.3559066418314156  |  every\n",
            "score : 2.3754931792191485  |  favor\n",
            "score : 2.475788122640005  |  inter\n",
            "score : 2.506718627768213  |  diffe\n",
            "score : 2.857466193191604  |  actua\n",
            "score : 2.8696190904948966  |  vacat\n",
            "score : 2.8739374274067053  |  break\n",
            "score : 2.9482391220747095  |  somet\n",
            "score : 3.689915894603251  |  satur\n",
            "-----\n",
            "score : 2.289540807795136  |  togeth\n",
            "score : 2.3770679032066107  |  everyo\n",
            "score : 2.416075812750425  |  favori\n",
            "score : 2.5006927326786945  |  intere\n",
            "score : 2.5171445939976946  |  differ\n",
            "score : 2.880436477321026  |  actual\n",
            "score : 2.9031521959886994  |  breakf\n",
            "score : 2.9182782701153562  |  vacati\n",
            "score : 2.972297442936868  |  someth\n",
            "score : 3.7237344225106614  |  saturd\n",
            "-----\n",
            "score : 2.3350943159817232  |  togethe\n",
            "score : 2.446762094650245  |  everyon\n",
            "score : 2.5183926114927577  |  favorit\n",
            "score : 2.5390363095451622  |  differe\n",
            "score : 2.562306229842637  |  interes\n",
            "score : 2.9419463596166064  |  breakfa\n",
            "score : 2.9716507168766464  |  actuall\n",
            "score : 2.9956006545852225  |  vacatio\n",
            "score : 3.015727518786682  |  somethi\n",
            "score : 3.7724886606258035  |  saturda\n",
            "-----\n",
            "score : 2.597642841341675  |  together\n",
            "score : 2.6140469657683054  |  differen\n",
            "score : 2.6676364403091712  |  interest\n",
            "score : 2.7609636091045506  |  everyone\n",
            "score : 2.9178031767523485  |  favorite\n",
            "score : 3.057110758967463  |  breakfas\n",
            "score : 3.1045238588471533  |  somethin\n",
            "score : 3.152618162740301  |  actually\n",
            "score : 3.2612501205666495  |  vacation\n",
            "score : 3.92796039593987  |  saturday\n",
            "-----\n",
            "\n",
            "Input sentence: ['i', 'n', 't', 'e', 'r', 'e', 's', 't', 'i', 'n', 'g']\n",
            "-----\n",
            "score : 1.6707114612227445  |  e\n",
            "score : 1.807510278504586  |  b\n",
            "score : 2.249284136677079  |  i\n",
            "score : 2.2616185795744985  |  s\n",
            "score : 2.352842286611663  |  d\n",
            "score : 2.4202546199094606  |  f\n",
            "score : 2.4217566081787782  |  a\n",
            "score : 2.5732630698683883  |  t\n",
            "score : 2.673227768298736  |  v\n",
            "score : 5.728212317909725  |  n\n",
            "-----\n",
            "score : 1.8957095265672588  |  ev\n",
            "score : 1.9301112115505092  |  br\n",
            "score : 2.432064372943425  |  in\n",
            "score : 2.5327204655295805  |  di\n",
            "score : 2.572588572219582  |  fa\n",
            "score : 2.761925315056802  |  ac\n",
            "score : 2.82856845754846  |  sa\n",
            "score : 2.8857077230733705  |  va\n",
            "score : 3.218444264170416  |  to\n",
            "score : 3.851637442734578  |  so\n",
            "-----\n",
            "score : 1.9059475347180048  |  eve\n",
            "score : 1.9400479341598211  |  bre\n",
            "score : 2.469687513598915  |  int\n",
            "score : 2.5612304448283756  |  dif\n",
            "score : 2.6750257438088645  |  fav\n",
            "score : 2.7880029031545908  |  act\n",
            "score : 2.9608010402573823  |  sat\n",
            "score : 3.034051830517288  |  vac\n",
            "score : 3.3448981359268686  |  tog\n",
            "score : 4.397442015873279  |  som\n",
            "-----\n",
            "score : 1.9288045675247212  |  ever\n",
            "score : 1.9673422033699839  |  brea\n",
            "score : 2.48643782121035  |  inte\n",
            "score : 2.596713497091392  |  diff\n",
            "score : 2.719411195701685  |  favo\n",
            "score : 2.808930962763761  |  actu\n",
            "score : 2.9947203172783134  |  satu\n",
            "score : 3.0557085293536086  |  vaca\n",
            "score : 3.3602522036008833  |  toge\n",
            "score : 4.426748477928843  |  some\n",
            "-----\n",
            "score : 1.9661960193189967  |  every\n",
            "score : 2.0043839957476486  |  break\n",
            "score : 2.499168949403533  |  inter\n",
            "score : 2.610905277002792  |  diffe\n",
            "score : 2.7408255647481456  |  favor\n",
            "score : 2.8312972660890443  |  actua\n",
            "score : 3.0403510581540556  |  satur\n",
            "score : 3.084249234666727  |  vacat\n",
            "score : 3.38067977221643  |  toget\n",
            "score : 4.446187058644018  |  somet\n",
            "-----\n",
            "score : 1.9848062813930634  |  everyo\n",
            "score : 2.032421183323969  |  breakf\n",
            "score : 2.5223280752451855  |  intere\n",
            "score : 2.621362864131305  |  differ\n",
            "score : 2.7759898135015493  |  favori\n",
            "score : 2.8556655553504644  |  actual\n",
            "score : 3.0753025190608607  |  saturd\n",
            "score : 3.1238599002072203  |  vacati\n",
            "score : 3.4100658437913336  |  togeth\n",
            "score : 4.469925191940069  |  someth\n",
            "-----\n",
            "score : 2.0566538968151633  |  everyon\n",
            "score : 2.069514934334903  |  breakfa\n",
            "score : 2.5753669995546864  |  interes\n",
            "score : 2.6413220540901885  |  differe\n",
            "score : 2.881638420033551  |  favorit\n",
            "score : 2.9332845286739393  |  actuall\n",
            "score : 3.1141313202673904  |  saturda\n",
            "score : 3.1974765207029594  |  vacatio\n",
            "score : 3.457311976996603  |  togethe\n",
            "score : 4.50758806668661  |  somethi\n",
            "-----\n",
            "score : 2.166544498624254  |  breakfas\n",
            "score : 2.3592681930460495  |  everyone\n",
            "score : 2.6867793969621396  |  interest\n",
            "score : 2.7137191317440506  |  differen\n",
            "score : 3.107340535701455  |  actually\n",
            "score : 3.2638002620067996  |  saturday\n",
            "score : 3.272483069145523  |  favorite\n",
            "score : 3.440947040235317  |  vacation\n",
            "score : 3.706128771516814  |  together\n",
            "score : 4.592389320036869  |  somethin\n",
            "-----\n",
            "score : 2.5351483301844224  |  breakfast\n",
            "score : 2.9134851529668393  |  everyone\n",
            "score : 3.1507797774957997  |  different\n",
            "score : 3.2584470530083474  |  interesti\n",
            "score : 3.590551362431726  |  actually\n",
            "score : 3.7420793102463956  |  saturday\n",
            "score : 3.8179092279038294  |  favorite\n",
            "score : 3.955454118200658  |  vacation\n",
            "score : 4.190819973646971  |  interestt\n",
            "score : 4.302314548748827  |  together\n",
            "-----\n",
            "score : 3.0846642946483884  |  breakfast\n",
            "score : 3.6793856032795635  |  different\n",
            "score : 4.4985507710486585  |  interestin\n",
            "score : 4.6625116701411855  |  everyone\n",
            "score : 4.932312310229618  |  interestii\n",
            "score : 5.194626187828493  |  interestit\n",
            "score : 5.238854368180332  |  breakfastt\n",
            "score : 5.2715967441913225  |  everyone\n",
            "score : 5.336266349266139  |  actually\n",
            "score : 5.376724521842336  |  interesti\n",
            "-----\n",
            "score : 4.83191719763224  |  breakfast\n",
            "score : 5.426691355942948  |  different\n",
            "score : 5.440239336236126  |  breakfast\n",
            "score : 5.6606371664039585  |  breakfast\n",
            "score : 5.670799630824688  |  breakfast\n",
            "score : 5.723366160643402  |  breakfast\n",
            "score : 5.860451086481605  |  breakfast\n",
            "score : 5.9170307658059995  |  breakfast\n",
            "score : 6.031564911261885  |  different\n",
            "score : 6.141929972034569  |  breakfast\n",
            "-----\n",
            "\n",
            "Input sentence: ['e', 'v', 'e', 'r', 'y', 'o', 'n', 'e']\n",
            "-----\n",
            "score : 1.250057704227792  |  s\n",
            "score : 1.5717902728655277  |  a\n",
            "score : 2.2828497610217378  |  b\n",
            "score : 2.382695662858245  |  e\n",
            "score : 2.6380085995582876  |  d\n",
            "score : 2.902677085732218  |  f\n",
            "score : 2.9045176598791573  |  v\n",
            "score : 2.9768919832010856  |  t\n",
            "score : 3.221972908580864  |  i\n",
            "score : 5.097929423676045  |  n\n",
            "-----\n",
            "score : 1.407135088346393  |  sa\n",
            "score : 1.9014511334691484  |  ac\n",
            "score : 2.5777173771727995  |  br\n",
            "score : 2.942466922903022  |  fa\n",
            "score : 2.9807299536675207  |  va\n",
            "score : 3.033184081588078  |  ev\n",
            "score : 3.129944118365047  |  di\n",
            "score : 3.515402492711113  |  in\n",
            "score : 3.597167843302053  |  aa\n",
            "score : 3.8572616734333307  |  ea\n",
            "-----\n",
            "score : 1.4889058012690175  |  sat\n",
            "score : 1.9255515838734272  |  act\n",
            "score : 2.594934501887651  |  bre\n",
            "score : 3.050751583516966  |  eve\n",
            "score : 3.0901850304109497  |  vac\n",
            "score : 3.190805238397256  |  dif\n",
            "score : 3.2834232418574785  |  fav\n",
            "score : 3.537150438236759  |  int\n",
            "score : 3.6823430682712317  |  aat\n",
            "score : 4.35719405401961  |  eae\n",
            "-----\n",
            "score : 1.5136523381452893  |  satu\n",
            "score : 1.9445790481050547  |  actu\n",
            "score : 2.618610318884396  |  brea\n",
            "score : 3.0859425651316355  |  ever\n",
            "score : 3.1136423132910775  |  vaca\n",
            "score : 3.23391807472244  |  diff\n",
            "score : 3.4195899042725855  |  favo\n",
            "score : 3.5723658072650673  |  inte\n",
            "score : 3.7028797195319565  |  aatu\n",
            "score : 4.735042461378017  |  eaer\n",
            "-----\n",
            "score : 1.5619211155000612  |  satur\n",
            "score : 1.9714675312139167  |  actua\n",
            "score : 2.6608456536917293  |  break\n",
            "score : 3.136592787910954  |  every\n",
            "score : 3.1401842387239576  |  vacat\n",
            "score : 3.255702387237061  |  diffe\n",
            "score : 3.4354220064348486  |  favor\n",
            "score : 3.5845129216968994  |  inter\n",
            "score : 4.132550976055755  |  aatua\n",
            "score : 4.8461416262869275  |  aatur\n",
            "-----\n",
            "score : 1.5928842947787993  |  saturd\n",
            "score : 2.0020120027837294  |  actual\n",
            "score : 2.6942269592846646  |  breakf\n",
            "score : 3.1618304293331216  |  everyo\n",
            "score : 3.190844995393434  |  vacati\n",
            "score : 3.265887103867129  |  differ\n",
            "score : 3.480382743363606  |  favori\n",
            "score : 3.6141579368326395  |  intere\n",
            "score : 4.4204348504589595  |  aatual\n",
            "score : 5.335744877617924  |  aaturd\n",
            "-----\n",
            "score : 1.6399198020453023  |  saturda\n",
            "score : 2.100860594719549  |  actuall\n",
            "score : 2.733599275230033  |  breakfa\n",
            "score : 3.2318235581324317  |  everyon\n",
            "score : 3.2811114396506915  |  vacatio\n",
            "score : 3.2919059614326955  |  differe\n",
            "score : 3.6065951724033  |  favorit\n",
            "score : 3.682546021125668  |  interes\n",
            "score : 4.665295204927732  |  aatuall\n",
            "score : 5.2670898030364555  |  actualy\n",
            "-----\n",
            "score : 1.7964089411532544  |  saturday\n",
            "score : 2.2814738879542897  |  actually\n",
            "score : 2.8440165248577074  |  breakfas\n",
            "score : 3.362646408990676  |  differen\n",
            "score : 3.529468685753005  |  vacation\n",
            "score : 3.6567677540529733  |  everyone\n",
            "score : 3.7878625892577866  |  interest\n",
            "score : 4.117559247474829  |  favorite\n",
            "score : 4.825013528016651  |  aatually\n",
            "score : 5.495783376163834  |  actualyy\n",
            "-----\n",
            "\n",
            "Input sentence: ['a', 'c', 't', 'u', 'a', 'l', 'l', 'y']\n",
            "-----\n",
            "score : 1.5237531937297029  |  a\n",
            "score : 1.5887957129818322  |  s\n",
            "score : 1.9990368334142192  |  b\n",
            "score : 2.156868897597565  |  e\n",
            "score : 2.5619669757332257  |  d\n",
            "score : 2.82418455282194  |  v\n",
            "score : 2.9394636080489938  |  f\n",
            "score : 2.9784594124355155  |  i\n",
            "score : 2.9979782232342087  |  t\n",
            "score : 5.152603151547042  |  n\n",
            "-----\n",
            "score : 1.7929201500671428  |  sa\n",
            "score : 1.802989056674548  |  ac\n",
            "score : 2.218767501437619  |  br\n",
            "score : 2.649467222011388  |  ev\n",
            "score : 2.9289795406054555  |  va\n",
            "score : 2.9966005485729994  |  di\n",
            "score : 2.9975891856192494  |  fa\n",
            "score : 3.266423414859174  |  in\n",
            "score : 3.8842754561536  |  aa\n",
            "score : 4.058302083296625  |  ea\n",
            "-----\n",
            "score : 1.8302899413294589  |  act\n",
            "score : 1.8844253305568515  |  sat\n",
            "score : 2.233768820148772  |  bre\n",
            "score : 2.6634176884071836  |  eve\n",
            "score : 3.031809427820376  |  vac\n",
            "score : 3.050373468861311  |  dif\n",
            "score : 3.2611995638888365  |  fav\n",
            "score : 3.292480569234688  |  int\n",
            "score : 3.979975729066972  |  aat\n",
            "score : 4.440205927381952  |  eae\n",
            "-----\n",
            "score : 1.8501550684329438  |  actu\n",
            "score : 1.9095408455561371  |  satu\n",
            "score : 2.257533931930703  |  brea\n",
            "score : 2.698823608440293  |  ever\n",
            "score : 3.0533261053731104  |  vaca\n",
            "score : 3.095958168853739  |  diff\n",
            "score : 3.32435762189111  |  inte\n",
            "score : 3.3728461910621  |  favo\n",
            "score : 4.002102699386053  |  aatu\n",
            "score : 4.7977461259711385  |  eaer\n",
            "-----\n",
            "score : 1.8742839114863126  |  actua\n",
            "score : 1.9632743298892998  |  satur\n",
            "score : 2.2977987385438947  |  break\n",
            "score : 2.7487126752085658  |  every\n",
            "score : 3.080462229479237  |  vacat\n",
            "score : 3.1169164180572158  |  diffe\n",
            "score : 3.33666571794918  |  inter\n",
            "score : 3.390271635017145  |  favor\n",
            "score : 4.3350194100253  |  aatua\n",
            "score : 4.937181467371207  |  eaery\n",
            "-----\n",
            "score : 1.9023346463304143  |  actual\n",
            "score : 1.9956781401250256  |  saturd\n",
            "score : 2.330055659046327  |  breakf\n",
            "score : 2.7731957919677552  |  everyo\n",
            "score : 3.1274262872653895  |  differ\n",
            "score : 3.1291414338377583  |  vacati\n",
            "score : 3.3660729666006293  |  intere\n",
            "score : 3.4327428311501813  |  favori\n",
            "score : 4.547127760165297  |  aatual\n",
            "score : 4.972361644625645  |  eaeryo\n",
            "-----\n",
            "score : 1.996202159336137  |  actuall\n",
            "score : 2.041422058251937  |  saturda\n",
            "score : 2.3692447870050666  |  breakfa\n",
            "score : 2.844940813059658  |  everyon\n",
            "score : 3.1529320723427547  |  differe\n",
            "score : 3.2156403422167923  |  vacatio\n",
            "score : 3.430512755531493  |  interes\n",
            "score : 3.5544081719576845  |  favorit\n",
            "score : 4.760742840915484  |  aatuall\n",
            "score : 5.073849894897289  |  eaeryon\n",
            "-----\n",
            "score : 2.176425237219312  |  actually\n",
            "score : 2.1978272105118313  |  saturday\n",
            "score : 2.476922858333494  |  breakfas\n",
            "score : 3.2249720965226087  |  differen\n",
            "score : 3.2605940399400506  |  everyone\n",
            "score : 3.462816098021093  |  vacation\n",
            "score : 3.5369056530315115  |  interest\n",
            "score : 4.046125744486169  |  favorite\n",
            "score : 4.920589068665045  |  aatually\n",
            "score : 5.350056324059581  |  everyont\n",
            "-----\n",
            "\n",
            "Input sentence: ['a', 'c', 't', 'u', 'a', 'l', 'l', 'y']\n",
            "-----\n",
            "score : 1.967509533634654  |  e\n",
            "score : 2.0563095432839793  |  i\n",
            "score : 2.150159214816919  |  d\n",
            "score : 2.155129725430395  |  s\n",
            "score : 2.16583803116629  |  t\n",
            "score : 2.256484757050531  |  a\n",
            "score : 2.306711441159945  |  b\n",
            "score : 2.340590770780973  |  f\n",
            "score : 2.773852125187564  |  v\n",
            "score : 5.673699153242192  |  c\n",
            "-----\n",
            "score : 2.1902795677028255  |  ev\n",
            "score : 2.2629916572806206  |  in\n",
            "score : 2.281625651849696  |  di\n",
            "score : 2.4500711766975205  |  br\n",
            "score : 2.45434481557318  |  to\n",
            "score : 2.4948888736951176  |  fa\n",
            "score : 2.530272527503329  |  ac\n",
            "score : 2.992062749163702  |  va\n",
            "score : 3.016732768535091  |  sa\n",
            "score : 3.0309772862836177  |  so\n",
            "-----\n",
            "score : 2.2034424226369604  |  eve\n",
            "score : 2.2971593189049844  |  dif\n",
            "score : 2.3239028843953493  |  int\n",
            "score : 2.4620031061709584  |  bre\n",
            "score : 2.535110789081529  |  tog\n",
            "score : 2.5730727041031245  |  act\n",
            "score : 2.5782696870420527  |  fav\n",
            "score : 3.0891200299269252  |  vac\n",
            "score : 3.2395060051616147  |  sat\n",
            "score : 3.2806065452018767  |  som\n",
            "-----\n",
            "score : 2.222179849457196  |  ever\n",
            "score : 2.3315656519906875  |  diff\n",
            "score : 2.3391847461079105  |  inte\n",
            "score : 2.4897981289880673  |  brea\n",
            "score : 2.549002930663276  |  toge\n",
            "score : 2.596258487800322  |  actu\n",
            "score : 2.6161277273712393  |  favo\n",
            "score : 3.111764316657536  |  vaca\n",
            "score : 3.278687942668976  |  satu\n",
            "score : 3.300377861476258  |  some\n",
            "-----\n",
            "score : 2.255749446032674  |  every\n",
            "score : 2.3442544025140712  |  diffe\n",
            "score : 2.3508370103732155  |  inter\n",
            "score : 2.5283830055480734  |  break\n",
            "score : 2.5705940929565854  |  toget\n",
            "score : 2.619704604204432  |  actua\n",
            "score : 2.638998298433315  |  favor\n",
            "score : 3.1408996120894  |  vacat\n",
            "score : 3.3166254628804586  |  somet\n",
            "score : 3.3229352009415987  |  satur\n",
            "-----\n",
            "score : 2.2780051261435297  |  everyo\n",
            "score : 2.354926194858626  |  differ\n",
            "score : 2.3761370043690087  |  intere\n",
            "score : 2.5589176446339157  |  breakf\n",
            "score : 2.5997246631568327  |  togeth\n",
            "score : 2.6419516300124504  |  actual\n",
            "score : 2.680833921549914  |  favori\n",
            "score : 3.188233857705666  |  vacati\n",
            "score : 3.341565027142563  |  someth\n",
            "score : 3.358999066515229  |  saturd\n",
            "-----\n",
            "score : 2.3498816261379174  |  everyon\n",
            "score : 2.377548654658022  |  differe\n",
            "score : 2.4379715125279113  |  interes\n",
            "score : 2.599037132306836  |  breakfa\n",
            "score : 2.6443163818083906  |  togethe\n",
            "score : 2.7381865443599236  |  actuall\n",
            "score : 2.7872473709561256  |  favorit\n",
            "score : 3.268733509562177  |  vacatio\n",
            "score : 3.3879081121084864  |  somethi\n",
            "score : 3.405576798141699  |  saturda\n",
            "-----\n",
            "score : 2.45556089680165  |  differen\n",
            "score : 2.5428322952764675  |  interest\n",
            "score : 2.6749324835678854  |  everyone\n",
            "score : 2.7144397192100778  |  breakfas\n",
            "score : 2.922238571102617  |  together\n",
            "score : 2.9253608745582453  |  actually\n",
            "score : 3.1906810419670615  |  favorite\n",
            "score : 3.475551888394518  |  somethin\n",
            "score : 3.53896478311963  |  vacation\n",
            "score : 3.5636795748575913  |  saturday\n",
            "-----\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b43dffe603f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# inference(test_datasets, keydelay_tokens, keytext_tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbeam_search_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeydelay_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeytext_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-e69b1a9ee4bf>\u001b[0m in \u001b[0;36mbeam_search_inference\u001b[0;34m(test_datasets, keydelay_tokens, keytext_tokens)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input sentence:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mgenerate_beam_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_next_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_search_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreak_at_eos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# inference(test_datasets, keydelay_tokens, keytext_tokens)\n",
        "beam_search_inference(test_datasets, keydelay_tokens, keytext_tokens)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Camstroke-Inference.ipynb",
      "provenance": [],
      "mount_file_id": "1FEw-o3zi1UWpFDKakC83SfnMkhrx6sBf",
      "authorship_tag": "ABX9TyPlzZik+Va4MlQY4/euve3I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
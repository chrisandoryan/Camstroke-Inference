{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisandoryan/Camstroke-Inference/blob/main/Camstroke_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBjlSxp4LrGr"
      },
      "source": [
        "# Import and Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6pPCxpiFLS2G"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVRaCR2_LvE8"
      },
      "source": [
        "# Model Builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "MZTpw5pRLdHE"
      },
      "outputs": [],
      "source": [
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "\n",
        "def build_models(num_encoder_tokens, num_decoder_tokens):\n",
        "    \"\"\"\n",
        "    ## Build the model\n",
        "    \"\"\"\n",
        "\n",
        "    # Define an input sequence and process it.\n",
        "    encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "    encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "    # We discard `encoder_outputs` and only keep the states.\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "    # We set up our decoder to return full output sequences,\n",
        "    # and to return internal states as well. We don't use the\n",
        "    # return states in the training model, but we will use them in inference.\n",
        "    decoder_lstm = keras.layers.LSTM(latent_dim, dropout=0.2, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "    # dropout = keras.layers.Dropout(rate=0.2)\n",
        "    # decoder_outputs = dropout(decoder_outputs)\n",
        "    \n",
        "    decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model that will turn\n",
        "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_models(path):\n",
        "    \"\"\"\n",
        "    ## Run inference (sampling)\n",
        "\n",
        "    1. encode input and retrieve initial decoder state\n",
        "    2. run one step of decoder with this initial state\n",
        "    and a \"start of sequence\" token as target.\n",
        "    Output will be the next target token.\n",
        "    3. Repeat with the current target token and current states\n",
        "    \"\"\"\n",
        "\n",
        "    # Define sampling models\n",
        "    # Restore the model and construct the encoder and decoder.\n",
        "    model = keras.models.load_model(path)\n",
        "    model.summary()\n",
        "\n",
        "    encoder_inputs = model.input[0]  # input_1\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "    encoder_states = [state_h_enc, state_c_enc]\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_inputs = model.input[1]  # input_2\n",
        "    decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_7\")\n",
        "    decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_8\")\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_lstm = model.layers[3]\n",
        "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "        decoder_inputs, initial_state=decoder_states_inputs\n",
        "    )\n",
        "    decoder_states = [state_h_dec, state_c_dec]\n",
        "    decoder_dense = model.layers[4]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "    )\n",
        "\n",
        "    return model, encoder_model, decoder_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32aJNHtmL3Q5"
      },
      "source": [
        "# Training and Evaluation Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Z4Wt8-Mi05"
      },
      "source": [
        "## Parameter Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "94Ib_9KNMYxP"
      },
      "outputs": [],
      "source": [
        "batch_size = 32  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "\n",
        "START_TOKEN = \"\\t\"\n",
        "END_TOKEN = \"\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z79A-zZNMp16"
      },
      "source": [
        "## Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "gyIdXN8HLx7P"
      },
      "outputs": [],
      "source": [
        "def prepare(datasets, keydelay_tokens, keytext_tokens):\n",
        "    # Vectorize the data.\n",
        "    keydelay_inputs = []\n",
        "    keytext_targets = []\n",
        "    keydelay_tokens = sorted(list(keydelay_tokens))\n",
        "    keytext_tokens = [START_TOKEN, END_TOKEN] + list(keytext_tokens)\n",
        "    keytext_tokens = sorted(list(keytext_tokens))\n",
        "    num_encoder_tokens = max(keydelay_tokens) + 1\n",
        "    num_decoder_tokens = len(keytext_tokens)\n",
        "\n",
        "    for d in datasets:\n",
        "        # train_df, test_df = np.split(d, [int(.9*len(d))])\n",
        "        keydelays = d['keydelay'].tolist()\n",
        "        keytexts = d['key2'].tolist()\n",
        "        keytexts = [START_TOKEN] + keytexts + [END_TOKEN]\n",
        "\n",
        "        keydelays = to_categorical(keydelays, num_classes=num_encoder_tokens)\n",
        "        keydelay_inputs.append(keydelays)\n",
        "        keytext_targets.append(keytexts)\n",
        "\n",
        "    max_encoder_seq_length = max([len(i) for i in keydelay_inputs])\n",
        "    max_decoder_seq_length = max([len(t) for t in keytext_targets])\n",
        "\n",
        "    print(\"Number of samples:\", len(keydelay_inputs))\n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "    input_token_index = dict([(char, i)\n",
        "                              for i, char in enumerate(keydelay_tokens)])\n",
        "    target_token_index = dict([(char, i)\n",
        "                               for i, char in enumerate(keytext_tokens)])\n",
        "\n",
        "    # one-hot encoding\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(keydelay_inputs), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(keydelay_inputs), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(keydelay_inputs), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    for i, (keydelay, keytext) in enumerate(zip(keydelay_inputs, keytext_targets)):\n",
        "        encoder_input_data[i, 0:len(keydelay)] = keydelay\n",
        "\n",
        "        for t, text in enumerate(keytext):\n",
        "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "            decoder_input_data[i, t, target_token_index[text]] = 1.0\n",
        "            if t > 0:\n",
        "                # decoder_target_data will be ahead by one timestep\n",
        "                # and will not include the start character.\n",
        "                decoder_target_data[i, t - 1, target_token_index[text]] = 1.0\n",
        "\n",
        "        # what's this for?\n",
        "        # i thinks this is to disregard the ' ' (on the tutorial they used target_token_index[' ']) so that the model won't predict it.\n",
        "        # decoder_input_data[i, t + 1:, target_token_index['a']] = 1.0\n",
        "        # decoder_target_data[i, t:, target_token_index['a']] = 1.0\n",
        "\n",
        "    # print(encoder_input_data)\n",
        "    print(encoder_input_data.shape)\n",
        "    # print(decoder_input_data)\n",
        "    print(decoder_input_data.shape)\n",
        "    # print(decoder_target_data)\n",
        "    print(decoder_target_data.shape)\n",
        "    # input()\n",
        "\n",
        "    # variables to be passed back for futher processing\n",
        "    data = (encoder_input_data, decoder_input_data, decoder_target_data)\n",
        "    tokens = (num_encoder_tokens, num_decoder_tokens)\n",
        "    sequences = (max_encoder_seq_length, max_decoder_seq_length)\n",
        "    indices = (input_token_index, target_token_index)\n",
        "\n",
        "    return data, tokens, sequences, indices\n",
        "\n",
        "def plot_training_performance(H):\n",
        "    # construct a plot that plots and saves the training history\n",
        "    N = np.arange(0, min(epochs, len(H.history[\"loss\"])))\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.savefig(f\"{model_path}/loss_plot.png\")\n",
        "\n",
        "    # construct a plot that plots and saves the training history\n",
        "    N = np.arange(0, min(epochs, len(H.history[\"accuracy\"])))\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(N, H.history[\"accuracy\"], label=\"train_accuracy\")\n",
        "    plt.plot(N, H.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.savefig(f\"{model_path}/acc_plot.png\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "YbAPPotgMdCL"
      },
      "outputs": [],
      "source": [
        "def generator(X_data, y_data, batch_size):\n",
        "  samples_per_epoch = len(X_data)\n",
        "  number_of_batches = samples_per_epoch / batch_size\n",
        "  counter = 0\n",
        "\n",
        "  while True:\n",
        "    X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "    y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
        "    counter += 1\n",
        "    yield X_batch,y_batch\n",
        "\n",
        "    # restart counter to yield data in the next epoch as well\n",
        "    if counter >= number_of_batches:\n",
        "        counter = 0\n",
        "\n",
        "def train(train_datasets, test_datasets, keydelay_tokens, keytext_tokens):\n",
        "    data, tokens, sequences, indices = prepare(\n",
        "        train_datasets, keydelay_tokens, keytext_tokens)\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data = data\n",
        "    num_encoder_tokens, num_decoder_tokens = tokens\n",
        "    max_encoder_seq_length, max_decoder_seq_length = sequences\n",
        "    input_token_index, target_token_index = indices\n",
        "\n",
        "    model = build_models(num_encoder_tokens, num_decoder_tokens)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    overfitCallback = EarlyStopping(monitor='val_loss', mode='min', patience=15)\n",
        "\n",
        "    history = model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[overfitCallback]\n",
        "    )\n",
        "\n",
        "    # history = model.fit_generator(\n",
        "    #     generator([encoder_input_data, decoder_input_data], decoder_target_data, batch_size),\n",
        "    #     epochs=epochs,\n",
        "    #     steps_per_epoch = len(train_datasets) / batch_size,\n",
        "    # )\n",
        "\n",
        "    # Save model\n",
        "    model.save(model_path)\n",
        "\n",
        "    # Plot training loss and accuracy\n",
        "    plot_training_performance(history)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA59tq1aM2nd"
      },
      "source": [
        "## Inference Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VskNqs_aMxEr"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/greatrxt/smartreply-word-level-seq2seq-with-beam-search\n",
        "def generate_beam_predictions(input_seq, n_next_sequences, beam_search_n, break_at_eos, models, tokens, indices):\n",
        "    model, encoder_model, decoder_model = models\n",
        "    num_encoder_tokens, num_decoder_tokens = tokens\n",
        "    input_token_index, target_token_index = indices\n",
        "\n",
        "    # Reverse-lookup token index to decode sequences back to\n",
        "    # something readable.\n",
        "    reverse_input_char_index = dict((i, char)\n",
        "                                    for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char)\n",
        "                                     for char, i in target_token_index.items())\n",
        "\n",
        "    distributions_scores_states = [[list(), 0.0, [None, None]]]\n",
        "    decoder_states_value = None\n",
        "\n",
        "    for _ in range(n_next_sequences):\n",
        "        sequence_temp_candidates = list()\n",
        "        for i in range(len(distributions_scores_states)):\n",
        "            # Generate empty target sequence of length 1.\n",
        "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "\n",
        "            seq, score, states_values = distributions_scores_states[i]\n",
        "\n",
        "            if len(distributions_scores_states) == 1:\n",
        "                # Encode the input as state vectors.\n",
        "                decoder_states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "                # Populate the first character of target sequence with the start character.\n",
        "                target_seq[0, 0, target_token_index[START_TOKEN]] = 1.0\n",
        "            else:\n",
        "                target_seq[0, 0, seq[-1]] = 1.0\n",
        "                decoder_states_value = states_values\n",
        "\n",
        "                candidate_sentence = \"\"\n",
        "                for token_index in seq:\n",
        "                    word = reverse_target_char_index[token_index]\n",
        "                    # print(\"Token Index\", token_index)\n",
        "                    # print(\"Char\", word)\n",
        "                    if word == END_TOKEN:\n",
        "                        break\n",
        "\n",
        "                    candidate_sentence += word\n",
        "\n",
        "                print(\"score :\", score, \" | \", candidate_sentence)\n",
        "\n",
        "            output_tokens_distribution, h, c = decoder_model.predict([target_seq] + decoder_states_value)\n",
        "            # print(\"OutputTokens\", output_tokens_distribution)\n",
        "\n",
        "            # Update states\n",
        "            decoder_states_value = [h, c]\n",
        "\n",
        "            predicted_distribution = output_tokens_distribution[0][-1]\n",
        "\n",
        "            for j in range(len(predicted_distribution)):\n",
        "                # print(\"J\", j)\n",
        "                # print(\"PREDIST\", predicted_distribution[j])\n",
        "                if predicted_distribution[j] > 0:\n",
        "                    candidate = [seq + [j], score - log(predicted_distribution[j]), decoder_states_value]\n",
        "                    # print(candidate)\n",
        "                    # input()\n",
        "                    if break_at_eos and j == END_TOKEN:\n",
        "                        continue\n",
        "                    else:\n",
        "                        sequence_temp_candidates.append(candidate)\n",
        "\n",
        "        # 2. score and sort all candidates\n",
        "        # print(\"SEQTEMP\", sequence_temp_candidates[:beam_search_n])\n",
        "        ordered = sorted(sequence_temp_candidates, key=lambda tup: tup[1], reverse=False) \n",
        "        # print(\"ORDERED\", ordered[:beam_search_n])\n",
        "        distributions_scores_states = ordered[:beam_search_n]\n",
        "        # print(\"DIST\", distributions_scores_states)\n",
        "        # input()\n",
        "        print(\"-----\")\n",
        "\n",
        "def decode_sequence(models, input_seq, num_decoder_tokens, max_decoder_seq_length, input_token_index, target_token_index):\n",
        "    model, encoder_model, decoder_model = models\n",
        "\n",
        "    # Reverse-lookup token index to decode sequences back to\n",
        "    # something readable.\n",
        "    reverse_input_char_index = dict((i, char)\n",
        "                                    for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char)\n",
        "                                     for char, i in target_token_index.items())\n",
        "\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[START_TOKEN]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        # print(output_tokens)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == END_TOKEN or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpVaJjspNHLL"
      },
      "source": [
        "## Inference Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "XmScw3RrM5Hw"
      },
      "outputs": [],
      "source": [
        "def inference(test_datasets, keydelay_tokens, keytext_tokens):\n",
        "    models = load_models(model_path)\n",
        "    data, tokens, sequences, indices = prepare(\n",
        "        test_datasets, keydelay_tokens, keytext_tokens)\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data = data\n",
        "    num_encoder_tokens, num_decoder_tokens = tokens\n",
        "    max_encoder_seq_length, max_decoder_seq_length = sequences\n",
        "    input_token_index, target_token_index = indices\n",
        "\n",
        "    total, correct = len(test_datasets), 0\n",
        "    for i, d in enumerate(test_datasets):\n",
        "        # Take one sequence (part of the training set)\n",
        "        # for trying out decoding.\n",
        "        input_seq = encoder_input_data[i: i + 1]\n",
        "        target_text = d['key2'].tolist()\n",
        "        decoded_sentence = decode_sequence(\n",
        "            models, input_seq, num_decoder_tokens, max_decoder_seq_length, input_token_index, target_token_index)\n",
        "        print(\"-\")\n",
        "        print(\"Input sentence:\", target_text)\n",
        "        print(\"Decoded sentence:\", decoded_sentence)\n",
        "        if ''.join(target_text) == decoded_sentence.strip():\n",
        "            correct += 1\n",
        "            print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
        "\n",
        "    return\n",
        "\n",
        "def beam_search_inference(test_datasets, keydelay_tokens, keytext_tokens):\n",
        "    models = load_models(model_path)\n",
        "    data, tokens, sequences, indices = prepare(\n",
        "        test_datasets, keydelay_tokens, keytext_tokens)\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data = data\n",
        "\n",
        "    for i, d in enumerate(test_datasets):\n",
        "        # Take one sequence (part of the training set)\n",
        "        # for trying out decoding.\n",
        "        input_seq = encoder_input_data[i: i + 1]\n",
        "        target_text = d['key2'].tolist()\n",
        "        print(\"Input sentence:\", target_text)\n",
        "        generate_beam_predictions(input_seq, n_next_sequences=len(target_text) + 1, beam_search_n=10, break_at_eos=True, models=models, tokens=tokens, indices=indices)\n",
        "        input()\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12H_GmDXMlJR"
      },
      "source": [
        "## Dataset Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "7WS6z4F-MZYZ"
      },
      "outputs": [],
      "source": [
        "def split_dataset(df, datasets):\n",
        "    # shuffle the datasets\n",
        "    random.shuffle(datasets)\n",
        "\n",
        "    # split to train/test dataset with 80:20 ratio.\n",
        "    train_datasets = datasets[:int(.80*len(datasets))]\n",
        "    test_datasets = datasets[:int(.20*len(datasets))]\n",
        "\n",
        "    keytext_tokens = df['keyname'].unique()\n",
        "    keydelay_tokens = df['keydelay'].unique()\n",
        "\n",
        "    df = None\n",
        "    return train_datasets, test_datasets, keytext_tokens, keydelay_tokens\n",
        "\n",
        "def read_villani_dataset():\n",
        "    data_path = \"./drive/MyDrive/S2/Thesis S2/villani_keystrokes.csv\"\n",
        "    df = pd.read_csv(data_path, header=0)\n",
        "    df['keydelay'] = df[\"timerelease\"] - df[\"timepress\"]\n",
        "    df['key2'] = df[\"keyname\"]\n",
        "\n",
        "    return df\n",
        "\n",
        "def split_words_villani(df):\n",
        "    chunk = []\n",
        "    for i, d in df.iterrows():\n",
        "        if len(d['keyname']) > 1:\n",
        "            if len(chunk) > 1:\n",
        "                chunk_df = pd.DataFrame(chunk)\n",
        "                yield chunk_df\n",
        "                chunk = []\n",
        "        else:\n",
        "            chunk.append(d.to_dict())\n",
        "\n",
        "def clean_dataset_villani(df):\n",
        "    prune = [\"inputtype\",\"repetition\",\"platform\",\"gender\",\"agegroup\",\"handedness\",\"awareness\",\"location\"]\n",
        "    for p in prune:\n",
        "        if p in df.columns:\n",
        "            df.drop(p, axis=1, inplace=True)\n",
        "\n",
        "def create_directory(directory):\n",
        "  if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "def save_processed_dataset(data):\n",
        "  path = f\"{model_path}/camstroke-inference-dataset.dat\"\n",
        "  create_directory(model_path)\n",
        "  with open(path, \"wb\") as f:\n",
        "        return pickle.dump(data, f)\n",
        "\n",
        "def load_processed_dataset(data):\n",
        "    path = f\"{model_path}/camstroke-inference-dataset.dat\"\n",
        "    create_directory(model_path)\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def read_rockyou_wordlist(n_data=50000):\n",
        "    data_path = \"./drive/MyDrive/S2/Thesis S2/rockyou.txt\"\n",
        "    with open(data_path, 'r', encoding='latin-1') as f:\n",
        "        data = [r for r in f.read().splitlines()]\n",
        "        if n_data > 0:\n",
        "            data = data[:n_data] # Top n only\n",
        "        df = pd.Series(data)\n",
        "        df.columns = ['password']\n",
        "        print(df.head())\n",
        "\n",
        "    return df\n",
        "\n",
        "def dfword_to_gram(df, n):\n",
        "    for i in range(0, df.shape[0], n):\n",
        "        yield df[max(0, i-1):i+n]\n",
        "    # return np.array_split(df, n)\n",
        "  \n",
        "def make_balanced_dataset(dataset, word_dictionary, min_sample=1000):\n",
        "    vc = pd.value_counts(np.array(word_dictionary))\n",
        "    print(vc)\n",
        "    for col_name, data in vc[(vc < min_sample)].items():\n",
        "      # print(col_name, data, 'hehe')\n",
        "      indices = [i for i, x in enumerate(word_dictionary) if x == col_name]\n",
        "      for i, x in enumerate(indices):\n",
        "        x = x - i\n",
        "        word_dictionary.pop(x)\n",
        "        dataset.pop(x)\n",
        "    vc = pd.value_counts(np.array(word_dictionary))\n",
        "    print(vc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Vc3dULOGQT"
      },
      "source": [
        "# Training Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "b5pdnRD1RYjN"
      },
      "outputs": [],
      "source": [
        "model_path = \"./drive/MyDrive/S2/camstroke-inference-models/s2s_model_top100000rockyou_fixedtype_cleanwords_2\"  # Path to store/load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2ju8IEzOyaO",
        "outputId": "68011609-4af3-428f-fd4c-49fda480e5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject 1/39 | Session ID: 207 | Exercise ID: fable\n",
            "Current Dataset Length:  0\n",
            "Subject 1/39 | Session ID: 422 | Exercise ID: fable\n",
            "Current Dataset Length:  59\n",
            "Subject 1/39 | Session ID: 430 | Exercise ID: fable\n",
            "Current Dataset Length:  120\n",
            "Subject 1/39 | Session ID: 590 | Exercise ID: fable\n",
            "Current Dataset Length:  181\n",
            "Subject 1/39 | Session ID: 670 | Exercise ID: fable\n",
            "Current Dataset Length:  242\n",
            "Subject 1/39 | Session ID: 919 | Exercise ID: fable\n",
            "Current Dataset Length:  302\n",
            "Subject 1/39 | Session ID: 1360 | Exercise ID: fable\n",
            "Current Dataset Length:  367\n",
            "Subject 1/39 | Session ID: 1476 | Exercise ID: fable\n",
            "Current Dataset Length:  427\n",
            "Subject 1/39 | Session ID: 1654 | Exercise ID: fable\n",
            "Current Dataset Length:  483\n",
            "Subject 2/39 | Session ID: 157 | Exercise ID: fable\n",
            "Current Dataset Length:  543\n",
            "Subject 2/39 | Session ID: 172 | Exercise ID: fable\n",
            "Current Dataset Length:  602\n",
            "Subject 2/39 | Session ID: 391 | Exercise ID: fable\n",
            "Current Dataset Length:  663\n",
            "Subject 2/39 | Session ID: 523 | Exercise ID: fable\n",
            "Current Dataset Length:  722\n",
            "Subject 2/39 | Session ID: 727 | Exercise ID: fable\n",
            "Current Dataset Length:  781\n",
            "Subject 2/39 | Session ID: 758 | Exercise ID: fable\n",
            "Current Dataset Length:  842\n",
            "Subject 2/39 | Session ID: 1046 | Exercise ID: fable\n",
            "Current Dataset Length:  899\n",
            "Subject 2/39 | Session ID: 1382 | Exercise ID: fable\n",
            "Current Dataset Length:  958\n",
            "Subject 2/39 | Session ID: 1636 | Exercise ID: fable\n",
            "Current Dataset Length:  1018\n",
            "Subject 3/39 | Session ID: 112 | Exercise ID: fable\n",
            "Current Dataset Length:  1080\n",
            "Subject 3/39 | Session ID: 378 | Exercise ID: fable\n",
            "Current Dataset Length:  1140\n",
            "Subject 3/39 | Session ID: 1168 | Exercise ID: fable\n",
            "Current Dataset Length:  1204\n",
            "Subject 3/39 | Session ID: 1197 | Exercise ID: fable\n",
            "Current Dataset Length:  1264\n",
            "Subject 3/39 | Session ID: 1245 | Exercise ID: fable\n",
            "Current Dataset Length:  1323\n",
            "Subject 3/39 | Session ID: 1401 | Exercise ID: fable\n",
            "Current Dataset Length:  1384\n",
            "Subject 3/39 | Session ID: 1609 | Exercise ID: fable\n",
            "Current Dataset Length:  1443\n",
            "Subject 3/39 | Session ID: 1692 | Exercise ID: fable\n",
            "Current Dataset Length:  1506\n",
            "Subject 4/39 | Session ID: 129 | Exercise ID: fable\n",
            "Current Dataset Length:  1564\n",
            "Subject 4/39 | Session ID: 218 | Exercise ID: fable\n",
            "Current Dataset Length:  1622\n",
            "Subject 4/39 | Session ID: 268 | Exercise ID: fable\n",
            "Current Dataset Length:  1680\n",
            "Subject 4/39 | Session ID: 368 | Exercise ID: fable\n",
            "Current Dataset Length:  1740\n",
            "Subject 4/39 | Session ID: 501 | Exercise ID: fable\n",
            "Current Dataset Length:  1802\n",
            "Subject 4/39 | Session ID: 674 | Exercise ID: fable\n",
            "Current Dataset Length:  1861\n",
            "Subject 4/39 | Session ID: 800 | Exercise ID: fable\n",
            "Current Dataset Length:  1924\n",
            "Subject 4/39 | Session ID: 976 | Exercise ID: fable\n",
            "Current Dataset Length:  1983\n",
            "Subject 4/39 | Session ID: 1222 | Exercise ID: fable\n",
            "Current Dataset Length:  2043\n",
            "Subject 4/39 | Session ID: 1344 | Exercise ID: fable\n",
            "Current Dataset Length:  2106\n",
            "Subject 5/39 | Session ID: 349 | Exercise ID: fable\n",
            "Current Dataset Length:  2166\n",
            "Subject 5/39 | Session ID: 612 | Exercise ID: fable\n",
            "Current Dataset Length:  2227\n",
            "Subject 5/39 | Session ID: 624 | Exercise ID: fable\n",
            "Current Dataset Length:  2288\n",
            "Subject 5/39 | Session ID: 718 | Exercise ID: fable\n",
            "Current Dataset Length:  2345\n",
            "Subject 5/39 | Session ID: 830 | Exercise ID: fable\n",
            "Current Dataset Length:  2403\n",
            "Subject 5/39 | Session ID: 841 | Exercise ID: fable\n",
            "Current Dataset Length:  2464\n",
            "Subject 5/39 | Session ID: 925 | Exercise ID: fable\n",
            "Current Dataset Length:  2524\n",
            "Subject 5/39 | Session ID: 1162 | Exercise ID: fable\n",
            "Current Dataset Length:  2585\n",
            "Subject 5/39 | Session ID: 1600 | Exercise ID: fable\n",
            "Current Dataset Length:  2645\n",
            "Subject 6/39 | Session ID: 180 | Exercise ID: fable\n",
            "Current Dataset Length:  2705\n",
            "Subject 6/39 | Session ID: 211 | Exercise ID: fable\n",
            "Current Dataset Length:  2766\n",
            "Subject 6/39 | Session ID: 221 | Exercise ID: fable\n",
            "Current Dataset Length:  2826\n",
            "Subject 6/39 | Session ID: 394 | Exercise ID: fable\n",
            "Current Dataset Length:  2886\n",
            "Subject 6/39 | Session ID: 405 | Exercise ID: fable\n",
            "Current Dataset Length:  2946\n",
            "Subject 6/39 | Session ID: 703 | Exercise ID: fable\n",
            "Current Dataset Length:  3007\n",
            "Subject 6/39 | Session ID: 706 | Exercise ID: fable\n",
            "Current Dataset Length:  3069\n",
            "Subject 6/39 | Session ID: 799 | Exercise ID: fable\n",
            "Current Dataset Length:  3126\n",
            "Subject 6/39 | Session ID: 837 | Exercise ID: fable\n",
            "Current Dataset Length:  3185\n",
            "Subject 6/39 | Session ID: 1410 | Exercise ID: fable\n",
            "Current Dataset Length:  3243\n",
            "Subject 7/39 | Session ID: 68 | Exercise ID: fable\n",
            "Current Dataset Length:  3303\n",
            "Subject 7/39 | Session ID: 220 | Exercise ID: fable\n",
            "Current Dataset Length:  3360\n",
            "Subject 7/39 | Session ID: 284 | Exercise ID: fable\n",
            "Current Dataset Length:  3416\n",
            "Subject 7/39 | Session ID: 454 | Exercise ID: fable\n",
            "Current Dataset Length:  3477\n",
            "Subject 7/39 | Session ID: 652 | Exercise ID: fable\n",
            "Current Dataset Length:  3538\n",
            "Subject 7/39 | Session ID: 667 | Exercise ID: fable\n",
            "Current Dataset Length:  3598\n",
            "Subject 7/39 | Session ID: 787 | Exercise ID: fable\n",
            "Current Dataset Length:  3659\n",
            "Subject 7/39 | Session ID: 1068 | Exercise ID: fable\n",
            "Current Dataset Length:  3718\n",
            "Subject 7/39 | Session ID: 1568 | Exercise ID: fable\n",
            "Current Dataset Length:  3779\n",
            "Subject 7/39 | Session ID: 1633 | Exercise ID: fable\n",
            "Current Dataset Length:  3841\n",
            "Subject 8/39 | Session ID: 1322 | Exercise ID: fable\n",
            "Current Dataset Length:  3901\n",
            "Subject 9/39 | Session ID: 95 | Exercise ID: fable\n",
            "Current Dataset Length:  3982\n",
            "Subject 9/39 | Session ID: 177 | Exercise ID: fable\n",
            "Current Dataset Length:  4047\n",
            "Subject 9/39 | Session ID: 279 | Exercise ID: fable\n",
            "Current Dataset Length:  4112\n",
            "Subject 9/39 | Session ID: 301 | Exercise ID: fable\n",
            "Current Dataset Length:  4175\n",
            "Subject 9/39 | Session ID: 1054 | Exercise ID: fable\n",
            "Current Dataset Length:  4236\n",
            "Subject 9/39 | Session ID: 1152 | Exercise ID: fable\n",
            "Current Dataset Length:  4303\n",
            "Subject 9/39 | Session ID: 1317 | Exercise ID: fable\n",
            "Current Dataset Length:  4378\n",
            "Subject 9/39 | Session ID: 1416 | Exercise ID: fable\n",
            "Current Dataset Length:  4448\n",
            "Subject 9/39 | Session ID: 1519 | Exercise ID: fable\n",
            "Current Dataset Length:  4505\n",
            "Subject 10/39 | Session ID: 75 | Exercise ID: fable\n",
            "Current Dataset Length:  4569\n",
            "Subject 10/39 | Session ID: 119 | Exercise ID: fable\n",
            "Current Dataset Length:  4631\n",
            "Subject 10/39 | Session ID: 589 | Exercise ID: fable\n",
            "Current Dataset Length:  4692\n",
            "Subject 10/39 | Session ID: 831 | Exercise ID: fable\n",
            "Current Dataset Length:  4751\n",
            "Subject 10/39 | Session ID: 1011 | Exercise ID: fable\n",
            "Current Dataset Length:  4812\n",
            "Subject 10/39 | Session ID: 1229 | Exercise ID: fable\n",
            "Current Dataset Length:  4871\n",
            "Subject 10/39 | Session ID: 1395 | Exercise ID: fable\n",
            "Current Dataset Length:  4929\n",
            "Subject 10/39 | Session ID: 1549 | Exercise ID: fable\n",
            "Current Dataset Length:  4989\n",
            "Subject 11/39 | Session ID: 73 | Exercise ID: fable\n",
            "Current Dataset Length:  5049\n",
            "Subject 11/39 | Session ID: 179 | Exercise ID: fable\n",
            "Current Dataset Length:  5110\n",
            "Subject 11/39 | Session ID: 193 | Exercise ID: fable\n",
            "Current Dataset Length:  5169\n",
            "Subject 11/39 | Session ID: 535 | Exercise ID: fable\n",
            "Current Dataset Length:  5227\n",
            "Subject 11/39 | Session ID: 891 | Exercise ID: fable\n",
            "Current Dataset Length:  5287\n",
            "Subject 11/39 | Session ID: 979 | Exercise ID: fable\n",
            "Current Dataset Length:  5348\n",
            "Subject 11/39 | Session ID: 1096 | Exercise ID: fable\n",
            "Current Dataset Length:  5410\n",
            "Subject 11/39 | Session ID: 1186 | Exercise ID: fable\n",
            "Current Dataset Length:  5470\n",
            "Subject 11/39 | Session ID: 1307 | Exercise ID: fable\n",
            "Current Dataset Length:  5526\n",
            "Subject 11/39 | Session ID: 1560 | Exercise ID: fable\n",
            "Current Dataset Length:  5587\n",
            "Subject 12/39 | Session ID: 543 | Exercise ID: fable\n",
            "Current Dataset Length:  5646\n",
            "Subject 12/39 | Session ID: 713 | Exercise ID: fable\n",
            "Current Dataset Length:  5706\n",
            "Subject 12/39 | Session ID: 775 | Exercise ID: fable\n",
            "Current Dataset Length:  5768\n",
            "Subject 12/39 | Session ID: 846 | Exercise ID: fable\n",
            "Current Dataset Length:  5828\n",
            "Subject 12/39 | Session ID: 1178 | Exercise ID: fable\n",
            "Current Dataset Length:  5888\n",
            "Subject 12/39 | Session ID: 1198 | Exercise ID: fable\n",
            "Current Dataset Length:  5948\n",
            "Subject 12/39 | Session ID: 1208 | Exercise ID: fable\n",
            "Current Dataset Length:  6009\n",
            "Subject 12/39 | Session ID: 1591 | Exercise ID: fable\n",
            "Current Dataset Length:  6068\n",
            "Subject 13/39 | Session ID: 50 | Exercise ID: fable\n",
            "Current Dataset Length:  6127\n",
            "Subject 13/39 | Session ID: 60 | Exercise ID: fable\n",
            "Current Dataset Length:  6189\n",
            "Subject 13/39 | Session ID: 812 | Exercise ID: fable\n",
            "Current Dataset Length:  6248\n",
            "Subject 13/39 | Session ID: 946 | Exercise ID: fable\n",
            "Current Dataset Length:  6308\n",
            "Subject 13/39 | Session ID: 1296 | Exercise ID: fable\n",
            "Current Dataset Length:  6369\n",
            "Subject 13/39 | Session ID: 1372 | Exercise ID: fable\n",
            "Current Dataset Length:  6427\n",
            "Subject 13/39 | Session ID: 1415 | Exercise ID: fable\n",
            "Current Dataset Length:  6486\n",
            "Subject 13/39 | Session ID: 1547 | Exercise ID: fable\n",
            "Current Dataset Length:  6538\n",
            "Subject 13/39 | Session ID: 1574 | Exercise ID: fable\n",
            "Current Dataset Length:  6598\n",
            "Subject 14/39 | Session ID: 118 | Exercise ID: fable\n",
            "Current Dataset Length:  6657\n",
            "Subject 14/39 | Session ID: 175 | Exercise ID: fable\n",
            "Current Dataset Length:  6720\n",
            "Subject 14/39 | Session ID: 194 | Exercise ID: fable\n",
            "Current Dataset Length:  6810\n",
            "Subject 14/39 | Session ID: 402 | Exercise ID: fable\n",
            "Current Dataset Length:  6872\n",
            "Subject 14/39 | Session ID: 476 | Exercise ID: fable\n",
            "Current Dataset Length:  6970\n",
            "Subject 14/39 | Session ID: 494 | Exercise ID: fable\n",
            "Current Dataset Length:  7028\n",
            "Subject 14/39 | Session ID: 608 | Exercise ID: fable\n",
            "Current Dataset Length:  7109\n",
            "Subject 14/39 | Session ID: 627 | Exercise ID: fable\n",
            "Current Dataset Length:  7168\n",
            "Subject 14/39 | Session ID: 634 | Exercise ID: fable\n",
            "Current Dataset Length:  7227\n",
            "Subject 14/39 | Session ID: 804 | Exercise ID: fable\n",
            "Current Dataset Length:  7315\n",
            "Subject 14/39 | Session ID: 914 | Exercise ID: fable\n",
            "Current Dataset Length:  7373\n",
            "Subject 14/39 | Session ID: 1132 | Exercise ID: fable\n",
            "Current Dataset Length:  7430\n",
            "Subject 14/39 | Session ID: 1252 | Exercise ID: fable\n",
            "Current Dataset Length:  7493\n",
            "Subject 14/39 | Session ID: 1653 | Exercise ID: fable\n",
            "Current Dataset Length:  7580\n",
            "Subject 14/39 | Session ID: 1707 | Exercise ID: fable\n",
            "Current Dataset Length:  7640\n",
            "Subject 15/39 | Session ID: 96 | Exercise ID: fable\n",
            "Current Dataset Length:  7702\n",
            "Subject 15/39 | Session ID: 481 | Exercise ID: fable\n",
            "Current Dataset Length:  7760\n",
            "Subject 15/39 | Session ID: 820 | Exercise ID: fable\n",
            "Current Dataset Length:  7819\n",
            "Subject 15/39 | Session ID: 1113 | Exercise ID: fable\n",
            "Current Dataset Length:  7880\n",
            "Subject 15/39 | Session ID: 1284 | Exercise ID: fable\n",
            "Current Dataset Length:  7941\n",
            "Subject 15/39 | Session ID: 1425 | Exercise ID: fable\n",
            "Current Dataset Length:  7999\n",
            "Subject 15/39 | Session ID: 1456 | Exercise ID: fable\n",
            "Current Dataset Length:  8058\n",
            "Subject 15/39 | Session ID: 1503 | Exercise ID: fable\n",
            "Current Dataset Length:  8116\n",
            "Subject 15/39 | Session ID: 1571 | Exercise ID: fable\n",
            "Current Dataset Length:  8175\n",
            "Subject 15/39 | Session ID: 1709 | Exercise ID: fable\n",
            "Current Dataset Length:  8235\n",
            "Subject 16/39 | Session ID: 34 | Exercise ID: fable\n",
            "Current Dataset Length:  8295\n",
            "Subject 16/39 | Session ID: 44 | Exercise ID: fable\n",
            "Current Dataset Length:  8357\n",
            "Subject 16/39 | Session ID: 51 | Exercise ID: fable\n",
            "Current Dataset Length:  8422\n",
            "Subject 16/39 | Session ID: 446 | Exercise ID: fable\n",
            "Current Dataset Length:  8479\n",
            "Subject 16/39 | Session ID: 714 | Exercise ID: fable\n",
            "Current Dataset Length:  8538\n",
            "Subject 16/39 | Session ID: 1313 | Exercise ID: fable\n",
            "Current Dataset Length:  8598\n",
            "Subject 16/39 | Session ID: 1399 | Exercise ID: fable\n",
            "Current Dataset Length:  8659\n",
            "Subject 16/39 | Session ID: 1453 | Exercise ID: fable\n",
            "Current Dataset Length:  8718\n",
            "Subject 16/39 | Session ID: 1526 | Exercise ID: fable\n",
            "Current Dataset Length:  8776\n",
            "Subject 17/39 | Session ID: 113 | Exercise ID: fable\n",
            "Current Dataset Length:  8837\n",
            "Subject 17/39 | Session ID: 183 | Exercise ID: fable\n",
            "Current Dataset Length:  8896\n",
            "Subject 17/39 | Session ID: 554 | Exercise ID: fable\n",
            "Current Dataset Length:  8955\n",
            "Subject 17/39 | Session ID: 772 | Exercise ID: fable\n",
            "Current Dataset Length:  9015\n",
            "Subject 17/39 | Session ID: 821 | Exercise ID: fable\n",
            "Current Dataset Length:  9074\n",
            "Subject 17/39 | Session ID: 875 | Exercise ID: fable\n",
            "Current Dataset Length:  9132\n",
            "Subject 17/39 | Session ID: 1206 | Exercise ID: fable\n",
            "Current Dataset Length:  9192\n",
            "Subject 17/39 | Session ID: 1302 | Exercise ID: fable\n",
            "Current Dataset Length:  9251\n",
            "Subject 17/39 | Session ID: 1641 | Exercise ID: fable\n",
            "Current Dataset Length:  9310\n",
            "Subject 17/39 | Session ID: 1650 | Exercise ID: fable\n",
            "Current Dataset Length:  9370\n",
            "Subject 18/39 | Session ID: 367 | Exercise ID: fable\n",
            "Current Dataset Length:  9430\n",
            "Subject 18/39 | Session ID: 628 | Exercise ID: fable\n",
            "Current Dataset Length:  9486\n",
            "Subject 18/39 | Session ID: 656 | Exercise ID: fable\n",
            "Current Dataset Length:  9550\n",
            "Subject 18/39 | Session ID: 733 | Exercise ID: fable\n",
            "Current Dataset Length:  9609\n",
            "Subject 18/39 | Session ID: 807 | Exercise ID: fable\n",
            "Current Dataset Length:  9669\n",
            "Subject 18/39 | Session ID: 869 | Exercise ID: fable\n",
            "Current Dataset Length:  9729\n",
            "Subject 18/39 | Session ID: 1031 | Exercise ID: fable\n",
            "Current Dataset Length:  9791\n",
            "Subject 18/39 | Session ID: 1157 | Exercise ID: fable\n",
            "Current Dataset Length:  9847\n",
            "Subject 18/39 | Session ID: 1271 | Exercise ID: fable\n",
            "Current Dataset Length:  9907\n",
            "Subject 18/39 | Session ID: 1465 | Exercise ID: fable\n",
            "Current Dataset Length:  9963\n",
            "Subject 19/39 | Session ID: 55 | Exercise ID: fable\n",
            "Current Dataset Length:  10024\n",
            "Subject 19/39 | Session ID: 170 | Exercise ID: fable\n",
            "Current Dataset Length:  10087\n",
            "Subject 19/39 | Session ID: 409 | Exercise ID: fable\n",
            "Current Dataset Length:  10152\n",
            "Subject 19/39 | Session ID: 604 | Exercise ID: fable\n",
            "Current Dataset Length:  10215\n",
            "Subject 19/39 | Session ID: 884 | Exercise ID: fable\n",
            "Current Dataset Length:  10273\n",
            "Subject 19/39 | Session ID: 907 | Exercise ID: fable\n",
            "Current Dataset Length:  10337\n",
            "Subject 19/39 | Session ID: 918 | Exercise ID: fable\n",
            "Current Dataset Length:  10398\n",
            "Subject 19/39 | Session ID: 971 | Exercise ID: fable\n",
            "Current Dataset Length:  10461\n",
            "Subject 19/39 | Session ID: 1154 | Exercise ID: fable\n",
            "Current Dataset Length:  10522\n",
            "Subject 20/39 | Session ID: 286 | Exercise ID: fable\n",
            "Current Dataset Length:  10585\n",
            "Subject 20/39 | Session ID: 347 | Exercise ID: fable\n",
            "Current Dataset Length:  10645\n",
            "Subject 20/39 | Session ID: 406 | Exercise ID: fable\n",
            "Current Dataset Length:  10705\n",
            "Subject 20/39 | Session ID: 802 | Exercise ID: fable\n",
            "Current Dataset Length:  10769\n",
            "Subject 20/39 | Session ID: 838 | Exercise ID: fable\n",
            "Current Dataset Length:  10830\n",
            "Subject 20/39 | Session ID: 1016 | Exercise ID: fable\n",
            "Current Dataset Length:  10888\n",
            "Subject 20/39 | Session ID: 1441 | Exercise ID: fable\n",
            "Current Dataset Length:  10951\n",
            "Subject 20/39 | Session ID: 1666 | Exercise ID: fable\n",
            "Current Dataset Length:  11009\n",
            "Subject 21/39 | Session ID: 197 | Exercise ID: fable\n",
            "Current Dataset Length:  11070\n",
            "Subject 21/39 | Session ID: 532 | Exercise ID: fable\n",
            "Current Dataset Length:  11129\n",
            "Subject 21/39 | Session ID: 620 | Exercise ID: fable\n",
            "Current Dataset Length:  11191\n",
            "Subject 21/39 | Session ID: 991 | Exercise ID: fable\n",
            "Current Dataset Length:  11250\n",
            "Subject 21/39 | Session ID: 1089 | Exercise ID: fable\n",
            "Current Dataset Length:  11315\n",
            "Subject 21/39 | Session ID: 1121 | Exercise ID: fable\n",
            "Current Dataset Length:  11377\n",
            "Subject 21/39 | Session ID: 1226 | Exercise ID: fable\n",
            "Current Dataset Length:  11442\n",
            "Subject 21/39 | Session ID: 1301 | Exercise ID: fable\n",
            "Current Dataset Length:  11504\n",
            "Subject 21/39 | Session ID: 1589 | Exercise ID: fable\n",
            "Current Dataset Length:  11565\n",
            "Subject 21/39 | Session ID: 1608 | Exercise ID: fable\n",
            "Current Dataset Length:  11628\n",
            "Subject 22/39 | Session ID: 91 | Exercise ID: fable\n",
            "Current Dataset Length:  11690\n",
            "Subject 22/39 | Session ID: 817 | Exercise ID: fable\n",
            "Current Dataset Length:  11757\n",
            "Subject 22/39 | Session ID: 872 | Exercise ID: fable\n",
            "Current Dataset Length:  11819\n",
            "Subject 22/39 | Session ID: 903 | Exercise ID: fable\n",
            "Current Dataset Length:  11878\n",
            "Subject 22/39 | Session ID: 1032 | Exercise ID: fable\n",
            "Current Dataset Length:  11937\n",
            "Subject 22/39 | Session ID: 1420 | Exercise ID: fable\n",
            "Current Dataset Length:  11998\n",
            "Subject 22/39 | Session ID: 1433 | Exercise ID: fable\n",
            "Current Dataset Length:  12059\n",
            "Subject 22/39 | Session ID: 1472 | Exercise ID: fable\n",
            "Current Dataset Length:  12118\n",
            "Subject 22/39 | Session ID: 1573 | Exercise ID: fable\n",
            "Current Dataset Length:  12180\n",
            "Subject 22/39 | Session ID: 1673 | Exercise ID: fable\n",
            "Current Dataset Length:  12239\n",
            "Subject 23/39 | Session ID: 584 | Exercise ID: fable\n",
            "Current Dataset Length:  12299\n",
            "Subject 23/39 | Session ID: 808 | Exercise ID: fable\n",
            "Current Dataset Length:  12358\n",
            "Subject 23/39 | Session ID: 931 | Exercise ID: fable\n",
            "Current Dataset Length:  12420\n",
            "Subject 23/39 | Session ID: 984 | Exercise ID: fable\n",
            "Current Dataset Length:  12481\n",
            "Subject 23/39 | Session ID: 1304 | Exercise ID: fable\n",
            "Current Dataset Length:  12542\n",
            "Subject 23/39 | Session ID: 1355 | Exercise ID: fable\n",
            "Current Dataset Length:  12602\n",
            "Subject 23/39 | Session ID: 1460 | Exercise ID: fable\n",
            "Current Dataset Length:  12666\n",
            "Subject 23/39 | Session ID: 1675 | Exercise ID: fable\n",
            "Current Dataset Length:  12725\n",
            "Subject 23/39 | Session ID: 1689 | Exercise ID: fable\n",
            "Current Dataset Length:  12785\n",
            "Subject 24/39 | Session ID: 126 | Exercise ID: fable\n",
            "Current Dataset Length:  12844\n",
            "Subject 24/39 | Session ID: 403 | Exercise ID: fable\n",
            "Current Dataset Length:  12905\n",
            "Subject 24/39 | Session ID: 522 | Exercise ID: fable\n",
            "Current Dataset Length:  12969\n",
            "Subject 24/39 | Session ID: 562 | Exercise ID: fable\n",
            "Current Dataset Length:  13028\n",
            "Subject 24/39 | Session ID: 602 | Exercise ID: fable\n",
            "Current Dataset Length:  13087\n",
            "Subject 24/39 | Session ID: 854 | Exercise ID: fable\n",
            "Current Dataset Length:  13146\n",
            "Subject 24/39 | Session ID: 1150 | Exercise ID: fable\n",
            "Current Dataset Length:  13206\n",
            "Subject 24/39 | Session ID: 1285 | Exercise ID: fable\n",
            "Current Dataset Length:  13268\n",
            "Subject 24/39 | Session ID: 1427 | Exercise ID: fable\n",
            "Current Dataset Length:  13327\n",
            "Subject 24/39 | Session ID: 1556 | Exercise ID: fable\n",
            "Current Dataset Length:  13387\n",
            "Subject 25/39 | Session ID: 120 | Exercise ID: fable\n",
            "Current Dataset Length:  13449\n",
            "Subject 25/39 | Session ID: 139 | Exercise ID: fable\n",
            "Current Dataset Length:  13507\n",
            "Subject 25/39 | Session ID: 222 | Exercise ID: fable\n",
            "Current Dataset Length:  13568\n",
            "Subject 25/39 | Session ID: 697 | Exercise ID: fable\n",
            "Current Dataset Length:  13630\n",
            "Subject 25/39 | Session ID: 1087 | Exercise ID: fable\n",
            "Current Dataset Length:  13690\n",
            "Subject 25/39 | Session ID: 1354 | Exercise ID: fable\n",
            "Current Dataset Length:  13747\n",
            "Subject 25/39 | Session ID: 1386 | Exercise ID: fable\n",
            "Current Dataset Length:  13806\n",
            "Subject 25/39 | Session ID: 1426 | Exercise ID: fable\n",
            "Current Dataset Length:  13867\n",
            "Subject 26/39 | Session ID: 88 | Exercise ID: fable\n",
            "Current Dataset Length:  13925\n",
            "Subject 26/39 | Session ID: 159 | Exercise ID: fable\n",
            "Current Dataset Length:  13986\n",
            "Subject 26/39 | Session ID: 244 | Exercise ID: fable\n",
            "Current Dataset Length:  14052\n",
            "Subject 26/39 | Session ID: 275 | Exercise ID: fable\n",
            "Current Dataset Length:  14112\n",
            "Subject 26/39 | Session ID: 663 | Exercise ID: fable\n",
            "Current Dataset Length:  14172\n",
            "Subject 26/39 | Session ID: 730 | Exercise ID: fable\n",
            "Current Dataset Length:  14233\n",
            "Subject 26/39 | Session ID: 751 | Exercise ID: fable\n",
            "Current Dataset Length:  14292\n",
            "Subject 26/39 | Session ID: 1464 | Exercise ID: fable\n",
            "Current Dataset Length:  14354\n",
            "Subject 26/39 | Session ID: 1542 | Exercise ID: fable\n",
            "Current Dataset Length:  14414\n",
            "Subject 26/39 | Session ID: 1635 | Exercise ID: fable\n",
            "Current Dataset Length:  14478\n",
            "Subject 27/39 | Session ID: 389 | Exercise ID: fable\n",
            "Current Dataset Length:  14544\n",
            "Subject 27/39 | Session ID: 488 | Exercise ID: fable\n",
            "Current Dataset Length:  14601\n",
            "Subject 27/39 | Session ID: 617 | Exercise ID: fable\n",
            "Current Dataset Length:  14656\n",
            "Subject 27/39 | Session ID: 637 | Exercise ID: fable\n",
            "Current Dataset Length:  14710\n",
            "Subject 27/39 | Session ID: 934 | Exercise ID: fable\n",
            "Current Dataset Length:  14766\n",
            "Subject 27/39 | Session ID: 1155 | Exercise ID: fable\n",
            "Current Dataset Length:  14820\n",
            "Subject 27/39 | Session ID: 1341 | Exercise ID: fable\n",
            "Current Dataset Length:  14892\n",
            "Subject 27/39 | Session ID: 1623 | Exercise ID: fable\n",
            "Current Dataset Length:  14948\n",
            "Subject 27/39 | Session ID: 1674 | Exercise ID: fable\n",
            "Current Dataset Length:  15002\n",
            "Subject 28/39 | Session ID: 131 | Exercise ID: fable\n",
            "Current Dataset Length:  15055\n",
            "Subject 28/39 | Session ID: 888 | Exercise ID: fable\n",
            "Current Dataset Length:  15116\n",
            "Subject 28/39 | Session ID: 959 | Exercise ID: fable\n",
            "Current Dataset Length:  15176\n",
            "Subject 28/39 | Session ID: 989 | Exercise ID: fable\n",
            "Current Dataset Length:  15237\n",
            "Subject 28/39 | Session ID: 1129 | Exercise ID: fable\n",
            "Current Dataset Length:  15298\n",
            "Subject 28/39 | Session ID: 1338 | Exercise ID: fable\n",
            "Current Dataset Length:  15358\n",
            "Subject 28/39 | Session ID: 1421 | Exercise ID: fable\n",
            "Current Dataset Length:  15417\n",
            "Subject 28/39 | Session ID: 1437 | Exercise ID: fable\n",
            "Current Dataset Length:  15480\n",
            "Subject 28/39 | Session ID: 1559 | Exercise ID: fable\n",
            "Current Dataset Length:  15537\n",
            "Subject 28/39 | Session ID: 1710 | Exercise ID: fable\n",
            "Current Dataset Length:  15599\n",
            "Subject 29/39 | Session ID: 142 | Exercise ID: fable\n",
            "Current Dataset Length:  15657\n",
            "Subject 29/39 | Session ID: 192 | Exercise ID: fable\n",
            "Current Dataset Length:  15732\n",
            "Subject 29/39 | Session ID: 228 | Exercise ID: fable\n",
            "Current Dataset Length:  15799\n",
            "Subject 29/39 | Session ID: 437 | Exercise ID: fable\n",
            "Current Dataset Length:  15862\n",
            "Subject 29/39 | Session ID: 492 | Exercise ID: fable\n",
            "Current Dataset Length:  15920\n",
            "Subject 29/39 | Session ID: 735 | Exercise ID: fable\n",
            "Current Dataset Length:  15988\n",
            "Subject 29/39 | Session ID: 782 | Exercise ID: fable\n",
            "Current Dataset Length:  16049\n",
            "Subject 29/39 | Session ID: 930 | Exercise ID: fable\n",
            "Current Dataset Length:  16112\n",
            "Subject 29/39 | Session ID: 1059 | Exercise ID: fable\n",
            "Current Dataset Length:  16175\n",
            "Subject 29/39 | Session ID: 1123 | Exercise ID: fable\n",
            "Current Dataset Length:  16236\n",
            "Subject 29/39 | Session ID: 1267 | Exercise ID: fable\n",
            "Current Dataset Length:  16295\n",
            "Subject 29/39 | Session ID: 1320 | Exercise ID: fable\n",
            "Current Dataset Length:  16352\n",
            "Subject 29/39 | Session ID: 1539 | Exercise ID: fable\n",
            "Current Dataset Length:  16427\n",
            "Subject 29/39 | Session ID: 1577 | Exercise ID: fable\n",
            "Current Dataset Length:  16502\n",
            "Subject 29/39 | Session ID: 1656 | Exercise ID: fable\n",
            "Current Dataset Length:  16562\n",
            "Subject 30/39 | Session ID: 377 | Exercise ID: fable\n",
            "Current Dataset Length:  16623\n",
            "Subject 30/39 | Session ID: 1008 | Exercise ID: fable\n",
            "Current Dataset Length:  16712\n",
            "Subject 31/39 | Session ID: 168 | Exercise ID: fable\n",
            "Current Dataset Length:  16774\n",
            "Subject 31/39 | Session ID: 215 | Exercise ID: fable\n",
            "Current Dataset Length:  16834\n",
            "Subject 31/39 | Session ID: 351 | Exercise ID: fable\n",
            "Current Dataset Length:  16898\n",
            "Subject 31/39 | Session ID: 927 | Exercise ID: fable\n",
            "Current Dataset Length:  16960\n",
            "Subject 31/39 | Session ID: 1138 | Exercise ID: fable\n",
            "Current Dataset Length:  17021\n",
            "Subject 31/39 | Session ID: 1159 | Exercise ID: fable\n",
            "Current Dataset Length:  17085\n",
            "Subject 31/39 | Session ID: 1276 | Exercise ID: fable\n",
            "Current Dataset Length:  17146\n",
            "Subject 31/39 | Session ID: 1351 | Exercise ID: fable\n",
            "Current Dataset Length:  17205\n",
            "Subject 31/39 | Session ID: 1500 | Exercise ID: fable\n",
            "Current Dataset Length:  17268\n",
            "Subject 31/39 | Session ID: 1725 | Exercise ID: fable\n",
            "Current Dataset Length:  17330\n",
            "Subject 32/39 | Session ID: 332 | Exercise ID: fable\n",
            "Current Dataset Length:  17386\n",
            "Subject 32/39 | Session ID: 439 | Exercise ID: fable\n",
            "Current Dataset Length:  17446\n",
            "Subject 32/39 | Session ID: 798 | Exercise ID: fable\n",
            "Current Dataset Length:  17506\n",
            "Subject 32/39 | Session ID: 856 | Exercise ID: fable\n",
            "Current Dataset Length:  17565\n",
            "Subject 32/39 | Session ID: 1227 | Exercise ID: fable\n",
            "Current Dataset Length:  17624\n",
            "Subject 32/39 | Session ID: 1242 | Exercise ID: fable\n",
            "Current Dataset Length:  17683\n",
            "Subject 32/39 | Session ID: 1290 | Exercise ID: fable\n",
            "Current Dataset Length:  17742\n",
            "Subject 32/39 | Session ID: 1393 | Exercise ID: fable\n",
            "Current Dataset Length:  17802\n",
            "Subject 32/39 | Session ID: 1551 | Exercise ID: fable\n",
            "Current Dataset Length:  17861\n",
            "Subject 33/39 | Session ID: 199 | Exercise ID: fable\n",
            "Current Dataset Length:  17925\n",
            "Subject 33/39 | Session ID: 310 | Exercise ID: fable\n",
            "Current Dataset Length:  17987\n",
            "Subject 33/39 | Session ID: 458 | Exercise ID: fable\n",
            "Current Dataset Length:  18045\n",
            "Subject 33/39 | Session ID: 566 | Exercise ID: fable\n",
            "Current Dataset Length:  18105\n",
            "Subject 33/39 | Session ID: 675 | Exercise ID: fable\n",
            "Current Dataset Length:  18165\n",
            "Subject 33/39 | Session ID: 712 | Exercise ID: fable\n",
            "Current Dataset Length:  18228\n",
            "Subject 33/39 | Session ID: 1467 | Exercise ID: fable\n",
            "Current Dataset Length:  18291\n",
            "Subject 33/39 | Session ID: 1492 | Exercise ID: fable\n",
            "Current Dataset Length:  18353\n",
            "Subject 33/39 | Session ID: 1588 | Exercise ID: fable\n",
            "Current Dataset Length:  18413\n",
            "Subject 34/39 | Session ID: 93 | Exercise ID: fable\n",
            "Current Dataset Length:  18474\n",
            "Subject 34/39 | Session ID: 147 | Exercise ID: fable\n",
            "Current Dataset Length:  18536\n",
            "Subject 34/39 | Session ID: 176 | Exercise ID: fable\n",
            "Current Dataset Length:  18585\n",
            "Subject 34/39 | Session ID: 254 | Exercise ID: fable\n",
            "Current Dataset Length:  18644\n",
            "Subject 34/39 | Session ID: 739 | Exercise ID: fable\n",
            "Current Dataset Length:  18693\n",
            "Subject 34/39 | Session ID: 744 | Exercise ID: fable\n",
            "Current Dataset Length:  18738\n",
            "Subject 34/39 | Session ID: 1070 | Exercise ID: fable\n",
            "Current Dataset Length:  18798\n",
            "Subject 34/39 | Session ID: 1347 | Exercise ID: fable\n",
            "Current Dataset Length:  18860\n",
            "Subject 34/39 | Session ID: 1552 | Exercise ID: fable\n",
            "Current Dataset Length:  18918\n",
            "Subject 35/39 | Session ID: 28 | Exercise ID: fable\n",
            "Current Dataset Length:  18976\n",
            "Subject 35/39 | Session ID: 247 | Exercise ID: fable\n",
            "Current Dataset Length:  19035\n",
            "Subject 35/39 | Session ID: 302 | Exercise ID: fable\n",
            "Current Dataset Length:  19097\n",
            "Subject 35/39 | Session ID: 395 | Exercise ID: fable\n",
            "Current Dataset Length:  19156\n",
            "Subject 35/39 | Session ID: 980 | Exercise ID: fable\n",
            "Current Dataset Length:  19222\n",
            "Subject 35/39 | Session ID: 1136 | Exercise ID: fable\n",
            "Current Dataset Length:  19280\n",
            "Subject 35/39 | Session ID: 1183 | Exercise ID: fable\n",
            "Current Dataset Length:  19340\n",
            "Subject 35/39 | Session ID: 1250 | Exercise ID: fable\n",
            "Current Dataset Length:  19402\n",
            "Subject 35/39 | Session ID: 1270 | Exercise ID: fable\n",
            "Current Dataset Length:  19463\n",
            "Subject 35/39 | Session ID: 1333 | Exercise ID: fable\n",
            "Current Dataset Length:  19519\n",
            "Subject 36/39 | Session ID: 457 | Exercise ID: fable\n",
            "Current Dataset Length:  19581\n",
            "Subject 36/39 | Session ID: 672 | Exercise ID: fable\n",
            "Current Dataset Length:  19646\n",
            "Subject 36/39 | Session ID: 1236 | Exercise ID: fable\n",
            "Current Dataset Length:  19708\n",
            "Subject 36/39 | Session ID: 1292 | Exercise ID: fable\n",
            "Current Dataset Length:  19768\n",
            "Subject 36/39 | Session ID: 1527 | Exercise ID: fable\n",
            "Current Dataset Length:  19831\n",
            "Subject 36/39 | Session ID: 1592 | Exercise ID: fable\n",
            "Current Dataset Length:  19895\n",
            "Subject 36/39 | Session ID: 1681 | Exercise ID: fable\n",
            "Current Dataset Length:  19961\n",
            "Subject 37/39 | Session ID: 102 | Exercise ID: fable\n",
            "Current Dataset Length:  20023\n",
            "Subject 37/39 | Session ID: 530 | Exercise ID: fable\n",
            "Current Dataset Length:  20089\n",
            "Subject 37/39 | Session ID: 585 | Exercise ID: fable\n",
            "Current Dataset Length:  20156\n",
            "Subject 37/39 | Session ID: 1166 | Exercise ID: fable\n",
            "Current Dataset Length:  20246\n",
            "Subject 37/39 | Session ID: 1331 | Exercise ID: fable\n",
            "Current Dataset Length:  20315\n",
            "Subject 38/39 | Session ID: 174 | Exercise ID: fable\n",
            "Current Dataset Length:  20391\n",
            "Subject 38/39 | Session ID: 204 | Exercise ID: fable\n",
            "Current Dataset Length:  20450\n",
            "Subject 38/39 | Session ID: 499 | Exercise ID: fable\n",
            "Current Dataset Length:  20509\n",
            "Subject 38/39 | Session ID: 600 | Exercise ID: fable\n",
            "Current Dataset Length:  20568\n",
            "Subject 38/39 | Session ID: 794 | Exercise ID: fable\n",
            "Current Dataset Length:  20627\n",
            "Subject 38/39 | Session ID: 1158 | Exercise ID: fable\n",
            "Current Dataset Length:  20686\n",
            "Subject 38/39 | Session ID: 1300 | Exercise ID: fable\n",
            "Current Dataset Length:  20745\n",
            "Subject 38/39 | Session ID: 1506 | Exercise ID: fable\n",
            "Current Dataset Length:  20805\n",
            "Subject 38/39 | Session ID: 1570 | Exercise ID: fable\n",
            "Current Dataset Length:  20864\n",
            "Subject 39/39 | Session ID: 282 | Exercise ID: fable\n",
            "Current Dataset Length:  20925\n",
            "Subject 39/39 | Session ID: 577 | Exercise ID: fable\n",
            "Current Dataset Length:  20985\n",
            "Subject 39/39 | Session ID: 578 | Exercise ID: fable\n",
            "Current Dataset Length:  21045\n",
            "Subject 39/39 | Session ID: 622 | Exercise ID: fable\n",
            "Current Dataset Length:  21110\n",
            "Subject 39/39 | Session ID: 850 | Exercise ID: fable\n",
            "Current Dataset Length:  21172\n",
            "Subject 39/39 | Session ID: 862 | Exercise ID: fable\n",
            "Current Dataset Length:  21232\n",
            "Subject 39/39 | Session ID: 874 | Exercise ID: fable\n",
            "Current Dataset Length:  21293\n",
            "Subject 39/39 | Session ID: 992 | Exercise ID: fable\n",
            "Current Dataset Length:  21352\n",
            "Subject 39/39 | Session ID: 1195 | Exercise ID: fable\n",
            "Current Dataset Length:  21426\n",
            "Subject 39/39 | Session ID: 1213 | Exercise ID: fable\n",
            "Current Dataset Length:  21486\n",
            "Subject 39/39 | Session ID: 1343 | Exercise ID: fable\n",
            "Current Dataset Length:  21545\n",
            "Subject 39/39 | Session ID: 1408 | Exercise ID: fable\n",
            "Current Dataset Length:  21604\n",
            "Subject 39/39 | Session ID: 1450 | Exercise ID: fable\n",
            "Current Dataset Length:  21692\n",
            "Subject 39/39 | Session ID: 1483 | Exercise ID: fable\n",
            "Current Dataset Length:  21762\n",
            "Subject 39/39 | Session ID: 1553 | Exercise ID: fable\n",
            "Current Dataset Length:  21825\n",
            "Subject 39/39 | Session ID: 1565 | Exercise ID: fable\n",
            "Current Dataset Length:  21896\n",
            "Subject 39/39 | Session ID: 1670 | Exercise ID: fable\n",
            "Current Dataset Length:  21961\n"
          ]
        }
      ],
      "source": [
        "df = read_villani_dataset()\n",
        "df.loc[df['keydelay'] >= 1000, 'keydelay'] = 1000\n",
        "\n",
        "# wl = read_rockyou_wordlist(n_data=100000)\n",
        "\n",
        "# freetext-mode only\n",
        "# df = df[df[\"inputtype\"] == \"free\"]\n",
        "\n",
        "# fixed-text only\n",
        "df = df[df[\"inputtype\"] == \"fixed\"]\n",
        "\n",
        "# all-mode\n",
        "# df = df[(df[\"inputtype\"] == \"fixed\") & (df[\"inputtype\"] == \"free\")]\n",
        "\n",
        "clean_dataset_villani(df)\n",
        "subjects = df.groupby('user')\n",
        "datasets = []\n",
        "word_dictionary = []\n",
        "\n",
        "for i, (subject_id, subject) in enumerate(subjects):\n",
        "    sessions = subject.groupby(\"session\")\n",
        "    for sess_id, session in sessions:\n",
        "        exercises = session.groupby(\"task\")\n",
        "        for exercise_id, exercise in exercises:\n",
        "            print(f\"Subject {i + 1}/{len(subjects)} | Session ID: {sess_id} | Exercise ID: {exercise_id}\")\n",
        "            print(\"Current Dataset Length: \", len(datasets))\n",
        "            for exerchunk in split_words_villani(exercise):\n",
        "                word_chunk = \"\".join(exerchunk['key2'].tolist())\n",
        "                \n",
        "                # Mode 1: Get all word combinations\n",
        "                # print(word_chunk)\n",
        "                if len(word_chunk) < 4:\n",
        "                    continue\n",
        "\n",
        "                datasets.append(exerchunk)\n",
        "                word_dictionary.append(word_chunk)\n",
        "\n",
        "                # Mode 2: Isolate to RockYou wordlist only\n",
        "                # if word_chunk in wl.values:\n",
        "                #     word_dictionary.append(word_chunk)\n",
        "                #     datasets.append(exerchunk)\n",
        "                    # for gram in dfword_to_gram(exerchunk, 3):\n",
        "                    #     # print(gram.head())\n",
        "                    #     datasets.append(gram)\n",
        "\n",
        "    # !-- Subject-specific Inference Attack\n",
        "    # train_datasets, test_datasets, keytext_tokens, keydelay_tokens = split_dataset(df, datasets)\n",
        "    # save_processed_dataset(datasets)\n",
        "    # datasets = []\n",
        "    # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q3Y_oNFbOHq",
        "outputId": "4cdf51c8-70c7-4406-ce85-c1101e3952d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that        1242\n",
            "weasel       921\n",
            "fell         625\n",
            "abat         609\n",
            "amouse       591\n",
            "            ... \n",
            "abrd           1\n",
            "upton          1\n",
            "dstility       1\n",
            "hods           1\n",
            "revealed       1\n",
            "Length: 2683, dtype: int64\n",
            "that             1242\n",
            "weasel            921\n",
            "fell              625\n",
            "abat              609\n",
            "amouse            591\n",
            "ground            579\n",
            "caught            571\n",
            "assured           568\n",
            "thus              566\n",
            "time              322\n",
            "wise              313\n",
            "this              307\n",
            "said              305\n",
            "life              304\n",
            "birds             299\n",
            "spared            299\n",
            "upon              298\n",
            "nature            298\n",
            "turn              298\n",
            "free              295\n",
            "about             295\n",
            "again             293\n",
            "mice              293\n",
            "good              291\n",
            "fable             291\n",
            "saying            288\n",
            "entreated         283\n",
            "abird             282\n",
            "story             282\n",
            "account           281\n",
            "enemy             280\n",
            "asecond           278\n",
            "another           278\n",
            "moral             275\n",
            "shortly           275\n",
            "afterwards        271\n",
            "hostility         271\n",
            "refused           270\n",
            "whom              266\n",
            "pleaded           265\n",
            "aweasel           261\n",
            "likewise          259\n",
            "aesop             258\n",
            "weasels           253\n",
            "aspecial          246\n",
            "escaped           239\n",
            "circumstances     224\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# !-- Whole-dataset Inference Attack\n",
        "make_balanced_dataset(datasets, word_dictionary, min_sample=50)\n",
        "save_processed_dataset(datasets)\n",
        "train_datasets, test_datasets, keytext_tokens, keydelay_tokens = split_dataset(df, datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7hjHXdm7NyoU",
        "outputId": "d9ca02c5-1258-42a7-c1bf-15667cff43c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 13566\n",
            "Number of unique input tokens: 1001\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 13\n",
            "Max sequence length for outputs: 15\n",
            "(13566, 13, 1001)\n",
            "(13566, 15, 66)\n",
            "(13566, 15, 66)\n",
            "Epoch 1/100\n",
            "340/340 [==============================] - 6s 10ms/step - loss: 1.1135 - accuracy: 0.1363 - val_loss: 1.0508 - val_accuracy: 0.1598\n",
            "Epoch 2/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.9200 - accuracy: 0.1993 - val_loss: 0.8298 - val_accuracy: 0.2381\n",
            "Epoch 3/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.7674 - accuracy: 0.2505 - val_loss: 0.6408 - val_accuracy: 0.3100\n",
            "Epoch 4/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.6562 - accuracy: 0.2879 - val_loss: 0.7545 - val_accuracy: 0.2410\n",
            "Epoch 5/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.5887 - accuracy: 0.3133 - val_loss: 0.4922 - val_accuracy: 0.3472\n",
            "Epoch 6/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.5398 - accuracy: 0.3297 - val_loss: 0.5032 - val_accuracy: 0.3432\n",
            "Epoch 7/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.4961 - accuracy: 0.3388 - val_loss: 0.4105 - val_accuracy: 0.3656\n",
            "Epoch 8/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.4401 - accuracy: 0.3460 - val_loss: 0.3525 - val_accuracy: 0.3676\n",
            "Epoch 9/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.3915 - accuracy: 0.3525 - val_loss: 0.3191 - val_accuracy: 0.3753\n",
            "Epoch 10/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.3670 - accuracy: 0.3547 - val_loss: 0.3038 - val_accuracy: 0.3773\n",
            "Epoch 11/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.3470 - accuracy: 0.3580 - val_loss: 0.2878 - val_accuracy: 0.3799\n",
            "Epoch 12/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.3269 - accuracy: 0.3598 - val_loss: 0.2761 - val_accuracy: 0.3774\n",
            "Epoch 13/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.3146 - accuracy: 0.3607 - val_loss: 0.2615 - val_accuracy: 0.3799\n",
            "Epoch 14/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.3072 - accuracy: 0.3621 - val_loss: 0.2979 - val_accuracy: 0.3714\n",
            "Epoch 15/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2960 - accuracy: 0.3642 - val_loss: 0.2505 - val_accuracy: 0.3816\n",
            "Epoch 16/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2880 - accuracy: 0.3644 - val_loss: 0.2382 - val_accuracy: 0.3817\n",
            "Epoch 17/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2790 - accuracy: 0.3649 - val_loss: 0.2442 - val_accuracy: 0.3769\n",
            "Epoch 18/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2705 - accuracy: 0.3661 - val_loss: 0.2293 - val_accuracy: 0.3809\n",
            "Epoch 19/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2636 - accuracy: 0.3671 - val_loss: 0.2188 - val_accuracy: 0.3832\n",
            "Epoch 20/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2564 - accuracy: 0.3681 - val_loss: 0.2153 - val_accuracy: 0.3831\n",
            "Epoch 21/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2550 - accuracy: 0.3670 - val_loss: 0.2163 - val_accuracy: 0.3800\n",
            "Epoch 22/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2471 - accuracy: 0.3684 - val_loss: 0.2076 - val_accuracy: 0.3827\n",
            "Epoch 23/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2415 - accuracy: 0.3684 - val_loss: 0.2062 - val_accuracy: 0.3823\n",
            "Epoch 24/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2364 - accuracy: 0.3697 - val_loss: 0.2142 - val_accuracy: 0.3775\n",
            "Epoch 25/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2321 - accuracy: 0.3699 - val_loss: 0.1962 - val_accuracy: 0.3833\n",
            "Epoch 26/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2255 - accuracy: 0.3714 - val_loss: 0.1962 - val_accuracy: 0.3825\n",
            "Epoch 27/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2241 - accuracy: 0.3714 - val_loss: 0.1913 - val_accuracy: 0.3847\n",
            "Epoch 28/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2183 - accuracy: 0.3725 - val_loss: 0.1864 - val_accuracy: 0.3856\n",
            "Epoch 29/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2163 - accuracy: 0.3729 - val_loss: 0.1837 - val_accuracy: 0.3842\n",
            "Epoch 30/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2135 - accuracy: 0.3741 - val_loss: 0.1848 - val_accuracy: 0.3854\n",
            "Epoch 31/100\n",
            "340/340 [==============================] - 3s 9ms/step - loss: 0.2080 - accuracy: 0.3752 - val_loss: 0.1792 - val_accuracy: 0.3856\n",
            "Epoch 32/100\n",
            "340/340 [==============================] - 3s 9ms/step - loss: 0.2077 - accuracy: 0.3754 - val_loss: 0.1854 - val_accuracy: 0.3835\n",
            "Epoch 33/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2047 - accuracy: 0.3759 - val_loss: 0.1900 - val_accuracy: 0.3813\n",
            "Epoch 34/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.2012 - accuracy: 0.3766 - val_loss: 0.1766 - val_accuracy: 0.3861\n",
            "Epoch 35/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1985 - accuracy: 0.3778 - val_loss: 0.1783 - val_accuracy: 0.3856\n",
            "Epoch 36/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1960 - accuracy: 0.3782 - val_loss: 0.1816 - val_accuracy: 0.3844\n",
            "Epoch 37/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1951 - accuracy: 0.3786 - val_loss: 0.1833 - val_accuracy: 0.3826\n",
            "Epoch 38/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1919 - accuracy: 0.3792 - val_loss: 0.1715 - val_accuracy: 0.3883\n",
            "Epoch 39/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1905 - accuracy: 0.3787 - val_loss: 0.1735 - val_accuracy: 0.3853\n",
            "Epoch 40/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1887 - accuracy: 0.3800 - val_loss: 0.1728 - val_accuracy: 0.3870\n",
            "Epoch 41/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1875 - accuracy: 0.3803 - val_loss: 0.1771 - val_accuracy: 0.3841\n",
            "Epoch 42/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1846 - accuracy: 0.3810 - val_loss: 0.1664 - val_accuracy: 0.3876\n",
            "Epoch 43/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1833 - accuracy: 0.3812 - val_loss: 0.1647 - val_accuracy: 0.3879\n",
            "Epoch 44/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1809 - accuracy: 0.3818 - val_loss: 0.1623 - val_accuracy: 0.3887\n",
            "Epoch 45/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1780 - accuracy: 0.3826 - val_loss: 0.1695 - val_accuracy: 0.3861\n",
            "Epoch 46/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1773 - accuracy: 0.3828 - val_loss: 0.1606 - val_accuracy: 0.3890\n",
            "Epoch 47/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1757 - accuracy: 0.3834 - val_loss: 0.1693 - val_accuracy: 0.3865\n",
            "Epoch 48/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1760 - accuracy: 0.3827 - val_loss: 0.1651 - val_accuracy: 0.3868\n",
            "Epoch 49/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1742 - accuracy: 0.3835 - val_loss: 0.1640 - val_accuracy: 0.3873\n",
            "Epoch 50/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1715 - accuracy: 0.3846 - val_loss: 0.1679 - val_accuracy: 0.3877\n",
            "Epoch 51/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1692 - accuracy: 0.3860 - val_loss: 0.1604 - val_accuracy: 0.3895\n",
            "Epoch 52/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1667 - accuracy: 0.3864 - val_loss: 0.1618 - val_accuracy: 0.3896\n",
            "Epoch 53/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1673 - accuracy: 0.3860 - val_loss: 0.1672 - val_accuracy: 0.3875\n",
            "Epoch 54/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1639 - accuracy: 0.3874 - val_loss: 0.1582 - val_accuracy: 0.3900\n",
            "Epoch 55/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1623 - accuracy: 0.3881 - val_loss: 0.1655 - val_accuracy: 0.3872\n",
            "Epoch 56/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1609 - accuracy: 0.3887 - val_loss: 0.1609 - val_accuracy: 0.3889\n",
            "Epoch 57/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1584 - accuracy: 0.3892 - val_loss: 0.1658 - val_accuracy: 0.3878\n",
            "Epoch 58/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1579 - accuracy: 0.3894 - val_loss: 0.1621 - val_accuracy: 0.3888\n",
            "Epoch 59/100\n",
            "340/340 [==============================] - 3s 8ms/step - loss: 0.1550 - accuracy: 0.3903 - val_loss: 0.1653 - val_accuracy: 0.3878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_28_layer_call_fn, lstm_cell_28_layer_call_and_return_conditional_losses, lstm_cell_29_layer_call_fn, lstm_cell_29_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/S2/camstroke-inference-models/s2s_model_top100000rockyou_fixedtype_cleanwords_2/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/S2/camstroke-inference-models/s2s_model_top100000rockyou_fixedtype_cleanwords_2/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f6354040250> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f63eb9ba5d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = train(train_datasets, test_datasets, keydelay_tokens, keytext_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwTB-PrQQ8mk"
      },
      "source": [
        "# Testing Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xF9jPAa2Q_my",
        "outputId": "93f9c84c-5b1a-41e0-a616-f8a06e8a0ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_26\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, None, 1001)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, None, 66)]   0           []                               \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)                  [(None, 256),        1288192     ['input_5[0][0]']                \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)                  [(None, None, 256),  330752      ['input_6[0][0]',                \n",
            "                                 (None, 256),                     'lstm_4[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_4[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, None, 66)     16962       ['lstm_5[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,635,906\n",
            "Trainable params: 1,635,906\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Number of samples: 3391\n",
            "Number of unique input tokens: 1001\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 13\n",
            "Max sequence length for outputs: 15\n",
            "(3391, 13, 1001)\n",
            "(3391, 15, 66)\n",
            "(3391, 15, 66)\n",
            "Input sentence: ['t', 'h', 'u', 's']\n",
            "\n",
            "-----\n",
            "score : 0.9712533109762157  |  f\n",
            "score : 1.2399911497609137  |  t\n",
            "score : 2.4820566359303866  |  l\n",
            "score : 2.553589737933012  |  s\n",
            "score : 2.958496939853599  |  u\n",
            "score : 3.4134530412037734  |  w\n",
            "score : 3.5252084533017385  |  a\n",
            "score : 3.650025370422054  |  m\n",
            "score : 4.028931538215735  |  g\n",
            "score : 6.322144470776491  |  r\n",
            "\n",
            "-----\n",
            "score : 1.6322448659543471  |  th\n",
            "score : 1.7326241161730798  |  fr\n",
            "score : 1.737018605156179  |  fe\n",
            "score : 2.558573232484978  |  li\n",
            "score : 2.732182569721918  |  sa\n",
            "score : 3.1364136775541227  |  up\n",
            "score : 3.155438056850264  |  tu\n",
            "score : 3.357586616751283  |  ti\n",
            "score : 3.733270722060571  |  mi\n",
            "score : 3.777857502354264  |  ab\n",
            "\n",
            "-----\n",
            "score : 1.8458444658112692  |  fel\n",
            "score : 1.910420158157717  |  fre\n",
            "score : 2.4453475410597116  |  tha\n",
            "score : 2.6506873444631656  |  lif\n",
            "score : 2.8103834476462786  |  sai\n",
            "score : 3.1465194589564938  |  thu\n",
            "score : 3.1506697682239935  |  thi\n",
            "score : 3.1925223806430436  |  upo\n",
            "score : 3.3298844344026817  |  tur\n",
            "score : 3.6736860259818442  |  tim\n",
            "\n",
            "-----\n",
            "score : 1.887646154982341  |  fell\n",
            "score : 1.9632844966555858  |  free\n",
            "score : 2.4885008280727745  |  that\n",
            "score : 2.6822819285789428  |  life\n",
            "score : 2.842763062070286  |  said\n",
            "score : 3.2641100724520213  |  upon\n",
            "score : 3.276833634981223  |  thus\n",
            "score : 3.301899837796544  |  this\n",
            "score : 3.4206231628989983  |  turn\n",
            "score : 3.709879048205986  |  time\n",
            "\n",
            "-----\n",
            "\n",
            "Input sentence: ['s', 'h', 'o', 'r', 't', 'l', 'y']\n",
            "\n",
            "-----\n",
            "score : 0.950624179569573  |  a\n",
            "score : 2.060762117783258  |  w\n",
            "score : 2.0923783055469807  |  s\n",
            "score : 2.2283036821079194  |  e\n",
            "score : 2.3147293566916995  |  p\n",
            "score : 2.391847320961767  |  r\n",
            "score : 3.673707715011545  |  c\n",
            "score : 4.5122697041236846  |  g\n",
            "score : 5.35943750976487  |  n\n",
            "score : 5.806398108899319  |  t\n",
            "\n",
            "-----\n",
            "score : 1.7491126965211268  |  as\n",
            "score : 1.9632945903059977  |  aw\n",
            "score : 2.0720361110991976  |  we\n",
            "score : 2.1458277334225  |  sh\n",
            "score : 2.26705403928215  |  es\n",
            "score : 2.341254082866987  |  pl\n",
            "score : 2.3997737644154538  |  re\n",
            "score : 3.680554939595928  |  ac\n",
            "score : 3.7377105782983526  |  an\n",
            "score : 4.447020744088547  |  cc\n",
            "\n",
            "-----\n",
            "score : 1.9685945289141082  |  awe\n",
            "score : 1.974044853291856  |  ase\n",
            "score : 2.0790079949031184  |  wea\n",
            "score : 2.1507221464166264  |  sho\n",
            "score : 2.280078298676828  |  esc\n",
            "score : 2.3471322502118657  |  ple\n",
            "score : 2.4094627350446576  |  ref\n",
            "score : 3.6315156103293775  |  ass\n",
            "score : 3.755797299457405  |  acc\n",
            "score : 3.7620672023124286  |  ano\n",
            "\n",
            "-----\n",
            "score : 1.9764241297494696  |  awea\n",
            "score : 2.0027836137944455  |  asec\n",
            "score : 2.0878006114853944  |  weas\n",
            "score : 2.1563242542931254  |  shor\n",
            "score : 2.2882902180851907  |  esca\n",
            "score : 2.3535252127109514  |  plea\n",
            "score : 2.4153421015111074  |  refu\n",
            "score : 3.677136306874999  |  assu\n",
            "score : 3.7672996611139005  |  acco\n",
            "score : 3.7722150681620206  |  anot\n",
            "\n",
            "-----\n",
            "score : 1.9929808631278547  |  aweas\n",
            "score : 2.0246187935792817  |  aseco\n",
            "score : 2.107563842079531  |  wease\n",
            "score : 2.1949774634071924  |  short\n",
            "score : 2.3199245435500013  |  escap\n",
            "score : 2.37432053413164  |  plead\n",
            "score : 2.442139296251737  |  refus\n",
            "score : 3.6942176077730204  |  assur\n",
            "score : 3.784325849483675  |  accou\n",
            "score : 3.7909379194057355  |  anoth\n",
            "\n",
            "-----\n",
            "score : 2.0165016081303917  |  awease\n",
            "score : 2.0705665753758313  |  asecon\n",
            "score : 2.14985538555734  |  weasel\n",
            "score : 2.2317492999140445  |  shortl\n",
            "score : 2.3594255761023146  |  escape\n",
            "score : 2.4007898084872  |  pleade\n",
            "score : 2.4714583407317714  |  refuse\n",
            "score : 3.716447508033497  |  assure\n",
            "score : 3.8290849185028137  |  accoun\n",
            "score : 3.8599476463767872  |  anothe\n",
            "\n",
            "-----\n",
            "score : 2.0430958064323117  |  aweasel\n",
            "score : 2.09719199179969  |  asecond\n",
            "score : 2.301026010680702  |  shortly\n",
            "score : 2.3285323782049896  |  weasels\n",
            "score : 2.3804006856169875  |  escaped\n",
            "score : 2.420810718701508  |  pleaded\n",
            "score : 2.500644940115567  |  refused\n",
            "score : 3.7428047252384467  |  assured\n",
            "score : 3.878001133578402  |  account\n",
            "score : 4.014963474325504  |  another\n",
            "\n",
            "-----\n",
            "\n",
            "Input sentence: ['s', 'a', 'i', 'd']\n",
            "\n",
            "-----\n",
            "score : 0.392814950768864  |  t\n",
            "score : 2.7733081109770352  |  l\n",
            "score : 2.9732592786244374  |  f\n",
            "score : 2.995720188785284  |  w\n",
            "score : 3.337639161128616  |  g\n",
            "score : 3.358513260891661  |  u\n",
            "score : 3.4019702160226464  |  m\n",
            "score : 3.564934138059159  |  a\n",
            "score : 3.873771961859502  |  s\n",
            "score : 6.616447900504272  |  b\n",
            "\n",
            "-----\n",
            "score : 0.6244245552362873  |  th\n",
            "score : 2.7718855946204037  |  tu\n",
            "score : 2.816807412218365  |  ti\n",
            "score : 2.8393380187440904  |  li\n",
            "score : 3.46445390143801  |  mi\n",
            "score : 3.4871656422137143  |  go\n",
            "score : 3.5046549057461145  |  up\n",
            "score : 3.5542534868241455  |  wh\n",
            "score : 3.733808446043325  |  fr\n",
            "score : 3.835982650814165  |  ab\n",
            "\n",
            "-----\n",
            "score : 1.1105017276259874  |  tha\n",
            "score : 2.3185094489990634  |  thu\n",
            "score : 2.9033759605739546  |  tur\n",
            "score : 2.9163898466435767  |  lif\n",
            "score : 2.953519149331009  |  thi\n",
            "score : 3.108736938899667  |  tim\n",
            "score : 3.551641995937049  |  upo\n",
            "score : 3.5668484870311263  |  goo\n",
            "score : 3.57775662806705  |  mic\n",
            "score : 3.723680248480945  |  who\n",
            "\n",
            "-----\n",
            "score : 1.1436896635428262  |  that\n",
            "score : 2.4760065622335747  |  thus\n",
            "score : 2.948577146733276  |  life\n",
            "score : 2.9795854883982265  |  turn\n",
            "score : 3.131928291333113  |  this\n",
            "score : 3.1443093583593056  |  time\n",
            "score : 3.59906662633431  |  good\n",
            "score : 3.613320647781843  |  mice\n",
            "score : 3.6256218171462415  |  upon\n",
            "score : 3.9217407163706124  |  whom\n",
            "\n",
            "-----\n",
            "\n",
            "Input sentence: ['r', 'e', 'f', 'u', 's', 'e', 'd']\n",
            "\n",
            "-----\n",
            "score : 1.1134016184614233  |  a\n",
            "score : 1.2371809504922187  |  p\n",
            "score : 1.8738081328969642  |  e\n",
            "score : 2.34881433984442  |  w\n",
            "score : 3.00735306002552  |  s\n",
            "score : 3.5929853924341706  |  r\n",
            "score : 3.904234148627898  |  c\n",
            "score : 4.85098093844114  |  g\n",
            "score : 5.581602248930815  |  n\n",
            "score : 5.760862521210561  |  l\n",
            "\n",
            "-----\n",
            "score : 1.2483714588984296  |  pl\n",
            "score : 1.913025299007592  |  ac\n",
            "score : 1.936573058412953  |  es\n",
            "score : 2.35418173387145  |  aw\n",
            "score : 2.3764095335603974  |  we\n",
            "score : 2.7990154014092496  |  as\n",
            "score : 3.206875356807949  |  sh\n",
            "score : 3.6226530637170447  |  re\n",
            "score : 4.284058501844585  |  cc\n",
            "score : 4.96384852311575  |  an\n",
            "\n",
            "-----\n",
            "score : 1.2546841021431323  |  ple\n",
            "score : 1.9336621610426288  |  acc\n",
            "score : 1.9472000301222725  |  esc\n",
            "score : 2.365019211324305  |  awe\n",
            "score : 2.384867096013989  |  wea\n",
            "score : 3.2032768925050608  |  ase\n",
            "score : 3.21674447724608  |  sho\n",
            "score : 3.6656327354916547  |  ref\n",
            "score : 4.473273389187863  |  ccc\n",
            "score : 4.633086436108217  |  ass\n",
            "\n",
            "-----\n",
            "score : 1.2613970860517054  |  plea\n",
            "score : 1.9414834718166742  |  acco\n",
            "score : 1.9558231516781663  |  esca\n",
            "score : 2.3724235846295065  |  awea\n",
            "score : 2.3951350931844204  |  weas\n",
            "score : 3.2232417024397377  |  shor\n",
            "score : 3.378918458629034  |  asec\n",
            "score : 3.6740303691811134  |  refu\n",
            "score : 4.700740245292631  |  assu\n",
            "score : 4.711304379157539  |  ccco\n",
            "\n",
            "-----\n",
            "score : 1.2810276721881126  |  plead\n",
            "score : 1.9570687789077492  |  accou\n",
            "score : 1.9854751048206976  |  escap\n",
            "score : 2.387806766156818  |  aweas\n",
            "score : 2.416092064180951  |  wease\n",
            "score : 3.2633250187827763  |  short\n",
            "score : 3.4026139893730147  |  aseco\n",
            "score : 3.7013603500616243  |  refus\n",
            "score : 4.719669664430362  |  assur\n",
            "score : 4.779241333023027  |  cccou\n",
            "\n",
            "-----\n",
            "score : 1.3082363120908025  |  pleade\n",
            "score : 2.0014410212070652  |  accoun\n",
            "score : 2.026927453177921  |  escape\n",
            "score : 2.4119261449042515  |  awease\n",
            "score : 2.458914823670451  |  weasel\n",
            "score : 3.2995092652421683  |  shortl\n",
            "score : 3.450465442624728  |  asecon\n",
            "score : 3.731256699109051  |  refuse\n",
            "score : 4.744628417369534  |  assure\n",
            "score : 4.9373439002965815  |  cccoun\n",
            "\n",
            "-----\n",
            "score : 1.3275542624835304  |  pleaded\n",
            "score : 2.04751582094919  |  escaped\n",
            "score : 2.0485795553167128  |  account\n",
            "score : 2.4384109650447803  |  aweasel\n",
            "score : 2.6365265390740005  |  weasels\n",
            "score : 3.3700210964989896  |  shortly\n",
            "score : 3.478320158420152  |  asecond\n",
            "score : 3.7602114697421958  |  refused\n",
            "score : 4.770678352745375  |  assured\n",
            "score : 5.005744111129673  |  cccount\n",
            "\n",
            "-----\n",
            "\n",
            "Input sentence: ['f', 'r', 'e', 'e']\n",
            "\n",
            "-----\n",
            "score : 0.6845416411648676  |  t\n",
            "score : 2.099946860120576  |  w\n",
            "score : 2.3599566669953354  |  a\n",
            "score : 2.685518090054761  |  f\n",
            "score : 2.817964455708759  |  m\n",
            "score : 2.9905719130788353  |  g\n",
            "score : 3.0571001704363923  |  l\n",
            "score : 3.65292532655004  |  s\n",
            "score : 3.9422321017799353  |  u\n",
            "score : 6.404390254375341  |  b\n",
            "\n",
            "-----\n",
            "score : 0.9649263444948383  |  th\n",
            "score : 2.4694860925087454  |  ab\n",
            "score : 2.489057876288033  |  wh\n",
            "score : 2.671713534589318  |  ti\n",
            "score : 2.874961094592128  |  mi\n",
            "score : 3.087771400629556  |  go\n",
            "score : 3.1166665354942977  |  li\n",
            "score : 3.1232086319101757  |  fe\n",
            "score : 3.2889009792051684  |  tu\n",
            "score : 3.368208248186326  |  wi\n",
            "\n",
            "-----\n",
            "score : 1.8194327020272225  |  thu\n",
            "score : 2.078710047393946  |  tha\n",
            "score : 2.540651608487673  |  aba\n",
            "score : 2.610163169698278  |  who\n",
            "score : 2.9572156601255206  |  tim\n",
            "score : 2.9996140724703317  |  mic\n",
            "score : 3.1484966550508235  |  goo\n",
            "score : 3.198788540840227  |  lif\n",
            "score : 3.2051749357428383  |  thi\n",
            "score : 3.2589115810099276  |  fel\n",
            "\n",
            "-----\n",
            "score : 1.9384397003911154  |  thus\n",
            "score : 2.117956638834643  |  that\n",
            "score : 2.5681522021684233  |  abat\n",
            "score : 2.788247837530007  |  whom\n",
            "score : 2.9921323706025946  |  time\n",
            "score : 3.0325813858604818  |  mice\n",
            "score : 3.1817926712572504  |  good\n",
            "score : 3.2313207895372047  |  life\n",
            "score : 3.3004121413308103  |  fell\n",
            "score : 3.344469129810827  |  this\n",
            "\n",
            "-----\n",
            "\n",
            "Input sentence: ['a', 's', 'p', 'e', 'c', 'i', 'a', 'l']\n",
            "\n",
            "-----\n",
            "score : 0.0827425352343991  |  a\n",
            "score : 3.495828548691851  |  l\n",
            "score : 4.287919843243036  |  s\n",
            "score : 5.067476133981532  |  e\n",
            "score : 5.078431876532041  |  c\n",
            "score : 5.146867623151007  |  r\n",
            "score : 5.463426478739389  |  p\n",
            "score : 5.472375803164319  |  w\n",
            "score : 6.420625606521759  |  h\n",
            "score : 6.947839606906415  |  g\n",
            "\n",
            "-----\n",
            "score : 0.1179040675245784  |  as\n",
            "score : 3.563663818940657  |  li\n",
            "score : 4.349519053227083  |  an\n",
            "score : 4.923001768470351  |  ac\n",
            "score : 5.009005769325856  |  sh\n",
            "score : 5.131585021777753  |  es\n",
            "score : 5.158892828923612  |  ss\n",
            "score : 5.514642743977278  |  re\n",
            "score : 5.744866660601693  |  we\n",
            "score : 5.963856891068751  |  pl\n",
            "\n",
            "-----\n",
            "score : 0.19235913649120318  |  asp\n",
            "score : 3.2859260217478896  |  ass\n",
            "score : 3.5798902172240483  |  lik\n",
            "score : 4.51123130336177  |  ano\n",
            "score : 4.9099494369865635  |  ase\n",
            "score : 5.015783982009679  |  sho\n",
            "score : 5.337195567736327  |  asc\n",
            "score : 5.557432531374851  |  ref\n",
            "score : 5.574926525043371  |  esc\n",
            "score : 5.623023282588954  |  aso\n",
            "\n",
            "-----\n",
            "score : 0.21005009117728926  |  aspe\n",
            "score : 3.586114990556231  |  like\n",
            "score : 3.805427634730548  |  asse\n",
            "score : 4.435432471379707  |  assu\n",
            "score : 4.629370324059006  |  anot\n",
            "score : 5.025512615313557  |  shor\n",
            "score : 5.3037283737608645  |  asee\n",
            "score : 5.418302095455146  |  aspc\n",
            "score : 5.593633687125705  |  asce\n",
            "score : 5.593680620190492  |  refu\n",
            "\n",
            "-----\n",
            "score : 0.2258109216777737  |  aspec\n",
            "score : 3.5935070541755563  |  likew\n",
            "score : 4.053656905193748  |  asser\n",
            "score : 4.4523881516696235  |  assur\n",
            "score : 4.680155238509354  |  anoth\n",
            "score : 5.093820859585269  |  short\n",
            "score : 5.236919314399613  |  aspew\n",
            "score : 5.421320111707254  |  aseec\n",
            "score : 5.463198618404448  |  aspcc\n",
            "score : 5.6620277306307685  |  refus\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-b43dffe603f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# inference(test_datasets, keydelay_tokens, keytext_tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbeam_search_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeydelay_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeytext_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-e69b1a9ee4bf>\u001b[0m in \u001b[0;36mbeam_search_inference\u001b[0;34m(test_datasets, keydelay_tokens, keytext_tokens)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtarget_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input sentence:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mgenerate_beam_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_next_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_search_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreak_at_eos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-181c28c3269a>\u001b[0m in \u001b[0;36mgenerate_beam_predictions\u001b[0;34m(input_seq, n_next_sequences, beam_search_n, break_at_eos, models, tokens, indices)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdistributions_scores_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbeam_search_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# print(\"DIST\", distributions_scores_states)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# inference(test_datasets, keydelay_tokens, keytext_tokens)\n",
        "beam_search_inference(test_datasets, keydelay_tokens, keytext_tokens)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Camstroke-Inference.ipynb",
      "provenance": [],
      "mount_file_id": "1FEw-o3zi1UWpFDKakC83SfnMkhrx6sBf",
      "authorship_tag": "ABX9TyO7RfQgQrZKULd8XPzvwgI1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}